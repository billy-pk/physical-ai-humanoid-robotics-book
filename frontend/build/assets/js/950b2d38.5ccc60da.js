"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[780],{1637:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>o,contentTitle:()=>a,default:()=>h,frontMatter:()=>t,metadata:()=>r,toc:()=>d});const r=JSON.parse('{"id":"module-2/chapter-2-4","title":"2.4 Visual SLAM Basics","description":"How does a robot know where it is? How does it build a map of an unknown environment? SLAM (Simultaneous Localization and Mapping) solves both problems at once using cameras.","source":"@site/docs/module-2/chapter-2-4.md","sourceDirName":"module-2","slug":"/module-2/chapter-2-4","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-4","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/chapter-2-4.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"2.4 Visual SLAM Basics"},"sidebar":"defaultSidebar","previous":{"title":"2.3 3D Vision and Depth Perception","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3"}}');var i=s(4848),l=s(8453);const t={sidebar_position:5,title:"2.4 Visual SLAM Basics"},a="Chapter 2.4: Visual SLAM Basics",o={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"The SLAM Problem",id:"the-slam-problem",level:2},{value:"Visual Odometry (VO)",id:"visual-odometry-vo",level:2},{value:"How It Works",id:"how-it-works",level:3},{value:"Example with OpenCV",id:"example-with-opencv",level:3},{value:"Visual SLAM",id:"visual-slam",level:2},{value:"Popular Visual SLAM Systems",id:"popular-visual-slam-systems",level:3},{value:"ORB-SLAM3: The Gold Standard",id:"orb-slam3-the-gold-standard",level:2},{value:"Key Features",id:"key-features",level:3},{value:"Installation",id:"installation",level:3},{value:"Running ORB-SLAM3",id:"running-orb-slam3",level:3},{value:"Using with ROS",id:"using-with-ros",level:3},{value:"Understanding SLAM Output",id:"understanding-slam-output",level:2},{value:"1. Trajectory",id:"1-trajectory",level:3},{value:"2. Map Points",id:"2-map-points",level:3},{value:"3. Keyframes",id:"3-keyframes",level:3},{value:"Visual-Inertial SLAM",id:"visual-inertial-slam",level:2},{value:"Example: VINS-Mono with ROS",id:"example-vins-mono-with-ros",level:3},{value:"When to Use Visual SLAM vs. LIDAR SLAM",id:"when-to-use-visual-slam-vs-lidar-slam",level:2},{value:"Practical Tips",id:"practical-tips",level:2},{value:"1. Camera Selection",id:"1-camera-selection",level:3},{value:"2. Calibration is Critical",id:"2-calibration-is-critical",level:3},{value:"3. Environment Matters",id:"3-environment-matters",level:3},{value:"Exercises",id:"exercises",level:2},{value:"1. Visual Odometry Experiment",id:"1-visual-odometry-experiment",level:3},{value:"2. ORB-SLAM3 Installation",id:"2-orb-slam3-installation",level:3},{value:"3. Visual vs. LIDAR Research",id:"3-visual-vs-lidar-research",level:3},{value:"4. DIY Camera Trajectory",id:"4-diy-camera-trajectory",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function c(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,l.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-24-visual-slam-basics",children:"Chapter 2.4: Visual SLAM Basics"})}),"\n",(0,i.jsxs)(n.p,{children:["How does a robot know where it is? How does it build a map of an unknown environment? ",(0,i.jsx)(n.strong,{children:"SLAM (Simultaneous Localization and Mapping)"})," solves both problems at once using cameras."]}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understand"})," the SLAM problem and why it's challenging"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Explain"})," visual odometry and visual SLAM concepts"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Apply"})," ORB-SLAM for camera-based localization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Recognize"})," when to use visual vs. LIDAR SLAM"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"the-slam-problem",children:"The SLAM Problem"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"SLAM"}),": Simultaneously estimate:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robot's pose"})," (position + orientation) over time"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Map"})," of the environment"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"The chicken-and-egg problem"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"To localize, you need a map"}),"\n",(0,i.jsx)(n.li,{children:"To build a map, you need to know your position"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"SLAM solves both together!"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"visual-odometry-vo",children:"Visual Odometry (VO)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Visual Odometry"})," = estimating robot motion from camera images."]}),"\n",(0,i.jsx)(n.h3,{id:"how-it-works",children:"How It Works"}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Extract features"})," from consecutive frames (ORB, SIFT)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Match features"})," between frames"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Estimate camera motion"})," from feature correspondences"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Integrate motion"})," to get trajectory"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-with-opencv",children:"Example with OpenCV"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\n# Feature detector\norb = cv2.ORB_create(nfeatures=2000)\n\n# Feature matcher\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n# Initialize\ncap = cv2.VideoCapture('robot_trajectory.mp4')\nret, prev_frame = cap.read()\nprev_gray = cv2.cvtColor(prev_frame, cv2.COLOR_BGR2GRAY)\n\n# Detect keypoints in first frame\nprev_kp, prev_des = orb.detectAndCompute(prev_gray, None)\n\n# Camera intrinsics (example values - get from calibration)\nK = np.array([[500, 0, 320],\n              [0, 500, 240],\n              [0, 0, 1]], dtype=np.float32)\n\n# Initialize pose\npose = np.eye(4)  # 4x4 transformation matrix\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n\n    # Detect features in current frame\n    curr_kp, curr_des = orb.detectAndCompute(gray, None)\n\n    # Match features\n    matches = bf.match(prev_des, curr_des)\n    matches = sorted(matches, key=lambda x: x.distance)\n\n    # Get matched keypoint coordinates\n    pts1 = np.float32([prev_kp[m.queryIdx].pt for m in matches[:100]])\n    pts2 = np.float32([curr_kp[m.trainIdx].pt for m in matches[:100]])\n\n    # Estimate essential matrix\n    E, mask = cv2.findEssentialMat(pts1, pts2, K, method=cv2.RANSAC,\n                                   prob=0.999, threshold=1.0)\n\n    # Recover pose (R, t)\n    _, R, t, mask = cv2.recoverPose(E, pts1, pts2, K)\n\n    # Update pose\n    T = np.eye(4)\n    T[:3, :3] = R\n    T[:3, 3] = t.squeeze()\n    pose = pose @ T\n\n    # Print current position\n    position = pose[:3, 3]\n    print(f\"Position: x={position[0]:.2f}, y={position[1]:.2f}, z={position[2]:.2f}\")\n\n    # Update for next iteration\n    prev_kp, prev_des = curr_kp, curr_des\n\ncap.release()\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Limitations of pure VO"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u274c Drift accumulates over time (no loop closure)"}),"\n",(0,i.jsx)(n.li,{children:"\u274c No map built"}),"\n",(0,i.jsx)(n.li,{children:"\u274c Can't re-localize if lost"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"This is where full SLAM comes in!"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"visual-slam",children:"Visual SLAM"}),"\n",(0,i.jsx)(n.p,{children:"Visual SLAM extends VO by:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Building a map"})," of 3D landmarks"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Loop closure detection"})," (recognize revisited places)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global optimization"})," (correct accumulated drift)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"popular-visual-slam-systems",children:"Popular Visual SLAM Systems"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"System"}),(0,i.jsx)(n.th,{children:"Type"}),(0,i.jsx)(n.th,{children:"Strengths"}),(0,i.jsx)(n.th,{children:"Use Case"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"ORB-SLAM3"})}),(0,i.jsx)(n.td,{children:"Feature-based"}),(0,i.jsx)(n.td,{children:"Fast, accurate, robust"}),(0,i.jsx)(n.td,{children:"General robotics"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"LSD-SLAM"})}),(0,i.jsx)(n.td,{children:"Direct (no features)"}),(0,i.jsx)(n.td,{children:"Dense maps"}),(0,i.jsx)(n.td,{children:"Dense reconstruction"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"RTAB-Map"})}),(0,i.jsx)(n.td,{children:"RGB-D"}),(0,i.jsx)(n.td,{children:"Works with depth cameras"}),(0,i.jsx)(n.td,{children:"Indoor robots"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"SVO"})}),(0,i.jsx)(n.td,{children:"Semi-direct"}),(0,i.jsx)(n.td,{children:"Very fast"}),(0,i.jsx)(n.td,{children:"Drones, micro-robots"})]})]})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"orb-slam3-the-gold-standard",children:"ORB-SLAM3: The Gold Standard"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"ORB-SLAM3"})," is the most widely used visual SLAM system."]}),"\n",(0,i.jsx)(n.h3,{id:"key-features",children:"Key Features"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\u2705 Works with ",(0,i.jsx)(n.strong,{children:"monocular"}),", ",(0,i.jsx)(n.strong,{children:"stereo"}),", or ",(0,i.jsx)(n.strong,{children:"RGB-D"})," cameras"]}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Real-time performance (30+ FPS)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Loop closure and global optimization"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Re-localization (recover from tracking loss)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Multi-map support"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Dependencies\nsudo apt install libopencv-dev libeigen3-dev libpangolin-dev\n\n# Clone and build\ngit clone https://github.com/UZ-SLAMLab/ORB_SLAM3.git\ncd ORB_SLAM3\nchmod +x build.sh\n./build.sh\n"})}),"\n",(0,i.jsx)(n.h3,{id:"running-orb-slam3",children:"Running ORB-SLAM3"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Monocular example (TUM dataset)\n./Examples/Monocular/mono_tum \\\n    Vocabulary/ORBvoc.txt \\\n    Examples/Monocular/TUM1.yaml \\\n    /path/to/dataset\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Outputs"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Real-time 3D map visualization"}),"\n",(0,i.jsx)(n.li,{children:"Trajectory (camera poses over time)"}),"\n",(0,i.jsx)(n.li,{children:"Keyframe database"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"using-with-ros",children:"Using with ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Build ROS wrapper\ncd Examples/ROS/ORB_SLAM3\nmkdir build && cd build\ncmake .. -DROS_BUILD_TYPE=Release\nmake\n\n# Run\nrosrun ORB_SLAM3 Mono Vocabulary/ORBvoc.txt Examples/Monocular/TUM1.yaml\n"})}),"\n",(0,i.jsx)(n.p,{children:"Subscribe to camera topic:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"rostopic echo /camera/image_raw\n"})}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"understanding-slam-output",children:"Understanding SLAM Output"}),"\n",(0,i.jsx)(n.h3,{id:"1-trajectory",children:"1. Trajectory"}),"\n",(0,i.jsx)(n.p,{children:"List of camera poses over time:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"timestamp, x, y, z, qx, qy, qz, qw\n1678901234.5, 0.0, 0.0, 0.0, 0, 0, 0, 1\n1678901234.6, 0.05, 0.01, 0.02, 0.01, 0, 0, 0.999\n...\n"})}),"\n",(0,i.jsx)(n.h3,{id:"2-map-points",children:"2. Map Points"}),"\n",(0,i.jsx)(n.p,{children:"3D landmarks (features) in the environment:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"x, y, z\n1.23, 0.45, 2.10\n0.98, -0.12, 1.87\n...\n"})}),"\n",(0,i.jsx)(n.h3,{id:"3-keyframes",children:"3. Keyframes"}),"\n",(0,i.jsx)(n.p,{children:"Subset of frames used for mapping (not every frame is kept)."}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"visual-inertial-slam",children:"Visual-Inertial SLAM"}),"\n",(0,i.jsx)(n.p,{children:"Combine camera with IMU for better performance."}),"\n",(0,i.jsx)(n.p,{children:(0,i.jsx)(n.strong,{children:"Why add IMU?"})}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Provides scale (monocular SLAM has scale ambiguity)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Better performance in fast motion"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Robust to visual degradation (blur, low texture)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Popular VI-SLAM systems"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"ORB-SLAM3 (supports IMU)"}),"\n",(0,i.jsx)(n.li,{children:"VINS-Mono"}),"\n",(0,i.jsx)(n.li,{children:"Kimera"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"example-vins-mono-with-ros",children:"Example: VINS-Mono with ROS"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Clone and build\ngit clone https://github.com/HKUST-Aerial-Robotics/VINS-Mono.git\ncd VINS-Mono\ncatkin_make\n\n# Run\nroslaunch vins_estimator euroc.launch\nroslaunch vins_estimator vins_rviz.launch\n\n# Play dataset\nrosbag play MH_01_easy.bag\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Visualization in RViz"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Green line: Estimated trajectory"}),"\n",(0,i.jsx)(n.li,{children:"Red points: Map landmarks"}),"\n",(0,i.jsx)(n.li,{children:"Blue frustums: Keyframe poses"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"when-to-use-visual-slam-vs-lidar-slam",children:"When to Use Visual SLAM vs. LIDAR SLAM"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Factor"}),(0,i.jsx)(n.th,{children:"Visual SLAM"}),(0,i.jsx)(n.th,{children:"LIDAR SLAM"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Cost"})}),(0,i.jsx)(n.td,{children:"Low (camera ~$50)"}),(0,i.jsx)(n.td,{children:"High (LIDAR $500-$5000)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Indoor"})}),(0,i.jsx)(n.td,{children:"Good (if textured)"}),(0,i.jsx)(n.td,{children:"Excellent"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Outdoor"})}),(0,i.jsx)(n.td,{children:"Excellent"}),(0,i.jsx)(n.td,{children:"Good"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Lighting"})}),(0,i.jsx)(n.td,{children:"Sensitive"}),(0,i.jsx)(n.td,{children:"Unaffected"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Speed"})}),(0,i.jsx)(n.td,{children:"Medium (30-60 Hz)"}),(0,i.jsx)(n.td,{children:"Fast (10-20 Hz)"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Map density"})}),(0,i.jsx)(n.td,{children:"Sparse features"}),(0,i.jsx)(n.td,{children:"Dense 3D points"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Best for"})}),(0,i.jsx)(n.td,{children:"Drones, wheeled robots"}),(0,i.jsx)(n.td,{children:"Autonomous vehicles"})]})]})]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Humanoid robots often use both"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Visual SLAM for rich environment understanding"}),"\n",(0,i.jsx)(n.li,{children:"LIDAR for reliable obstacle avoidance"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"practical-tips",children:"Practical Tips"}),"\n",(0,i.jsx)(n.h3,{id:"1-camera-selection",children:"1. Camera Selection"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"For visual SLAM"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Global shutter"})," (not rolling shutter) \u2014 prevents motion distortion"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Wide FOV"})," (>90\xb0) \u2014 tracks more features"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"High frame rate"})," (60+ FPS) \u2014 better for fast motion"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"2-calibration-is-critical",children:"2. Calibration is Critical"}),"\n",(0,i.jsx)(n.p,{children:"Poor calibration \u2192 poor SLAM performance."}),"\n",(0,i.jsx)(n.p,{children:"Calibrate:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Intrinsic parameters (focal length, distortion)"}),"\n",(0,i.jsx)(n.li,{children:"Stereo baseline (for stereo cameras)"}),"\n",(0,i.jsx)(n.li,{children:"IMU-camera extrinsics (for VI-SLAM)"}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Tools:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/ethz-asl/kalibr",children:"Kalibr"})," (ROS-based)"]}),"\n",(0,i.jsx)(n.li,{children:"OpenCV calibration"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-environment-matters",children:"3. Environment Matters"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"SLAM works best with"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u2705 Rich visual features (posters, furniture, trees)"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Good lighting"}),"\n",(0,i.jsx)(n.li,{children:"\u2705 Moderate motion (not too fast)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"SLAM struggles with"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"\u274c Blank walls"}),"\n",(0,i.jsx)(n.li,{children:"\u274c Repetitive patterns (tiles, bricks)"}),"\n",(0,i.jsx)(n.li,{children:"\u274c Motion blur"}),"\n",(0,i.jsx)(n.li,{children:"\u274c Reflective/transparent surfaces"}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"1-visual-odometry-experiment",children:"1. Visual Odometry Experiment"}),"\n",(0,i.jsxs)(n.p,{children:["Download the ",(0,i.jsx)(n.a,{href:"https://vision.in.tum.de/data/datasets/rgbd-dataset",children:"TUM RGB-D dataset"}),". Implement simple visual odometry using ORB features. Plot the estimated trajectory vs. ground truth. What's the drift after 1 minute?"]}),"\n",(0,i.jsx)(n.h3,{id:"2-orb-slam3-installation",children:"2. ORB-SLAM3 Installation"}),"\n",(0,i.jsx)(n.p,{children:"Install ORB-SLAM3 and run it on a sample dataset. Observe:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"How many map points are created?"}),"\n",(0,i.jsx)(n.li,{children:"How many keyframes?"}),"\n",(0,i.jsx)(n.li,{children:"Does loop closure occur?"}),"\n",(0,i.jsx)(n.li,{children:"What's the average tracking FPS?"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"3-visual-vs-lidar-research",children:"3. Visual vs. LIDAR Research"}),"\n",(0,i.jsx)(n.p,{children:"Find a recent paper comparing visual SLAM and LIDAR SLAM. Summarize:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Which performed better indoors?"}),"\n",(0,i.jsx)(n.li,{children:"Which performed better outdoors?"}),"\n",(0,i.jsx)(n.li,{children:"What were the failure cases for each?"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"4-diy-camera-trajectory",children:"4. DIY Camera Trajectory"}),"\n",(0,i.jsx)(n.p,{children:"Record a video walking around a room with your phone (keep it steady!). Process it with ORB-SLAM3 monocular mode. Can it reconstruct the room? Where does it fail?"}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,i.jsxs)(n.p,{children:["\u2705 ",(0,i.jsx)(n.strong,{children:"SLAM"})," solves localization and mapping simultaneously\n\u2705 ",(0,i.jsx)(n.strong,{children:"Visual odometry"})," estimates motion from camera but drifts over time\n\u2705 ",(0,i.jsx)(n.strong,{children:"Visual SLAM"})," builds maps, detects loop closures, and corrects drift\n\u2705 ",(0,i.jsx)(n.strong,{children:"ORB-SLAM3"})," is the state-of-the-art for most robotics applications\n\u2705 ",(0,i.jsx)(n.strong,{children:"Visual-Inertial SLAM"})," (camera + IMU) is more robust\n\u2705 SLAM requires ",(0,i.jsx)(n.strong,{children:"calibrated cameras"})," and ",(0,i.jsx)(n.strong,{children:"feature-rich environments"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://arxiv.org/abs/2007.11898",children:"ORB-SLAM3 Paper"})," (Campos et al., 2021)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.youtube.com/watch?v=U6vr3iNrwRA",children:"Visual SLAM Tutorial"})," (Cyrill Stachniss lectures)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.em,{children:"Introduction to Visual SLAM"})," by Xiang Gao (free book)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://github.com/xdspacelab/openvslam",children:"OpenVSLAM"})," (alternative to ORB-SLAM)"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Previous"}),": ",(0,i.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3",children:"\u2190 Chapter 2.3: 3D Vision and Depth"})]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:["\ud83c\udf89 ",(0,i.jsx)(n.strong,{children:"Module 2 Complete!"})," You now understand computer vision from classical techniques to deep learning, 3D perception, and visual SLAM. Ready to make robots move intelligently in Module 3!"]})]})}function h(e={}){const{wrapper:n}={...(0,l.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(c,{...e})}):c(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>a});var r=s(6540);const i={},l=r.createContext(i);function t(e){const n=r.useContext(l);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:t(e.components),r.createElement(l.Provider,{value:n},e.children)}}}]);
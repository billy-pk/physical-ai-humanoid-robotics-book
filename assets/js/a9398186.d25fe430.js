"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4818],{1966:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/chapter-4-1","title":"4.1 Voice-to-Action with OpenAI Whisper","description":"Voice commands enable natural human-robot interaction. OpenAI Whisper provides state-of-the-art speech recognition that converts spoken language to text\u2014the first step in the VLA pipeline. This chapter covers Whisper installation, ROS 2 integration, and real-time voice command processing.","source":"@site/docs/module-4/chapter-4-1.md","sourceDirName":"module-4","slug":"/module-4/chapter-4-1","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-1","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-4/chapter-4-1.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"4.1 Voice-to-Action with OpenAI Whisper"},"sidebar":"defaultSidebar","previous":{"title":"Module 4 Introduction","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/intro"},"next":{"title":"4.2 LLM-Based Cognitive Planning","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-2"}}');var r=s(4848),o=s(8453);const t={sidebar_position:2,title:"4.1 Voice-to-Action with OpenAI Whisper"},l="Chapter 4.1: Voice-to-Action with OpenAI Whisper",a={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: Whisper Fundamentals",id:"part-1-whisper-fundamentals",level:2},{value:"What is OpenAI Whisper?",id:"what-is-openai-whisper",level:3},{value:"Whisper Models",id:"whisper-models",level:3},{value:"Audio Processing Pipeline",id:"audio-processing-pipeline",level:3},{value:"Part 2: Hands-On Tutorial",id:"part-2-hands-on-tutorial",level:2},{value:"Project: Real-Time Voice Command System",id:"project-real-time-voice-command-system",level:3},{value:"Step 1: Install Whisper",id:"step-1-install-whisper",level:3},{value:"Step 2: Test Whisper with Audio File",id:"step-2-test-whisper-with-audio-file",level:3},{value:"Step 3: Create ROS 2 Whisper Node",id:"step-3-create-ros-2-whisper-node",level:3},{value:"Step 4: Build and Launch",id:"step-4-build-and-launch",level:3},{value:"Step 5: Command Processing Node",id:"step-5-command-processing-node",level:3},{value:"Step 6: Test Complete Pipeline",id:"step-6-test-complete-pipeline",level:3},{value:"Step 7: Debugging Common Issues",id:"step-7-debugging-common-issues",level:3},{value:"Issue 1: &quot;No module named &#39;whisper&#39;&quot;",id:"issue-1-no-module-named-whisper",level:4},{value:"Issue 2: &quot;Microphone not found&quot; or &quot;Permission denied&quot;",id:"issue-2-microphone-not-found-or-permission-denied",level:4},{value:"Issue 3: &quot;Low recognition accuracy&quot;",id:"issue-3-low-recognition-accuracy",level:4},{value:"Issue 4: &quot;High latency&quot; or &quot;Delayed transcription&quot;",id:"issue-4-high-latency-or-delayed-transcription",level:4},{value:"Issue 5: &quot;Memory usage too high&quot;",id:"issue-5-memory-usage-too-high",level:4},{value:"Part 3: Advanced Topics (Optional)",id:"part-3-advanced-topics-optional",level:2},{value:"Streaming Whisper",id:"streaming-whisper",level:3},{value:"Multi-Language Support",id:"multi-language-support",level:3},{value:"Noise Reduction",id:"noise-reduction",level:3},{value:"Integration with Capstone",id:"integration-with-capstone",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic Voice Recognition (Required)",id:"exercise-1-basic-voice-recognition-required",level:3},{value:"Exercise 2: ROS 2 Integration (Required)",id:"exercise-2-ros-2-integration-required",level:3},{value:"Exercise 3: Noise Robustness Test (Challenge)",id:"exercise-3-noise-robustness-test-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-41-voice-to-action-with-openai-whisper",children:"Chapter 4.1: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,r.jsx)(n.p,{children:"Voice commands enable natural human-robot interaction. OpenAI Whisper provides state-of-the-art speech recognition that converts spoken language to text\u2014the first step in the VLA pipeline. This chapter covers Whisper installation, ROS 2 integration, and real-time voice command processing."}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Install"})," OpenAI Whisper and configure for speech recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integrate"})," Whisper with ROS 2 for real-time voice command processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handle"})," audio preprocessing and noise reduction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Process"})," multi-language voice input"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Debug"})," common audio and recognition issues"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Humble"})," configured (Module 1)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Python 3.10+"})," with pip"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Microphone"})," connected (or simulated audio for testing)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Basic audio"})," concepts (sample rate, channels, formats)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Linux audio"})," setup (ALSA/PulseAudio)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-1-whisper-fundamentals",children:"Part 1: Whisper Fundamentals"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-openai-whisper",children:"What is OpenAI Whisper?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Whisper"})," is OpenAI's automatic speech recognition (ASR) system that:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Transcribes"})," audio to text with high accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Supports"})," 99+ languages"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Handles"})," accents, background noise, and technical vocabulary"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Runs"})," locally (no API calls required)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open-source"}),": Free to use and modify"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Why Whisper for humanoid robots?"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": State-of-the-art recognition (WER < 5% for English)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Works with noise and accents"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Privacy"}),": Runs locally (no cloud API)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-language"}),": Supports international deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time"}),": Can process audio streams in real-time"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"whisper-models",children:"Whisper Models"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Size"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"tiny"})}),(0,r.jsx)(n.td,{children:"39M"}),(0,r.jsx)(n.td,{children:"Fastest"}),(0,r.jsx)(n.td,{children:"Good"}),(0,r.jsx)(n.td,{children:"Real-time, low-resource"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"base"})}),(0,r.jsx)(n.td,{children:"74M"}),(0,r.jsx)(n.td,{children:"Fast"}),(0,r.jsx)(n.td,{children:"Very Good"}),(0,r.jsx)(n.td,{children:"Real-time, balanced"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"small"})}),(0,r.jsx)(n.td,{children:"244M"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"High accuracy"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"medium"})}),(0,r.jsx)(n.td,{children:"769M"}),(0,r.jsx)(n.td,{children:"Slow"}),(0,r.jsx)(n.td,{children:"Excellent"}),(0,r.jsx)(n.td,{children:"Best accuracy"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"large"})}),(0,r.jsx)(n.td,{children:"1550M"}),(0,r.jsx)(n.td,{children:"Slowest"}),(0,r.jsx)(n.td,{children:"Best"}),(0,r.jsx)(n.td,{children:"Maximum accuracy"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For real-time robotics"}),": Use ",(0,r.jsx)(n.code,{children:"base"})," or ",(0,r.jsx)(n.code,{children:"small"})," (good balance of speed/accuracy)."]}),"\n",(0,r.jsx)(n.h3,{id:"audio-processing-pipeline",children:"Audio Processing Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Typical pipeline"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Audio Capture"}),": Microphone \u2192 raw audio stream"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Preprocessing"}),": Noise reduction, normalization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Whisper Inference"}),": Audio \u2192 text transcription"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Post-processing"}),": Text cleaning, command extraction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS 2 Publishing"}),": Text \u2192 ROS 2 topic"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-2-hands-on-tutorial",children:"Part 2: Hands-On Tutorial"}),"\n",(0,r.jsx)(n.h3,{id:"project-real-time-voice-command-system",children:"Project: Real-Time Voice Command System"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Set up Whisper for real-time speech recognition and publish transcribed text to ROS 2 topics."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tools"}),": OpenAI Whisper, ROS 2 Humble, Python 3.10+, microphone"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-install-whisper",children:"Step 1: Install Whisper"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install Whisper"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install via pip\npip3 install openai-whisper\n\n# Install audio dependencies\nsudo apt install ffmpeg\n\n# Verify installation\nwhisper --help\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install ROS 2 audio packages"})," (optional, for ROS 2 audio topics):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-humble-audio-common\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-2-test-whisper-with-audio-file",children:"Step 2: Test Whisper with Audio File"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Download test audio"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Download sample audio (or record your own)\nwget https://github.com/openai/whisper/raw/main/tests/jfk.flac\n\n# Transcribe\nwhisper jfk.flac --model base\n\n# Expected output: Text file with transcription\ncat jfk.flac.txt\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Test with microphone"})," (record first):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Record 5 seconds of audio\narecord -d 5 -f cd test.wav\n\n# Transcribe\nwhisper test.wav --model base\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-3-create-ros-2-whisper-node",children:"Step 3: Create ROS 2 Whisper Node"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create package"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ros_ws/src\nros2 pkg create --build-type ament_python voice_commands --dependencies rclpy std_msgs\ncd voice_commands\nmkdir -p voice_commands\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create node"}),": ",(0,r.jsx)(n.code,{children:"voice_commands/whisper_node.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nROS 2 node for OpenAI Whisper speech recognition\nROS 2 Humble | Python 3.10+ | Whisper\n\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport whisper\nimport pyaudio\nimport numpy as np\nimport queue\nimport threading\nfrom datetime import datetime\n\nclass WhisperNode(Node):\n    \"\"\"\n    Real-time speech recognition using OpenAI Whisper\n    Publishes transcribed text to /voice_commands/text topic\n    \"\"\"\n    def __init__(self):\n        super().__init__('whisper_node')\n        \n        # Parameters\n        self.declare_parameter('model', 'base')  # Whisper model size\n        self.declare_parameter('language', 'en')  # Language code\n        self.declare_parameter('sample_rate', 16000)  # Audio sample rate\n        self.declare_parameter('chunk_size', 4096)  # Audio chunk size\n        self.declare_parameter('vad_threshold', 0.5)  # Voice activity detection\n        \n        model_name = self.get_parameter('model').value\n        self.language = self.get_parameter('language').value\n        sample_rate = self.get_parameter('sample_rate').value\n        chunk_size = self.get_parameter('chunk_size').value\n        \n        # Load Whisper model\n        self.get_logger().info(f'Loading Whisper model: {model_name}')\n        self.model = whisper.load_model(model_name)\n        self.get_logger().info('Whisper model loaded')\n        \n        # Publisher\n        self.text_pub = self.create_publisher(String, '/voice_commands/text', 10)\n        \n        # Audio setup\n        self.audio = pyaudio.PyAudio()\n        self.sample_rate = sample_rate\n        self.chunk_size = chunk_size\n        self.audio_queue = queue.Queue()\n        \n        # Start audio capture thread\n        self.audio_thread = threading.Thread(target=self.audio_capture_loop)\n        self.audio_thread.daemon = True\n        self.audio_thread.start()\n        \n        # Start processing timer\n        self.process_timer = self.create_timer(2.0, self.process_audio)  # Process every 2 seconds\n        \n        self.get_logger().info('Whisper node started, listening...')\n    \n    def audio_capture_loop(self):\n        \"\"\"Capture audio from microphone in background thread\"\"\"\n        try:\n            stream = self.audio.open(\n                format=pyaudio.paInt16,\n                channels=1,\n                rate=self.sample_rate,\n                input=True,\n                frames_per_buffer=self.chunk_size\n            )\n            \n            self.get_logger().info('Microphone opened, capturing audio...')\n            \n            while rclpy.ok():\n                try:\n                    audio_data = stream.read(self.chunk_size, exception_on_overflow=False)\n                    audio_array = np.frombuffer(audio_data, dtype=np.int16).astype(np.float32) / 32768.0\n                    self.audio_queue.put(audio_array)\n                except Exception as e:\n                    self.get_logger().error(f'Audio capture error: {e}')\n                    \n        except Exception as e:\n            self.get_logger().error(f'Failed to open microphone: {e}')\n            self.get_logger().error('Make sure microphone is connected and permissions granted')\n    \n    def process_audio(self):\n        \"\"\"Process accumulated audio with Whisper\"\"\"\n        if self.audio_queue.qsize() < 10:  # Need enough audio chunks\n            return\n        \n        # Collect audio chunks\n        audio_chunks = []\n        while not self.audio_queue.empty() and len(audio_chunks) < 50:  # ~2 seconds\n            audio_chunks.append(self.audio_queue.get())\n        \n        if len(audio_chunks) == 0:\n            return\n        \n        # Concatenate audio\n        audio_data = np.concatenate(audio_chunks)\n        \n        # Check for voice activity (simple energy-based VAD)\n        energy = np.mean(audio_data ** 2)\n        vad_threshold = self.get_parameter('vad_threshold').value\n        \n        if energy < vad_threshold:\n            return  # No voice detected\n        \n        # Transcribe with Whisper\n        try:\n            result = self.model.transcribe(\n                audio_data,\n                language=self.language,\n                task='transcribe',\n                fp16=False  # Use FP32 for compatibility\n            )\n            \n            text = result['text'].strip()\n            \n            if text and len(text) > 2:  # Ignore very short transcriptions\n                self.get_logger().info(f'Transcribed: \"{text}\"')\n                \n                # Publish to ROS 2\n                msg = String()\n                msg.data = text\n                self.text_pub.publish(msg)\n                \n        except Exception as e:\n            self.get_logger().error(f'Whisper transcription error: {e}')\n    \n    def destroy_node(self):\n        \"\"\"Cleanup\"\"\"\n        self.audio.terminate()\n        super().destroy_node()\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = WhisperNode()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install dependencies"})," (",(0,r.jsx)(n.code,{children:"setup.py"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from setuptools import setup\n\nsetup(\n    name='voice_commands',\n    version='0.0.1',\n    packages=['voice_commands'],\n    install_requires=[\n        'setuptools',\n        'rclpy',\n        'openai-whisper',\n        'pyaudio',\n        'numpy',\n    ],\n    entry_points={\n        'console_scripts': [\n            'whisper_node = voice_commands.whisper_node:main',\n        ],\n    },\n)\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Install system dependencies"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install PyAudio dependencies\nsudo apt install portaudio19-dev python3-pyaudio\n\n# Install Python packages\npip3 install pyaudio numpy\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-4-build-and-launch",children:"Step 4: Build and Launch"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Build package"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"cd ~/isaac_ros_ws\ncolcon build --packages-select voice_commands\nsource install/setup.bash\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Launch node"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"ros2 run voice_commands whisper_node\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'[INFO] [whisper_node]: Loading Whisper model: base\n[INFO] [whisper_node]: Whisper model loaded\n[INFO] [whisper_node]: Microphone opened, capturing audio...\n[INFO] [whisper_node]: Whisper node started, listening...\n[INFO] [whisper_node]: Transcribed: "hello robot"\n[INFO] [whisper_node]: Transcribed: "pick up the cup"\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Verify topic"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 2: Echo transcribed text\nros2 topic echo /voice_commands/text\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-5-command-processing-node",children:"Step 5: Command Processing Node"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create command processor"}),": ",(0,r.jsx)(n.code,{children:"voice_commands/command_processor.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nProcess voice commands and extract intent\nROS 2 Humble | Python 3.10+\n\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport re\n\nclass CommandProcessor(Node):\n    \"\"\"\n    Processes transcribed text and extracts robot commands\n    \"\"\"\n    def __init__(self):\n        super().__init__('command_processor')\n        \n        # Subscribe to transcribed text\n        self.text_sub = self.create_subscription(\n            String,\n            '/voice_commands/text',\n            self.text_callback,\n            10\n        )\n        \n        # Publisher for processed commands\n        self.cmd_pub = self.create_publisher(String, '/voice_commands/command', 10)\n        \n        # Command patterns (simple keyword matching)\n        self.commands = {\n            'navigate': ['go to', 'move to', 'navigate', 'walk to'],\n            'pick': ['pick up', 'grab', 'take', 'get'],\n            'place': ['place', 'put', 'set down', 'drop'],\n            'stop': ['stop', 'halt', 'freeze'],\n            'come': ['come here', 'come to me', 'follow me'],\n        }\n        \n        self.get_logger().info('Command processor started')\n    \n    def text_callback(self, msg):\n        \"\"\"Process transcribed text and extract command\"\"\"\n        text = msg.data.lower()\n        \n        # Simple keyword matching (will be replaced with LLM in Chapter 4.2)\n        for command_type, keywords in self.commands.items():\n            for keyword in keywords:\n                if keyword in text:\n                    self.get_logger().info(f'Command detected: {command_type} from \"{text}\"')\n                    \n                    # Publish command\n                    cmd_msg = String()\n                    cmd_msg.data = f'{command_type}:{text}'\n                    self.cmd_pub.publish(cmd_msg)\n                    return\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = CommandProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Add to setup.py"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"entry_points={\n    'console_scripts': [\n        'whisper_node = voice_commands.whisper_node:main',\n        'command_processor = voice_commands.command_processor:main',\n    ],\n},\n"})}),"\n",(0,r.jsx)(n.h3,{id:"step-6-test-complete-pipeline",children:"Step 6: Test Complete Pipeline"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Launch both nodes"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Whisper node\nros2 run voice_commands whisper_node\n\n# Terminal 2: Command processor\nros2 run voice_commands command_processor\n\n# Terminal 3: Monitor commands\nros2 topic echo /voice_commands/command\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Test commands"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:'Say: "Go to the kitchen"'}),"\n",(0,r.jsx)(n.li,{children:'Say: "Pick up the cup"'}),"\n",(0,r.jsx)(n.li,{children:'Say: "Stop"'}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:'[INFO] [command_processor]: Command detected: navigate from "go to the kitchen"\ndata: "navigate:go to the kitchen"\n---\n[INFO] [command_processor]: Command detected: pick from "pick up the cup"\ndata: "pick:pick up the cup"\n'})}),"\n",(0,r.jsx)(n.h3,{id:"step-7-debugging-common-issues",children:"Step 7: Debugging Common Issues"}),"\n",(0,r.jsx)(n.h4,{id:"issue-1-no-module-named-whisper",children:"Issue 1: \"No module named 'whisper'\""}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Import error when launching node"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:'# Verify Whisper installed\npip3 show openai-whisper\n\n# Reinstall if needed\npip3 install --upgrade openai-whisper\n\n# Check Python path\npython3 -c "import whisper; print(whisper.__file__)"\n'})}),"\n",(0,r.jsx)(n.h4,{id:"issue-2-microphone-not-found-or-permission-denied",children:'Issue 2: "Microphone not found" or "Permission denied"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Audio capture fails"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# List audio devices\narecord -l\n\n# Test microphone\narecord -d 5 test.wav\n\n# Check permissions (Linux)\ngroups  # Should include 'audio' group\nsudo usermod -a -G audio $USER\n# Log out and back in\n\n# Check PulseAudio\npulseaudio --check\npulseaudio --start\n"})}),"\n",(0,r.jsx)(n.h4,{id:"issue-3-low-recognition-accuracy",children:'Issue 3: "Low recognition accuracy"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Whisper transcribes incorrectly"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use larger model\nmodel_name = 'small'  # Instead of 'base'\n\n# Specify language explicitly\nlanguage = 'en'  # English\n\n# Improve audio quality\n# Reduce background noise\n# Speak clearly and closer to microphone\n"})}),"\n",(0,r.jsx)(n.h4,{id:"issue-4-high-latency-or-delayed-transcription",children:'Issue 4: "High latency" or "Delayed transcription"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Long delay between speech and transcription"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use smaller model\nmodel_name = 'tiny'  # Faster but less accurate\n\n# Reduce processing interval\nprocess_timer = self.create_timer(1.0, self.process_audio)  # Process every 1 second\n\n# Optimize audio chunk size\nchunk_size = 2048  # Smaller chunks = faster processing\n"})}),"\n",(0,r.jsx)(n.h4,{id:"issue-5-memory-usage-too-high",children:'Issue 5: "Memory usage too high"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": System runs out of memory"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use smaller model\nmodel_name = 'tiny'  # 39M parameters vs. 769M for medium\n\n# Process audio in smaller batches\nwhile len(audio_chunks) < 25:  # Shorter audio segments\n\n# Clear audio queue periodically\nif self.audio_queue.qsize() > 100:\n    # Clear old audio\n    while not self.audio_queue.empty():\n        self.audio_queue.get()\n"})}),"\n",(0,r.jsx)(n.h2,{id:"part-3-advanced-topics-optional",children:"Part 3: Advanced Topics (Optional)"}),"\n",(0,r.jsx)(n.h3,{id:"streaming-whisper",children:"Streaming Whisper"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Real-time streaming"})," (lower latency):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use streaming API (if available)\n# Process audio chunks as they arrive\n# Instead of accumulating 2 seconds\n"})}),"\n",(0,r.jsx)(n.h3,{id:"multi-language-support",children:"Multi-Language Support"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Detect language automatically"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Whisper can detect language\nresult = self.model.transcribe(\n    audio_data,\n    language=None,  # Auto-detect\n    task='transcribe'\n)\n\ndetected_language = result['language']\nself.get_logger().info(f'Detected language: {detected_language}')\n"})}),"\n",(0,r.jsx)(n.h3,{id:"noise-reduction",children:"Noise Reduction"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Preprocess audio"})," (reduce noise):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import noisereduce as nr\n\n# Reduce noise before transcription\naudio_clean = nr.reduce_noise(\n    y=audio_data,\n    sr=self.sample_rate\n)\n\n# Then transcribe cleaned audio\nresult = self.model.transcribe(audio_clean, ...)\n"})}),"\n",(0,r.jsx)(n.h2,{id:"integration-with-capstone",children:"Integration with Capstone"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How this chapter contributes"})," to the Week 13 autonomous humanoid:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Voice input"}),": Capstone will accept natural language commands via Whisper"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Real-time processing"}),": Low-latency transcription enables responsive interaction"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Multi-language"}),": Supports international deployment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": Works with noise and accents (real-world conditions)"]}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Understanding Whisper integration now is essential for the capstone voice interface."}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"You learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 Installed ",(0,r.jsx)(n.strong,{children:"OpenAI Whisper"})," for speech recognition"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Created ",(0,r.jsx)(n.strong,{children:"ROS 2 node"})," for real-time voice transcription"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Implemented ",(0,r.jsx)(n.strong,{children:"command processing"})," pipeline"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Handled ",(0,r.jsx)(n.strong,{children:"audio capture"})," and preprocessing"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 Debugged ",(0,r.jsx)(n.strong,{children:"common audio and recognition issues"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next steps"}),": In Chapter 4.2, you'll integrate LLM-based planning to decompose voice commands into executable robot actions."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-basic-voice-recognition-required",children:"Exercise 1: Basic Voice Recognition (Required)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Set up Whisper and verify transcription accuracy."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Install Whisper and test with audio file"}),"\n",(0,r.jsx)(n.li,{children:"Record 10 voice commands"}),"\n",(0,r.jsx)(n.li,{children:"Transcribe each command"}),"\n",(0,r.jsx)(n.li,{children:"Measure accuracy (correct words / total words)"}),"\n",(0,r.jsx)(n.li,{children:"Test with different Whisper models (tiny, base, small)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Whisper installed and working"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","10 commands transcribed"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Accuracy > 80% for clear speech"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Comparison of model performance documented"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 90 minutes"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-ros-2-integration-required",children:"Exercise 2: ROS 2 Integration (Required)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Integrate Whisper with ROS 2 for real-time commands."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create Whisper ROS 2 node"}),"\n",(0,r.jsx)(n.li,{children:"Configure microphone input"}),"\n",(0,r.jsx)(n.li,{children:"Publish transcribed text to ROS 2 topic"}),"\n",(0,r.jsx)(n.li,{children:"Create command processor node"}),"\n",(0,r.jsx)(n.li,{children:"Test end-to-end pipeline"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Whisper node publishes to ",(0,r.jsx)(n.code,{children:"/voice_commands/text"})]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Command processor extracts commands"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Commands published to ",(0,r.jsx)(n.code,{children:"/voice_commands/command"})]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Latency < 3 seconds from speech to command"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-noise-robustness-test-challenge",children:"Exercise 3: Noise Robustness Test (Challenge)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Test Whisper accuracy with different noise levels."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Record commands in quiet environment (baseline)"}),"\n",(0,r.jsx)(n.li,{children:"Record same commands with background noise"}),"\n",(0,r.jsx)(n.li,{children:"Record with different microphone distances"}),"\n",(0,r.jsx)(n.li,{children:"Compare accuracy across conditions"}),"\n",(0,r.jsx)(n.li,{children:"Implement noise reduction if needed"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Metrics"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Word Error Rate (WER) for each condition"}),"\n",(0,r.jsx)(n.li,{children:"Recognition latency"}),"\n",(0,r.jsx)(n.li,{children:"Command extraction success rate"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 180 minutes"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper GitHub"})," - Official repository"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://arxiv.org/abs/2212.04356",children:"Whisper Paper"})," - Research paper"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://people.csail.mit.edu/hubert/pyaudio/docs/",children:"PyAudio Documentation"})," - Audio I/O library"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"http://wiki.ros.org/audio_common",children:"ROS 2 Audio Common"})," - ROS 2 audio packages"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": [Chapter 4.2: LLM-Based Cognitive Planning \u2192](chapter-4 to 2.md)"]})]})}function h(e={}){const{wrapper:n}={...(0,o.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>l});var i=s(6540);const r={},o=i.createContext(r);function t(e){const n=i.useContext(o);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:t(e.components),i.createElement(o.Provider,{value:n},e.children)}}}]);
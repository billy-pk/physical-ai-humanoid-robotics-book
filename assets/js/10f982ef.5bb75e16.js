"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[2784],{2570:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>d,contentTitle:()=>l,default:()=>h,frontMatter:()=>t,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/intro","title":"Module 4 Introduction","description":"Duration: Weeks 11 to 13 (3 weeks)","source":"@site/docs/module-4/intro.md","sourceDirName":"module-4","slug":"/module-4/intro","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-4/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Module 4 Introduction"},"sidebar":"defaultSidebar","previous":{"title":"3.5 Sim-to-Real Transfer Workflows","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-5"},"next":{"title":"4.1 Voice-to-Action with OpenAI Whisper","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-1"}}');var r=i(4848),o=i(8453);const t={sidebar_position:1,title:"Module 4 Introduction"},l="Module 4: Vision-Language-Action (VLA)",d={},c=[{value:"What You&#39;ll Build",id:"what-youll-build",level:2},{value:"Module Overview",id:"module-overview",level:2},{value:"Learning Path",id:"learning-path",level:3},{value:"Tools &amp; Technologies",id:"tools--technologies",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Week-by-Week Timeline",id:"week-by-week-timeline",level:2},{value:"Assessment (30% of final grade - Capstone Project)",id:"assessment-30-of-final-grade---capstone-project",level:2},{value:"Real-World Applications",id:"real-world-applications",level:2},{value:"Success Stories: What Students Built",id:"success-stories-what-students-built",level:2},{value:"Why VLA Over Traditional Programming?",id:"why-vla-over-traditional-programming",level:2},{value:"Getting Help",id:"getting-help",level:2},{value:"Ready to Start?",id:"ready-to-start",level:2}];function a(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,o.R)(),...n.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(e.header,{children:(0,r.jsx)(e.h1,{id:"module-4-vision-language-action-vla",children:"Module 4: Vision-Language-Action (VLA)"})}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Duration"}),": Weeks 11 to 13 (3 weeks)\n",(0,r.jsx)(e.strong,{children:"Focus"}),": Integrating voice commands, LLM-based planning, and multi-modal interaction to create an autonomous humanoid robot that understands natural language and executes complex tasks"]}),"\n",(0,r.jsx)(e.h2,{id:"what-youll-build",children:"What You'll Build"}),"\n",(0,r.jsx)(e.p,{children:"By the end of this module, you will have created:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Voice command system"})," using OpenAI Whisper for speech-to-text"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LLM-based task planner"})," using GPT-4 or open-source models for natural language understanding"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Natural language to ROS 2 Actions"})," translator converting commands to robot behaviors"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Multi-modal integration"})," combining speech, vision, and gesture recognition"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Complete autonomous humanoid system"})," demonstrating end-to-end VLA pipeline"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Module Project (Capstone)"}),": An autonomous humanoid robot that:"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'Understands voice commands ("Pick up the red cup and place it on the table")'}),"\n",(0,r.jsx)(e.li,{children:"Plans complex tasks using LLM reasoning"}),"\n",(0,r.jsx)(e.li,{children:"Executes actions via ROS 2 (navigation, manipulation, interaction)"}),"\n",(0,r.jsx)(e.li,{children:"Integrates vision, language, and action in a unified system"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Vision-Language-Action (VLA)"})," is the cutting-edge paradigm for humanoid robotics. By combining computer vision, natural language processing, and robot control, VLA enables robots to understand human intent and execute complex, multi-step tasks autonomously."]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Why VLA matters for Physical AI"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Natural Interaction"}),": Humans communicate via speech, not code"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Complex Reasoning"}),": LLMs decompose high-level goals into executable steps"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Multi-Modal Understanding"}),": Vision + language = richer world understanding"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Generalization"}),": Same system handles diverse tasks without reprogramming"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Industry Adoption"}),": Used by Figure AI, Tesla Optimus, Boston Dynamics Atlas"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA Pipeline"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Voice Input"}),': "Pick up the cup and bring it to me"']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Speech-to-Text"}),": Whisper converts audio \u2192 text"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LLM Planning"}),': GPT-4 decomposes task \u2192 ["navigate to cup", "grasp cup", "navigate to human", "release cup"]']}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Action Execution"}),": ROS 2 Actions execute each step"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Feedback Loop"}),": Vision confirms completion, LLM adjusts plan if needed"]}),"\n"]}),"\n",(0,r.jsx)(e.h3,{id:"learning-path",children:"Learning Path"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Chapter 4.1: Voice-to-Action with OpenAI Whisper"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Install and configure OpenAI Whisper for speech recognition"}),"\n",(0,r.jsx)(e.li,{children:"Integrate Whisper with ROS 2 for real-time voice commands"}),"\n",(0,r.jsx)(e.li,{children:"Handle audio preprocessing and noise reduction"}),"\n",(0,r.jsx)(e.li,{children:"Process multi-language voice input"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Chapter 4.2: LLM-Based Cognitive Planning"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Set up GPT-4 or open-source LLM (Llama, Mistral) for task planning"}),"\n",(0,r.jsx)(e.li,{children:"Implement prompt engineering for robot task decomposition"}),"\n",(0,r.jsx)(e.li,{children:"Create planning pipeline: goal \u2192 sub-tasks \u2192 executable actions"}),"\n",(0,r.jsx)(e.li,{children:"Handle ambiguous commands and error recovery"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Chapter 4.3: Natural Language to ROS 2 Actions"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Map natural language commands to ROS 2 Action goals"}),"\n",(0,r.jsx)(e.li,{children:"Implement action executor coordinating multiple behaviors"}),"\n",(0,r.jsx)(e.li,{children:"Handle action preconditions and postconditions"}),"\n",(0,r.jsx)(e.li,{children:"Create feedback mechanisms for action completion"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Chapter 4.4: Multi-Modal Integration (Speech + Vision + Gesture)"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Combine voice commands with visual perception"}),"\n",(0,r.jsx)(e.li,{children:"Implement gesture recognition for human-robot interaction"}),"\n",(0,r.jsx)(e.li,{children:"Fuse multi-modal inputs for robust understanding"}),"\n",(0,r.jsx)(e.li,{children:"Create unified perception-action pipeline"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Chapter 4.5: Humanoid Kinematics & Balance Control"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Understand forward and inverse kinematics for humanoids"}),"\n",(0,r.jsx)(e.li,{children:"Implement balance control algorithms (ZMP, LIPM)"}),"\n",(0,r.jsx)(e.li,{children:"Configure walking gaits and manipulation poses"}),"\n",(0,r.jsx)(e.li,{children:"Integrate with ROS 2 control stack"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Capstone Project: Autonomous Humanoid System"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Integrate all modules (Whisper + LLM + ROS 2 + Navigation + Manipulation)"}),"\n",(0,r.jsx)(e.li,{children:"Demonstrate end-to-end VLA pipeline"}),"\n",(0,r.jsx)(e.li,{children:"Handle complex, multi-step tasks"}),"\n",(0,r.jsx)(e.li,{children:"Present complete system demonstration"}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"tools--technologies",children:"Tools & Technologies"}),"\n",(0,r.jsx)(e.p,{children:"You will use:"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"OpenAI Whisper"}),": Speech-to-text model - ",(0,r.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"GitHub"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GPT-4 API"})," or ",(0,r.jsx)(e.strong,{children:"Open-source LLMs"}),": Task planning (Llama 3, Mistral, Claude)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"LangChain"}),": LLM orchestration framework - ",(0,r.jsx)(e.a,{href:"https://python.langchain.com/",children:"Documentation"})]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Humble"}),": Robot control (from Modules 1 to 3)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Python 3.10+"}),": Primary development language"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"PyTorch/TensorFlow"}),": For local LLM inference (optional)"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"Installation guides provided in Chapter 4.1."}),"\n",(0,r.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(e.p,{children:"From Module 1 (Weeks 3 to 5):"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Humble"})," with rclpy experience"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"ROS 2 Actions"})," understanding"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"From Module 2 (Weeks 6 to 7):"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Gazebo simulation"})," experience"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sensor integration"})," (cameras, LiDAR)"]}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:"From Module 3 (Weeks 8 to 10):"}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Isaac Sim"})," or ",(0,r.jsx)(e.strong,{children:"Gazebo"})," simulation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"VSLAM"})," and ",(0,r.jsx)(e.strong,{children:"Nav2"})," navigation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Sim-to-real"})," understanding"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"New Requirements"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Python 3.10+"})," with pip"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"OpenAI API key"})," (for GPT-4) or ",(0,r.jsx)(e.strong,{children:"local LLM setup"})," (for open-source models)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Microphone"})," for voice input (or simulated audio)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Basic NLP knowledge"}),": Prompts, tokens, embeddings (we'll cover as needed)"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Don't worry if you're rusty"}),"\u2014we review key concepts as needed!"]}),"\n",(0,r.jsx)(e.h2,{id:"week-by-week-timeline",children:"Week-by-Week Timeline"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Week 11: Voice & Language"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Chapter 4.1: Voice-to-Action with OpenAI Whisper"}),"\n",(0,r.jsx)(e.li,{children:"Chapter 4.2: LLM-Based Cognitive Planning"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Week 12: Integration & Control"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Chapter 4.3: Natural Language to ROS 2 Actions"}),"\n",(0,r.jsx)(e.li,{children:"Chapter 4.4: Multi-Modal Integration (Speech + Vision + Gesture)"}),"\n"]}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Week 13: Capstone Project"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Chapter 4.5: Humanoid Kinematics & Balance Control"}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Capstone Project"}),": Complete autonomous humanoid system demonstration"]}),"\n"]}),"\n",(0,r.jsx)(e.h2,{id:"assessment-30-of-final-grade---capstone-project",children:"Assessment (30% of final grade - Capstone Project)"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Project"}),": Autonomous Humanoid VLA System"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Functional"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Voice command system accepting natural language"}),"\n",(0,r.jsx)(e.li,{children:"LLM-based task planning decomposing complex goals"}),"\n",(0,r.jsx)(e.li,{children:"ROS 2 Action execution (navigation, manipulation, interaction)"}),"\n",(0,r.jsx)(e.li,{children:"Multi-modal integration (voice + vision + gesture)"}),"\n",(0,r.jsx)(e.li,{children:"End-to-end demonstration of 3+ complex tasks"}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.li,{children:["\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Technical"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Complete ROS 2 package with proper structure"}),"\n",(0,r.jsx)(e.li,{children:"Whisper integration for speech recognition"}),"\n",(0,r.jsx)(e.li,{children:"LLM integration (GPT-4 or open-source) for planning"}),"\n",(0,r.jsx)(e.li,{children:"ROS 2 Action servers for robot behaviors"}),"\n",(0,r.jsx)(e.li,{children:"Multi-modal fusion pipeline"}),"\n",(0,r.jsx)(e.li,{children:"Comprehensive README with setup and usage"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Deliverables"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"GitHub Repository"}),":","\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/src"}),": Python nodes (Whisper, LLM, action executor)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/launch"}),": ROS 2 launch files"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/config"}),": LLM prompts, action mappings"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/docs"}),": System architecture, API documentation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/README.md"}),": Complete setup guide"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.code,{children:"/demo_video.mp4"}),": System demonstration (7 to 10 minutes)"]}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Video Demo Must Show"}),":"]}),"\n",(0,r.jsxs)(e.ol,{children:["\n",(0,r.jsx)(e.li,{children:'Voice command input ("Pick up the red cup")'}),"\n",(0,r.jsx)(e.li,{children:"LLM task decomposition (showing planned steps)"}),"\n",(0,r.jsx)(e.li,{children:"ROS 2 Action execution (navigation, manipulation)"}),"\n",(0,r.jsx)(e.li,{children:"Multi-modal feedback (vision confirming actions)"}),"\n",(0,r.jsx)(e.li,{children:"Error handling and recovery"}),"\n",(0,r.jsx)(e.li,{children:"Complete end-to-end task execution"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Grading Rubric"}),":"]}),"\n",(0,r.jsxs)(e.table,{children:[(0,r.jsx)(e.thead,{children:(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.th,{children:"Criterion"}),(0,r.jsx)(e.th,{children:"Excellent (90 to 100%)"}),(0,r.jsx)(e.th,{children:"Good (75 to 89%)"}),(0,r.jsx)(e.th,{children:"Needs Work (less than 75%)"})]})}),(0,r.jsxs)(e.tbody,{children:[(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Functionality"})}),(0,r.jsx)(e.td,{children:"All systems working, handles complex tasks, robust error handling"}),(0,r.jsx)(e.td,{children:"Most features working, handles simple tasks"}),(0,r.jsx)(e.td,{children:"Missing features, frequent errors"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"LLM Integration"})}),(0,r.jsx)(e.td,{children:"Sophisticated prompts, robust planning, handles ambiguity"}),(0,r.jsx)(e.td,{children:"Basic prompts, simple planning"}),(0,r.jsx)(e.td,{children:"Poor planning, frequent failures"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Voice Recognition"})}),(0,r.jsx)(e.td,{children:"High accuracy, handles noise, multi-language"}),(0,r.jsx)(e.td,{children:"Good accuracy, basic noise handling"}),(0,r.jsx)(e.td,{children:"Low accuracy, frequent errors"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Action Execution"})}),(0,r.jsx)(e.td,{children:"Smooth execution, proper error handling, feedback loops"}),(0,r.jsx)(e.td,{children:"Mostly smooth, basic error handling"}),(0,r.jsx)(e.td,{children:"Choppy execution, poor error handling"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Multi-Modal"})}),(0,r.jsx)(e.td,{children:"Seamless integration, robust fusion"}),(0,r.jsx)(e.td,{children:"Basic integration, some gaps"}),(0,r.jsx)(e.td,{children:"Poor integration, missing modalities"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Code Quality"})}),(0,r.jsx)(e.td,{children:"Clean, well-documented, modular architecture"}),(0,r.jsx)(e.td,{children:"Readable, some documentation"}),(0,r.jsx)(e.td,{children:"Hard to understand, poor structure"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Documentation"})}),(0,r.jsx)(e.td,{children:"Complete setup guide, architecture diagrams, API docs"}),(0,r.jsx)(e.td,{children:"Basic instructions, some diagrams"}),(0,r.jsx)(e.td,{children:"Incomplete or confusing"})]}),(0,r.jsxs)(e.tr,{children:[(0,r.jsx)(e.td,{children:(0,r.jsx)(e.strong,{children:"Demo"})}),(0,r.jsx)(e.td,{children:"Professional video, showcases all features, clear explanation"}),(0,r.jsx)(e.td,{children:"Shows main features, acceptable quality"}),(0,r.jsx)(e.td,{children:"Unclear or missing key features"})]})]})]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Submission"}),":\nSubmit via course LMS by ",(0,r.jsx)(e.strong,{children:"end of Week 13"}),".\nLate penalty: -10% per day (max 3 days late)."]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"What you'll be able to build after this module"}),":"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Autonomous Humanoid Assistants"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'"Bring me a glass of water" \u2192 Robot navigates, grasps cup, fills water, brings to human'}),"\n",(0,r.jsx)(e.li,{children:'"Clean up the toys in the living room" \u2192 Robot identifies toys, picks them up, places in box'}),"\n",(0,r.jsx)(e.li,{children:'"Help me find my keys" \u2192 Robot searches environment, identifies keys, reports location'}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Industrial Humanoid Robots"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:'"Inspect the machinery for defects" \u2192 Robot navigates, uses vision to inspect, reports findings'}),"\n",(0,r.jsx)(e.li,{children:'"Move the boxes from warehouse A to B" \u2192 Robot plans path, loads boxes, transports, unloads'}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Research Platforms"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Test new VLA architectures"}),"\n",(0,r.jsx)(e.li,{children:"Evaluate LLM planning strategies"}),"\n",(0,r.jsx)(e.li,{children:"Develop multi-modal fusion algorithms"}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"success-stories-what-students-built",children:"Success Stories: What Students Built"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Week 11 Milestone"}),": Voice commands recognized, LLM planning working, basic actions executed"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Week 12 Milestone"}),": Multi-modal integration complete, complex tasks decomposed and executed"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Week 13 Milestone"}),": Complete autonomous humanoid system demonstrating end-to-end VLA\u2014ready for deployment!"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"why-vla-over-traditional-programming",children:"Why VLA Over Traditional Programming?"}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Traditional approach"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Hardcode every behavior"}),"\n",(0,r.jsx)(e.li,{children:"Limited to predefined scenarios"}),"\n",(0,r.jsx)(e.li,{children:"Requires reprogramming for new tasks"}),"\n",(0,r.jsx)(e.li,{children:"Brittle error handling"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"VLA approach"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsx)(e.li,{children:"Natural language commands"}),"\n",(0,r.jsx)(e.li,{children:"Handles novel scenarios via LLM reasoning"}),"\n",(0,r.jsx)(e.li,{children:"Adapts to new tasks without code changes"}),"\n",(0,r.jsx)(e.li,{children:"Robust error recovery via LLM replanning"}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Industry examples"}),":"]}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Figure AI"}),": Humanoid robots controlled via natural language"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Tesla Optimus"}),": LLM-based task planning for humanoid"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"Boston Dynamics"}),": Exploring VLA for Atlas humanoid"]}),"\n"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"getting-help",children:"Getting Help"}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Stuck on VLA implementation?"})}),"\n",(0,r.jsxs)(e.ul,{children:["\n",(0,r.jsxs)(e.li,{children:["Check ",(0,r.jsx)(e.strong,{children:"Chapter X.X Debugging Sections"})," (every chapter includes 3 to 4 common issues)"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"https://github.com/openai/whisper",children:"OpenAI Whisper Documentation"})," - Speech recognition"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"https://python.langchain.com/",children:"LangChain Documentation"})," - LLM orchestration"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.a,{href:"https://docs.ros.org/en/humble/Tutorials/Intermediate/Writing-an-Action-Server-Client/Py.html",children:"ROS 2 Actions Tutorial"})," - Action implementation"]}),"\n",(0,r.jsxs)(e.li,{children:[(0,r.jsx)(e.strong,{children:"AI Book Assistant"})," (bottom-right corner) - Trained on this course content"]}),"\n"]}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Office Hours"}),": See course schedule for TA support"]}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsx)(e.h2,{id:"ready-to-start",children:"Ready to Start?"}),"\n",(0,r.jsx)(e.p,{children:"This module integrates everything from Modules 1 to 3 into a complete autonomous system. You'll build the AI-robot brain that enables natural human-robot interaction and complex task execution."}),"\n",(0,r.jsx)(e.p,{children:(0,r.jsx)(e.strong,{children:"Let's build the future of humanoid robotics."})}),"\n",(0,r.jsx)(e.hr,{}),"\n",(0,r.jsxs)(e.p,{children:[(0,r.jsx)(e.strong,{children:"Next"}),": [Chapter 4.1: Voice-to-Action with OpenAI Whisper \u2192](chapter-4 to 1.md)"]})]})}function h(n={}){const{wrapper:e}={...(0,o.R)(),...n.components};return e?(0,r.jsx)(e,{...n,children:(0,r.jsx)(a,{...n})}):a(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>t,x:()=>l});var s=i(6540);const r={},o=s.createContext(r);function t(n){const e=s.useContext(o);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(r):n.components||r:t(n.components),s.createElement(o.Provider,{value:e},n.children)}}}]);
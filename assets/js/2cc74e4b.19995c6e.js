"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[6083],{2929:(n,e,t)=>{t.r(e),t.d(e,{assets:()=>i,contentTitle:()=>l,default:()=>p,frontMatter:()=>a,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"module-4/chapter-4-2","title":"4.2 LLM-Based Cognitive Planning","description":"Large Language Models (LLMs) enable robots to understand natural language and decompose complex goals into executable steps. This chapter covers integrating GPT-4 or open-source LLMs (Llama, Mistral) with ROS 2 for task planning, prompt engineering, and handling ambiguous commands.","source":"@site/docs/module-4/chapter-4-2.md","sourceDirName":"module-4","slug":"/module-4/chapter-4-2","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-2","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-4/chapter-4-2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"4.2 LLM-Based Cognitive Planning"},"sidebar":"defaultSidebar","previous":{"title":"4.1 Voice-to-Action with OpenAI Whisper","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-1"},"next":{"title":"4.3 Natural Language to ROS 2 Actions","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-3"}}');var o=t(4848),r=t(8453);const a={sidebar_position:3,title:"4.2 LLM-Based Cognitive Planning"},l="Chapter 4.2: LLM-Based Cognitive Planning",i={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: LLM Planning Fundamentals",id:"part-1-llm-planning-fundamentals",level:2},{value:"What is LLM-Based Planning?",id:"what-is-llm-based-planning",level:3},{value:"LLM Options",id:"llm-options",level:3},{value:"Planning Pipeline",id:"planning-pipeline",level:3},{value:"Part 2: Hands-On Tutorial",id:"part-2-hands-on-tutorial",level:2},{value:"Project: LLM Task Planner for Humanoid Robot",id:"project-llm-task-planner-for-humanoid-robot",level:3},{value:"Step 1: Set Up OpenAI API (Option A)",id:"step-1-set-up-openai-api-option-a",level:3},{value:"Step 2: Set Up Local LLM (Option B - Llama 3)",id:"step-2-set-up-local-llm-option-b---llama-3",level:3},{value:"Step 3: Create LLM Planner Node",id:"step-3-create-llm-planner-node",level:3},{value:"Step 4: Test LLM Planning",id:"step-4-test-llm-planning",level:3},{value:"Step 5: Enhanced Prompt Engineering",id:"step-5-enhanced-prompt-engineering",level:3},{value:"Step 6: Handle Ambiguity and Context",id:"step-6-handle-ambiguity-and-context",level:3},{value:"Step 7: Error Recovery and Replanning",id:"step-7-error-recovery-and-replanning",level:3},{value:"Step 8: Debugging Common Issues",id:"step-8-debugging-common-issues",level:3},{value:"Issue 1: &quot;API key not found&quot; or &quot;Rate limit exceeded&quot;",id:"issue-1-api-key-not-found-or-rate-limit-exceeded",level:4},{value:"Issue 2: &quot;LLM returns invalid JSON&quot;",id:"issue-2-llm-returns-invalid-json",level:4},{value:"Issue 3: &quot;Planning too slow&quot; (high latency)",id:"issue-3-planning-too-slow-high-latency",level:4},{value:"Issue 4: &quot;Plan doesn&#39;t match command intent&quot;",id:"issue-4-plan-doesnt-match-command-intent",level:4},{value:"Part 3: Advanced Topics (Optional)",id:"part-3-advanced-topics-optional",level:2},{value:"Few-Shot Learning",id:"few-shot-learning",level:3},{value:"Function Calling (Structured Output)",id:"function-calling-structured-output",level:3},{value:"Integration with Capstone",id:"integration-with-capstone",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic LLM Planning (Required)",id:"exercise-1-basic-llm-planning-required",level:3},{value:"Exercise 2: Prompt Engineering (Required)",id:"exercise-2-prompt-engineering-required",level:3},{value:"Exercise 3: Context-Aware Planning (Challenge)",id:"exercise-3-context-aware-planning-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function d(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"chapter-42-llm-based-cognitive-planning",children:"Chapter 4.2: LLM-Based Cognitive Planning"})}),"\n",(0,o.jsx)(e.p,{children:"Large Language Models (LLMs) enable robots to understand natural language and decompose complex goals into executable steps. This chapter covers integrating GPT-4 or open-source LLMs (Llama, Mistral) with ROS 2 for task planning, prompt engineering, and handling ambiguous commands."}),"\n",(0,o.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,o.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Set up"})," GPT-4 API or local LLM (Llama, Mistral) for task planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Implement"})," prompt engineering for robot task decomposition"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Create"})," planning pipeline: goal \u2192 sub-tasks \u2192 executable actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Handle"})," ambiguous commands and error recovery via LLM reasoning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Integrate"})," LLM planning with ROS 2 action system"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Chapter 4.1"})," completed (Whisper voice recognition)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"OpenAI API key"})," (for GPT-4) OR ",(0,o.jsx)(e.strong,{children:"local LLM setup"})," (for open-source models)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Python 3.10+"})," with pip"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Basic NLP concepts"}),": Prompts, tokens, embeddings (we'll cover as needed)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Understanding"})," of ROS 2 Actions (from Module 1)"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"part-1-llm-planning-fundamentals",children:"Part 1: LLM Planning Fundamentals"}),"\n",(0,o.jsx)(e.h3,{id:"what-is-llm-based-planning",children:"What is LLM-Based Planning?"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"LLM planning"})," uses language models to:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Understand"}),' natural language goals ("Pick up the red cup and place it on the table")']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Decompose"})," complex tasks into sub-tasks"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Generate"})," executable action sequences"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Handle"}),' ambiguity and context ("the cup" \u2192 which cup?)']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Recover"})," from errors by replanning"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:(0,o.jsx)(e.strong,{children:"Why LLMs for robot planning?"})}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural language"}),": No need to program every scenario"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Reasoning"}),": LLMs can infer implicit steps"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Generalization"}),": Handles novel situations"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context awareness"}),': Understands "the cup" refers to previously mentioned object']}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"llm-options",children:"LLM Options"}),"\n",(0,o.jsxs)(e.table,{children:[(0,o.jsx)(e.thead,{children:(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.th,{children:"Model"}),(0,o.jsx)(e.th,{children:"Type"}),(0,o.jsx)(e.th,{children:"Cost"}),(0,o.jsx)(e.th,{children:"Latency"}),(0,o.jsx)(e.th,{children:"Use Case"})]})}),(0,o.jsxs)(e.tbody,{children:[(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"GPT-4"})}),(0,o.jsx)(e.td,{children:"API (OpenAI)"}),(0,o.jsx)(e.td,{children:"$$$"}),(0,o.jsx)(e.td,{children:"Low"}),(0,o.jsx)(e.td,{children:"Production, high accuracy"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"GPT-3.5-turbo"})}),(0,o.jsx)(e.td,{children:"API (OpenAI)"}),(0,o.jsx)(e.td,{children:"$"}),(0,o.jsx)(e.td,{children:"Low"}),(0,o.jsx)(e.td,{children:"Development, good accuracy"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Claude 3"})}),(0,o.jsx)(e.td,{children:"API (Anthropic)"}),(0,o.jsx)(e.td,{children:"$$"}),(0,o.jsx)(e.td,{children:"Low"}),(0,o.jsx)(e.td,{children:"Alternative to GPT-4"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Llama 3"})}),(0,o.jsx)(e.td,{children:"Local (Meta)"}),(0,o.jsx)(e.td,{children:"Free"}),(0,o.jsx)(e.td,{children:"Medium"}),(0,o.jsx)(e.td,{children:"Privacy, offline"})]}),(0,o.jsxs)(e.tr,{children:[(0,o.jsx)(e.td,{children:(0,o.jsx)(e.strong,{children:"Mistral"})}),(0,o.jsx)(e.td,{children:"Local/API"}),(0,o.jsx)(e.td,{children:"Free/$"}),(0,o.jsx)(e.td,{children:"Medium"}),(0,o.jsx)(e.td,{children:"Open-source alternative"})]})]})]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"This chapter covers"}),": GPT-4 API (easiest) and Llama 3 local (privacy-focused)."]}),"\n",(0,o.jsx)(e.h3,{id:"planning-pipeline",children:"Planning Pipeline"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Typical pipeline"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Voice Input"}),': "Pick up the red cup and bring it to me"']}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"LLM Prompt"}),": System prompt + user command"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task Decomposition"}),": LLM generates step-by-step plan"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action Extraction"}),": Parse LLM output \u2192 ROS 2 Actions"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Execution"}),": Execute actions sequentially"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Feedback"}),": LLM replans if action fails"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"part-2-hands-on-tutorial",children:"Part 2: Hands-On Tutorial"}),"\n",(0,o.jsx)(e.h3,{id:"project-llm-task-planner-for-humanoid-robot",children:"Project: LLM Task Planner for Humanoid Robot"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Goal"}),": Set up LLM (GPT-4 or Llama) to decompose voice commands into executable robot actions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Tools"}),": OpenAI API (or local LLM), LangChain, ROS 2 Humble, Python 3.10+"]}),"\n",(0,o.jsx)(e.h3,{id:"step-1-set-up-openai-api-option-a",children:"Step 1: Set Up OpenAI API (Option A)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Get API key"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsxs)(e.li,{children:["Visit: ",(0,o.jsx)(e.a,{href:"https://platform.openai.com/api-keys",children:"https://platform.openai.com/api-keys"})]}),"\n",(0,o.jsx)(e.li,{children:"Create account (or sign in)"}),"\n",(0,o.jsx)(e.li,{children:"Generate API key"}),"\n",(0,o.jsx)(e.li,{children:"Save key securely (don't commit to git!)"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Install OpenAI Python library"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip3 install openai python-dotenv\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Create environment file"}),": ",(0,o.jsx)(e.code,{children:".env"})]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"OPENAI_API_KEY=sk-your-api-key-here\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Test API connection"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nTest OpenAI API connection\n"""\nimport os\nfrom dotenv import load_dotenv\nfrom openai import OpenAI\n\nload_dotenv()\n\nclient = OpenAI(api_key=os.getenv(\'OPENAI_API_KEY\'))\n\nresponse = client.chat.completions.create(\n    model="gpt-4",\n    messages=[\n        {"role": "user", "content": "Say hello"}\n    ]\n)\n\nprint(response.choices[0].message.content)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-2-set-up-local-llm-option-b---llama-3",children:"Step 2: Set Up Local LLM (Option B - Llama 3)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Install Ollama"})," (easy local LLM runner):"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:'# Install Ollama\ncurl -fsSL https://ollama.com/install.sh | sh\n\n# Pull Llama 3 model\nollama pull llama3\n\n# Test\nollama run llama3 "Say hello"\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Install Python client"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"pip3 install ollama\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Test connection"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"import ollama\n\nresponse = ollama.chat(model='llama3', messages=[\n    {'role': 'user', 'content': 'Say hello'}\n])\n\nprint(response['message']['content'])\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-3-create-llm-planner-node",children:"Step 3: Create LLM Planner Node"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Create planner node"}),": ",(0,o.jsx)(e.code,{children:"voice_commands/llm_planner.py"})]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nLLM-based task planner for humanoid robot\nROS 2 Humble | Python 3.10+ | GPT-4 or Llama\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\nimport os\nfrom dotenv import load_dotenv\n\n# Try OpenAI first, fallback to Ollama\ntry:\n    from openai import OpenAI\n    OPENAI_AVAILABLE = True\nexcept ImportError:\n    OPENAI_AVAILABLE = False\n\ntry:\n    import ollama\n    OLLAMA_AVAILABLE = True\nexcept ImportError:\n    OLLAMA_AVAILABLE = False\n\nclass LLMPlanner(Node):\n    """\n    Uses LLM to decompose natural language commands into robot actions\n    """\n    def __init__(self):\n        super().__init__(\'llm_planner\')\n        \n        # Parameters\n        self.declare_parameter(\'llm_provider\', \'openai\')  # \'openai\' or \'ollama\'\n        self.declare_parameter(\'model\', \'gpt-4\')  # Model name\n        \n        provider = self.get_parameter(\'llm_provider\').value\n        model_name = self.get_parameter(\'model\').value\n        \n        # Initialize LLM client\n        if provider == \'openai\' and OPENAI_AVAILABLE:\n            load_dotenv()\n            api_key = os.getenv(\'OPENAI_API_KEY\')\n            if not api_key:\n                self.get_logger().error(\'OPENAI_API_KEY not found in .env file\')\n                raise ValueError(\'OpenAI API key required\')\n            self.client = OpenAI(api_key=api_key)\n            self.model = model_name\n            self.provider = \'openai\'\n            self.get_logger().info(f\'Using OpenAI model: {self.model}\')\n        elif provider == \'ollama\' and OLLAMA_AVAILABLE:\n            self.client = ollama\n            self.model = model_name if model_name != \'gpt-4\' else \'llama3\'\n            self.provider = \'ollama\'\n            self.get_logger().info(f\'Using Ollama model: {self.model}\')\n        else:\n            self.get_logger().error(\'No LLM provider available. Install openai or ollama.\')\n            raise RuntimeError(\'LLM provider not available\')\n        \n        # System prompt for robot planning\n        self.system_prompt = """You are a task planner for a humanoid robot. \nYour job is to decompose natural language commands into a sequence of executable robot actions.\n\nAvailable actions:\n- navigate_to(location): Move robot to specified location\n- pick_up(object): Grasp and lift an object\n- place(object, location): Put object at location\n- look_at(object): Turn head/camera toward object\n- speak(text): Say something to the human\n- wait(duration): Wait for specified seconds\n\nOutput format: JSON array of actions, each with "action" and "parameters" fields.\nExample:\nInput: "Pick up the red cup and place it on the table"\nOutput: [\n  {"action": "look_at", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "red cup"}},\n  {"action": "pick_up", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "table"}},\n  {"action": "place", "parameters": {"object": "red cup", "location": "table"}}\n]\n\nBe precise and break down complex tasks into clear steps."""\n        \n        # Subscribe to voice commands\n        self.cmd_sub = self.create_subscription(\n            String,\n            \'/voice_commands/command\',\n            self.command_callback,\n            10\n        )\n        \n        # Publisher for planned actions\n        self.plan_pub = self.create_publisher(String, \'/voice_commands/plan\', 10)\n        \n        self.get_logger().info(\'LLM planner started, ready to plan tasks\')\n    \n    def command_callback(self, msg):\n        """Process voice command and generate plan"""\n        command = msg.data\n        \n        # Extract command text (remove prefix like "navigate:")\n        if \':\' in command:\n            command = command.split(\':\', 1)[1]\n        \n        self.get_logger().info(f\'Planning for command: "{command}"\')\n        \n        # Generate plan using LLM\n        plan = self.generate_plan(command)\n        \n        if plan:\n            # Publish plan\n            plan_msg = String()\n            plan_msg.data = json.dumps(plan)\n            self.plan_pub.publish(plan_msg)\n            \n            self.get_logger().info(f\'Generated plan: {len(plan)} actions\')\n            for i, action in enumerate(plan, 1):\n                self.get_logger().info(f\'  {i}. {action["action"]}({action["parameters"]})\')\n    \n    def generate_plan(self, command):\n        """Use LLM to generate action plan"""\n        try:\n            if self.provider == \'openai\':\n                response = self.client.chat.completions.create(\n                    model=self.model,\n                    messages=[\n                        {"role": "system", "content": self.system_prompt},\n                        {"role": "user", "content": f"Plan the following task: {command}"}\n                    ],\n                    temperature=0.3,  # Lower = more deterministic\n                    max_tokens=500\n                )\n                content = response.choices[0].message.content\n            else:  # ollama\n                response = self.client.chat(\n                    model=self.model,\n                    messages=[\n                        {"role": "system", "content": self.system_prompt},\n                        {"role": "user", "content": f"Plan the following task: {command}"}\n                    ]\n                )\n                content = response[\'message\'][\'content\']\n            \n            # Parse JSON from LLM response\n            # LLM might include markdown code blocks, extract JSON\n            if \'```json\' in content:\n                json_start = content.find(\'```json\') + 7\n                json_end = content.find(\'```\', json_start)\n                content = content[json_start:json_end].strip()\n            elif \'```\' in content:\n                json_start = content.find(\'```\') + 3\n                json_end = content.find(\'```\', json_start)\n                content = content[json_start:json_end].strip()\n            \n            plan = json.loads(content)\n            \n            # Validate plan structure\n            if isinstance(plan, list) and all(\'action\' in item for item in plan):\n                return plan\n            else:\n                self.get_logger().error(f\'Invalid plan format: {plan}\')\n                return None\n                \n        except json.JSONDecodeError as e:\n            self.get_logger().error(f\'Failed to parse LLM response as JSON: {e}\')\n            self.get_logger().error(f\'LLM response: {content}\')\n            return None\n        except Exception as e:\n            self.get_logger().error(f\'LLM planning error: {e}\')\n            return None\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = LLMPlanner()\n    try:\n        rclpy.spin(node)\n    except KeyboardInterrupt:\n        pass\n    finally:\n        node.destroy_node()\n        rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Add to setup.py"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"install_requires=[\n    'setuptools',\n    'rclpy',\n    'openai',  # Optional\n    'ollama',  # Optional\n    'python-dotenv',\n],\n"})}),"\n",(0,o.jsx)(e.h3,{id:"step-4-test-llm-planning",children:"Step 4: Test LLM Planning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Launch planner"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"cd ~/isaac_ros_ws\ncolcon build --packages-select voice_commands\nsource install/setup.bash\n\n# Launch planner (with OpenAI)\nros2 run voice_commands llm_planner --ros-args -p llm_provider:=openai -p model:=gpt-4\n\n# Or with Ollama\nros2 run voice_commands llm_planner --ros-args -p llm_provider:=ollama -p model:=llama3\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Test with command"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Terminal 2: Send test command\nros2 topic pub --once /voice_commands/command std_msgs/String \\\n  \"{data: 'navigate:Pick up the red cup and place it on the table'}\"\n\n# Terminal 3: Monitor plan\nros2 topic echo /voice_commands/plan\n"})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Expected Output"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-json",children:'{\n  "data": "[{\\"action\\": \\"look_at\\", \\"parameters\\": {\\"object\\": \\"red cup\\"}}, {\\"action\\": \\"navigate_to\\", \\"parameters\\": {\\"location\\": \\"red cup\\"}}, {\\"action\\": \\"pick_up\\", \\"parameters\\": {\\"object\\": \\"red cup\\"}}, {\\"action\\": \\"navigate_to\\", \\"parameters\\": {\\"location\\": \\"table\\"}}, {\\"action\\": \\"place\\", \\"parameters\\": {\\"object\\": \\"red cup\\", \\"location\\": \\"table\\"}}]"\n}\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-5-enhanced-prompt-engineering",children:"Step 5: Enhanced Prompt Engineering"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Improved system prompt"})," (with context awareness):"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'self.system_prompt = """You are a task planner for a humanoid robot named "Robo".\n\nRobot capabilities:\n- Navigation: Can move to named locations (kitchen, living room, table, etc.)\n- Manipulation: Can pick up objects (cups, books, tools) and place them\n- Perception: Has cameras and can identify objects by color/shape\n- Communication: Can speak to humans\n\nCurrent context:\n- Robot is in the living room\n- Known objects: red cup (on coffee table), blue book (on shelf), green bottle (on floor)\n- Known locations: kitchen, living room, bedroom, table, shelf\n\nWhen planning:\n1. Break complex tasks into atomic actions\n2. Consider preconditions (e.g., must navigate before picking up)\n3. Use context to resolve ambiguous references ("the cup" \u2192 "red cup")\n4. Add look_at actions before manipulation for object identification\n5. Verify actions are executable (robot can reach, object exists)\n\nOutput format: JSON array of actions.\nEach action has:\n- "action": action name (navigate_to, pick_up, place, look_at, speak, wait)\n- "parameters": dict with action-specific parameters\n- "precondition": (optional) what must be true before this action\n- "postcondition": (optional) what becomes true after this action\n\nExample:\nInput: "Bring me the red cup"\nOutput: [\n  {"action": "look_at", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "coffee table"}},\n  {"action": "pick_up", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "human"}},\n  {"action": "place", "parameters": {"object": "red cup", "location": "human hand"}}\n]"""\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-6-handle-ambiguity-and-context",children:"Step 6: Handle Ambiguity and Context"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context manager"}),": ",(0,o.jsx)(e.code,{children:"voice_commands/context_manager.py"})]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nManages robot context for LLM planning\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass ContextManager(Node):\n    """\n    Tracks robot state and provides context to LLM\n    """\n    def __init__(self):\n        super().__init__(\'context_manager\')\n        \n        # Robot state\n        self.current_location = "living_room"\n        self.held_object = None\n        self.known_objects = {\n            "red_cup": {"location": "coffee_table", "color": "red"},\n            "blue_book": {"location": "shelf", "color": "blue"},\n            "green_bottle": {"location": "floor", "color": "green"},\n        }\n        \n        # Subscribe to action completion\n        self.action_complete_sub = self.create_subscription(\n            String,\n            \'/voice_commands/action_complete\',\n            self.action_complete_callback,\n            10\n        )\n        \n        # Publisher for context updates\n        self.context_pub = self.create_publisher(String, \'/voice_commands/context\', 10)\n        \n    def action_complete_callback(self, msg):\n        """Update context when action completes"""\n        result = json.loads(msg.data)\n        action = result.get(\'action\')\n        \n        if action == \'pick_up\':\n            self.held_object = result.get(\'object\')\n            # Update object location\n            if self.held_object in self.known_objects:\n                self.known_objects[self.held_object][\'location\'] = \'robot_hand\'\n        elif action == \'place\':\n            self.held_object = None\n            if \'object\' in result and \'location\' in result:\n                if result[\'object\'] in self.known_objects:\n                    self.known_objects[result[\'object\']][\'location\'] = result[\'location\']\n        elif action == \'navigate_to\':\n            self.current_location = result.get(\'location\', self.current_location)\n        \n        # Publish updated context\n        self.publish_context()\n    \n    def publish_context(self):\n        """Publish current context"""\n        context = {\n            "current_location": self.current_location,\n            "held_object": self.held_object,\n            "known_objects": self.known_objects\n        }\n        \n        msg = String()\n        msg.data = json.dumps(context)\n        self.context_pub.publish(msg)\n    \n    def get_context_string(self):\n        """Get context as string for LLM prompt"""\n        context_str = f"Robot is currently in: {self.current_location}\\n"\n        if self.held_object:\n            context_str += f"Robot is holding: {self.held_object}\\n"\n        context_str += "Known objects:\\n"\n        for obj, info in self.known_objects.items():\n            context_str += f"  - {obj} ({info[\'color\']}) at {info[\'location\']}\\n"\n        return context_str\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Update LLM planner"})," to use context:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# In LLMPlanner.__init__\nself.context_sub = self.create_subscription(\n    String,\n    \'/voice_commands/context\',\n    self.context_callback,\n    10\n)\nself.context = {}\n\n# In generate_plan\ncontext_str = self.get_context_string()\nmessages=[\n    {"role": "system", "content": self.system_prompt},\n    {"role": "user", "content": f"Context:\\n{context_str}\\n\\nPlan the following task: {command}"}\n]\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-7-error-recovery-and-replanning",children:"Step 7: Error Recovery and Replanning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Error handler"}),": ",(0,o.jsx)(e.code,{children:"voice_commands/error_handler.py"})]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nHandle action failures and replan using LLM\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nimport json\n\nclass ErrorHandler(Node):\n    def __init__(self):\n        super().__init__(\'error_handler\')\n        \n        # Subscribe to action failures\n        self.error_sub = self.create_subscription(\n            String,\n            \'/voice_commands/action_error\',\n            self.error_callback,\n            10\n        )\n        \n        # Publisher for replanning request\n        self.replan_pub = self.create_publisher(String, \'/voice_commands/replan\', 10)\n        \n    def error_callback(self, msg):\n        """Handle action failure and request replanning"""\n        error_data = json.loads(msg.data)\n        failed_action = error_data.get(\'action\')\n        error_message = error_data.get(\'error\')\n        original_command = error_data.get(\'original_command\')\n        \n        self.get_logger().warn(f\'Action failed: {failed_action} - {error_message}\')\n        \n        # Request LLM to replan with error context\n        replan_request = {\n            "original_command": original_command,\n            "failed_action": failed_action,\n            "error": error_message,\n            "request": "replan"\n        }\n        \n        msg = String()\n        msg.data = json.dumps(replan_request)\n        self.replan_pub.publish(msg)\n'})}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Update LLM planner"})," to handle replanning:"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Subscribe to replan requests\nself.replan_sub = self.create_subscription(\n    String,\n    \'/voice_commands/replan\',\n    self.replan_callback,\n    10\n)\n\ndef replan_callback(self, msg):\n    """Replan after action failure"""\n    request = json.loads(msg.data)\n    original_command = request[\'original_command\']\n    failed_action = request[\'failed_action\']\n    error = request[\'error\']\n    \n    # Ask LLM to replan considering the error\n    replan_prompt = f"""Previous plan failed:\n- Original command: {original_command}\n- Failed action: {failed_action}\n- Error: {error}\n\nPlease replan the task, avoiding the failed action or finding an alternative approach."""\n    \n    # Generate new plan\n    new_plan = self.generate_plan_with_prompt(replan_prompt)\n    # ... (publish new plan)\n'})}),"\n",(0,o.jsx)(e.h3,{id:"step-8-debugging-common-issues",children:"Step 8: Debugging Common Issues"}),"\n",(0,o.jsx)(e.h4,{id:"issue-1-api-key-not-found-or-rate-limit-exceeded",children:'Issue 1: "API key not found" or "Rate limit exceeded"'}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Symptoms"}),": OpenAI API errors"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-bash",children:"# Check .env file exists and has API key\ncat .env | grep OPENAI_API_KEY\n\n# Verify API key is valid\npython3 -c \"from openai import OpenAI; import os; from dotenv import load_dotenv; load_dotenv(); client = OpenAI(api_key=os.getenv('OPENAI_API_KEY')); print('API key valid')\"\n\n# For rate limits: Use smaller model or add retry logic\n"})}),"\n",(0,o.jsx)(e.h4,{id:"issue-2-llm-returns-invalid-json",children:'Issue 2: "LLM returns invalid JSON"'}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Symptoms"}),": JSON parsing errors"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Add JSON extraction (already in code)\n# Try multiple parsing strategies\n# Use structured output (if available): response_format={"type": "json_object"}\n\n# For OpenAI GPT-4:\nresponse = client.chat.completions.create(\n    ...,\n    response_format={"type": "json_object"}  # Forces JSON output\n)\n'})}),"\n",(0,o.jsx)(e.h4,{id:"issue-3-planning-too-slow-high-latency",children:'Issue 3: "Planning too slow" (high latency)'}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Symptoms"}),": Long delay between command and plan"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Use faster model\nmodel = 'gpt-3.5-turbo'  # Faster than gpt-4\n\n# Reduce max_tokens\nmax_tokens = 300  # Shorter responses\n\n# Use local LLM (Ollama) for lower latency\nprovider = 'ollama'\nmodel = 'llama3'\n"})}),"\n",(0,o.jsx)(e.h4,{id:"issue-4-plan-doesnt-match-command-intent",children:'Issue 4: "Plan doesn\'t match command intent"'}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Symptoms"}),": LLM generates wrong actions"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:"# Improve system prompt (more specific)\n# Add examples in few-shot learning\n# Use temperature=0.0 for more deterministic output\ntemperature = 0.0  # Most deterministic\n\n# Add validation: Check if plan makes sense\ndef validate_plan(plan, command):\n    # Check plan contains expected actions\n    # Verify parameters are reasonable\n    pass\n"})}),"\n",(0,o.jsx)(e.h2,{id:"part-3-advanced-topics-optional",children:"Part 3: Advanced Topics (Optional)"}),"\n",(0,o.jsx)(e.h3,{id:"few-shot-learning",children:"Few-Shot Learning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Add examples to prompt"}),":"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'self.system_prompt += """\n\nExamples:\n\nInput: "Go to the kitchen"\nOutput: [{"action": "navigate_to", "parameters": {"location": "kitchen"}}]\n\nInput: "Pick up the cup"\nOutput: [\n  {"action": "look_at", "parameters": {"object": "cup"}},\n  {"action": "navigate_to", "parameters": {"location": "cup"}},\n  {"action": "pick_up", "parameters": {"object": "cup"}}\n]\n\nInput: "Bring me the red cup"\nOutput: [\n  {"action": "look_at", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "red cup"}},\n  {"action": "pick_up", "parameters": {"object": "red cup"}},\n  {"action": "navigate_to", "parameters": {"location": "human"}},\n  {"action": "place", "parameters": {"object": "red cup", "location": "human hand"}}\n]"""\n'})}),"\n",(0,o.jsx)(e.h3,{id:"function-calling-structured-output",children:"Function Calling (Structured Output)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Use OpenAI function calling"})," (more reliable JSON):"]}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'response = client.chat.completions.create(\n    model="gpt-4",\n    messages=[...],\n    functions=[{\n        "name": "generate_plan",\n        "description": "Generate robot action plan",\n        "parameters": {\n            "type": "object",\n            "properties": {\n                "actions": {\n                    "type": "array",\n                    "items": {\n                        "type": "object",\n                        "properties": {\n                            "action": {"type": "string"},\n                            "parameters": {"type": "object"}\n                        }\n                    }\n                }\n            }\n        }\n    }],\n    function_call={"name": "generate_plan"}\n)\n\n# Extract function call result\nfunction_call = response.choices[0].message.function_call\nplan = json.loads(function_call.arguments)[\'actions\']\n'})}),"\n",(0,o.jsx)(e.h2,{id:"integration-with-capstone",children:"Integration with Capstone"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"How this chapter contributes"})," to the Week 13 autonomous humanoid:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Task decomposition"}),": Capstone will use LLM to break complex commands into steps"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Natural language understanding"}),": Handles ambiguous and complex commands"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Error recovery"}),": LLM replans when actions fail"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Context awareness"}),": Understands references and maintains state"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"Understanding LLM planning now is essential for the capstone cognitive system."}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"You learned:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:["\u2705 Set up ",(0,o.jsx)(e.strong,{children:"GPT-4 API or local LLM"})," (Llama) for task planning"]}),"\n",(0,o.jsxs)(e.li,{children:["\u2705 Implemented ",(0,o.jsx)(e.strong,{children:"prompt engineering"})," for robot task decomposition"]}),"\n",(0,o.jsxs)(e.li,{children:["\u2705 Created ",(0,o.jsx)(e.strong,{children:"planning pipeline"})," converting goals to executable actions"]}),"\n",(0,o.jsxs)(e.li,{children:["\u2705 Handled ",(0,o.jsx)(e.strong,{children:"ambiguous commands"})," and context awareness"]}),"\n",(0,o.jsxs)(e.li,{children:["\u2705 Implemented ",(0,o.jsx)(e.strong,{children:"error recovery"})," via LLM replanning"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next steps"}),": In Chapter 4.3, you'll map LLM-generated plans to ROS 2 Actions for execution."]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,o.jsx)(e.h3,{id:"exercise-1-basic-llm-planning-required",children:"Exercise 1: Basic LLM Planning (Required)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Set up LLM and generate plans for simple commands."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Set up OpenAI API or Ollama"}),"\n",(0,o.jsx)(e.li,{children:"Create LLM planner node"}),"\n",(0,o.jsxs)(e.li,{children:["Test with 5 different commands:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Go to the kitchen"'}),"\n",(0,o.jsx)(e.li,{children:'"Pick up the cup"'}),"\n",(0,o.jsx)(e.li,{children:'"Bring me the red cup"'}),"\n",(0,o.jsx)(e.li,{children:'"Place the book on the shelf"'}),"\n",(0,o.jsx)(e.li,{children:'"Stop"'}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.li,{children:"Verify plans are valid JSON"}),"\n",(0,o.jsx)(e.li,{children:"Document plan quality (correctness, completeness)"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,o.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","LLM planner node running"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Plans generated for all 5 commands"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Plans are valid JSON"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Plans contain reasonable actions"]}),"\n",(0,o.jsxs)(e.li,{className:"task-list-item",children:[(0,o.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Planning latency < 5 seconds"]}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-2-prompt-engineering-required",children:"Exercise 2: Prompt Engineering (Required)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Optimize prompts for better planning quality."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Create baseline prompt"}),"\n",(0,o.jsx)(e.li,{children:"Test with 10 commands"}),"\n",(0,o.jsx)(e.li,{children:"Measure plan quality (correctness, completeness)"}),"\n",(0,o.jsx)(e.li,{children:"Iterate on prompt (add examples, clarify actions)"}),"\n",(0,o.jsx)(e.li,{children:"Compare plan quality before/after prompt improvements"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Metrics"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Plan correctness (% of correct actions)"}),"\n",(0,o.jsx)(e.li,{children:"Plan completeness (% of required steps included)"}),"\n",(0,o.jsx)(e.li,{children:"Action parameter accuracy"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Estimated Time"}),": 180 minutes"]}),"\n",(0,o.jsx)(e.h3,{id:"exercise-3-context-aware-planning-challenge",children:"Exercise 3: Context-Aware Planning (Challenge)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Objective"}),": Implement context management for better planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Create context manager tracking robot state"}),"\n",(0,o.jsx)(e.li,{children:"Integrate context into LLM prompts"}),"\n",(0,o.jsxs)(e.li,{children:["Test with context-dependent commands:","\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:'"Pick up the cup" (when multiple cups exist)'}),"\n",(0,o.jsx)(e.li,{children:'"Put it down" (referring to held object)'}),"\n",(0,o.jsx)(e.li,{children:'"Go back" (return to previous location)'}),"\n"]}),"\n"]}),"\n",(0,o.jsx)(e.li,{children:"Verify context improves plan accuracy"}),"\n",(0,o.jsx)(e.li,{children:"Document context impact on planning"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsx)(e.li,{children:"Context manager node"}),"\n",(0,o.jsx)(e.li,{children:"Context integrated into prompts"}),"\n",(0,o.jsx)(e.li,{children:"Test results showing improvement"}),"\n"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Estimated Time"}),": 240 minutes"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://platform.openai.com/docs",children:"OpenAI API Documentation"})," - GPT-4 API reference"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://ollama.com/docs",children:"Ollama Documentation"})," - Local LLM setup"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://python.langchain.com/",children:"LangChain Documentation"})," - LLM orchestration"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.a,{href:"https://www.promptingguide.ai/",children:"Prompt Engineering Guide"})," - Best practices"]}),"\n"]}),"\n",(0,o.jsx)(e.hr,{}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Next"}),": [Chapter 4.3: Natural Language to ROS 2 Actions \u2192](chapter-4 to 3.md)"]})]})}function p(n={}){const{wrapper:e}={...(0,r.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,t)=>{t.d(e,{R:()=>a,x:()=>l});var s=t(6540);const o={},r=s.createContext(o);function a(n){const e=s.useContext(r);return s.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),s.createElement(r.Provider,{value:e},n.children)}}}]);
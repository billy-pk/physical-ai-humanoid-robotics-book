"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[224],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},9040:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2/chapter-2-2","title":"2.2 Deep Learning for Vision","description":"Deep learning transformed computer vision, enabling robots to recognize objects with human-level accuracy. This chapter covers Convolutional Neural Networks (CNNs) and modern object detection for robotics.","source":"@site/docs/module-2/chapter-2-2.md","sourceDirName":"module-2","slug":"/module-2/chapter-2-2","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/chapter-2-2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"2.2 Deep Learning for Vision"},"sidebar":"defaultSidebar","previous":{"title":"2.1 Computer Vision Fundamentals","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1"},"next":{"title":"2.3 3D Vision and Depth Perception","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3"}}');var r=i(4848),s=i(8453);const o={sidebar_position:3,title:"2.2 Deep Learning for Vision"},l="Chapter 2.2: Deep Learning for Vision",a={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Why Deep Learning for Vision?",id:"why-deep-learning-for-vision",level:2},{value:"Convolutional Neural Networks (CNNs)",id:"convolutional-neural-networks-cnns",level:2},{value:"Key Components",id:"key-components",level:3},{value:"CNN Architecture Example",id:"cnn-architecture-example",level:3},{value:"Object Detection with Pre-Trained Models",id:"object-detection-with-pre-trained-models",level:2},{value:"Example: Image Classification with ResNet",id:"example-image-classification-with-resnet",level:3},{value:"Real-Time Object Detection: YOLO",id:"real-time-object-detection-yolo",level:2},{value:"Why YOLO for Robots?",id:"why-yolo-for-robots",level:3},{value:"Using YOLOv8 (Ultralytics)",id:"using-yolov8-ultralytics",level:3},{value:"Getting 3D Position from Detection",id:"getting-3d-position-from-detection",level:3},{value:"Transfer Learning for Custom Objects",id:"transfer-learning-for-custom-objects",level:2},{value:"Process",id:"process",level:3},{value:"Fine-Tuning YOLOv8",id:"fine-tuning-yolov8",level:3},{value:"Semantic Segmentation",id:"semantic-segmentation",level:2},{value:"Example with DeepLabV3",id:"example-with-deeplabv3",level:3},{value:"Practical Tips for Robotics",id:"practical-tips-for-robotics",level:2},{value:"1. Model Selection",id:"1-model-selection",level:3},{value:"2. Hardware Acceleration",id:"2-hardware-acceleration",level:3},{value:"3. Confidence Thresholding",id:"3-confidence-thresholding",level:3},{value:"Exercises",id:"exercises",level:2},{value:"1. Pre-Trained Model Exploration",id:"1-pre-trained-model-exploration",level:3},{value:"2. Custom Object Detection",id:"2-custom-object-detection",level:3},{value:"3. 3D Position Estimation",id:"3-3d-position-estimation",level:3},{value:"4. Segmentation vs Detection",id:"4-segmentation-vs-detection",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-22-deep-learning-for-vision",children:"Chapter 2.2: Deep Learning for Vision"})}),"\n",(0,r.jsx)(n.p,{children:"Deep learning transformed computer vision, enabling robots to recognize objects with human-level accuracy. This chapter covers Convolutional Neural Networks (CNNs) and modern object detection for robotics."}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," CNN architecture and how it processes images"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply"})," pre-trained models for object detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," transfer learning for robot-specific tasks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use"})," YOLO for real-time detection in robotics"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"why-deep-learning-for-vision",children:"Why Deep Learning for Vision?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Classical CV limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Hand-crafted features (edges, corners) don't generalize well"}),"\n",(0,r.jsx)(n.li,{children:"Breaks down with lighting changes, occlusions, viewpoint shifts"}),"\n",(0,r.jsx)(n.li,{children:"Requires expert tuning for each task"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Deep learning advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Learns features automatically from data"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Robust to variations (lighting, angles, backgrounds)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Achieves superhuman accuracy on many tasks"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Transfer learning: pre-trained models work out-of-the-box"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"convolutional-neural-networks-cnns",children:"Convolutional Neural Networks (CNNs)"}),"\n",(0,r.jsx)(n.p,{children:"CNNs are specialized neural networks for processing grid-like data (images)."}),"\n",(0,r.jsx)(n.h3,{id:"key-components",children:"Key Components"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"1. Convolutional Layers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Apply filters (kernels) to detect patterns"}),"\n",(0,r.jsx)(n.li,{children:"Early layers: edges, textures"}),"\n",(0,r.jsx)(n.li,{children:"Deep layers: object parts, full objects"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"2. Pooling Layers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Reduce spatial dimensions (downsampling)"}),"\n",(0,r.jsx)(n.li,{children:"Max pooling: take maximum value in region"}),"\n",(0,r.jsx)(n.li,{children:"Makes network invariant to small translations"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"3. Fully Connected Layers"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Final layers for classification"}),"\n",(0,r.jsx)(n.li,{children:"Combine features to make predictions"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"cnn-architecture-example",children:"CNN Architecture Example"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Input Image (224\xd7224\xd73)\n     \u2193\nConv Layer 1 (32 filters, 3\xd73) \u2192 ReLU \u2192 MaxPool\n     \u2193\nConv Layer 2 (64 filters, 3\xd73) \u2192 ReLU \u2192 MaxPool\n     \u2193\nConv Layer 3 (128 filters, 3\xd73) \u2192 ReLU \u2192 MaxPool\n     \u2193\nFlatten \u2192 Fully Connected (512) \u2192 ReLU \u2192 Dropout\n     \u2193\nOutput (1000 classes)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"object-detection-with-pre-trained-models",children:"Object Detection with Pre-Trained Models"}),"\n",(0,r.jsxs)(n.p,{children:["Use ",(0,r.jsx)(n.strong,{children:"pre-trained models"})," (trained on ImageNet, COCO) instead of training from scratch."]}),"\n",(0,r.jsx)(n.h3,{id:"example-image-classification-with-resnet",children:"Example: Image Classification with ResNet"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import torch\nimport torchvision.transforms as transforms\nfrom torchvision.models import resnet50\nfrom PIL import Image\nimport requests\n\n# Load pre-trained ResNet50\nmodel = resnet50(pretrained=True)\nmodel.eval()\n\n# Image preprocessing (required by ResNet)\npreprocess = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225]\n    )\n])\n\n# Load and preprocess image\nimg = Image.open(\'robot_view.jpg\')\ninput_tensor = preprocess(img).unsqueeze(0)  # Add batch dimension\n\n# Inference\nwith torch.no_grad():\n    output = model(input_tensor)\n\n# Get predicted class\nprobabilities = torch.nn.functional.softmax(output[0], dim=0)\ntop5_prob, top5_idx = torch.topk(probabilities, 5)\n\n# Load ImageNet labels\nlabels_url = "https://raw.githubusercontent.com/anishathalye/imagenet-simple-labels/master/imagenet-simple-labels.json"\nlabels = requests.get(labels_url).json()\n\n# Print predictions\nfor i in range(5):\n    print(f"{labels[top5_idx[i]]}: {top5_prob[i].item()*100:.2f}%")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output example"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"coffee mug: 87.34%\ncup: 8.21%\nespresso: 2.15%\n...\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"real-time-object-detection-yolo",children:"Real-Time Object Detection: YOLO"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"YOLO (You Only Look Once)"})," is the go-to detector for robotics\u2014fast and accurate."]}),"\n",(0,r.jsx)(n.h3,{id:"why-yolo-for-robots",children:"Why YOLO for Robots?"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Model"}),(0,r.jsx)(n.th,{children:"Speed"}),(0,r.jsx)(n.th,{children:"Accuracy"}),(0,r.jsx)(n.th,{children:"Use Case"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Faster R-CNN"})}),(0,r.jsx)(n.td,{children:"~5 FPS"}),(0,r.jsx)(n.td,{children:"Very High"}),(0,r.jsx)(n.td,{children:"Offline analysis"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"SSD"})}),(0,r.jsx)(n.td,{children:"~20 FPS"}),(0,r.jsx)(n.td,{children:"Medium"}),(0,r.jsx)(n.td,{children:"Moderate real-time"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"YOLOv5/v8"})}),(0,r.jsx)(n.td,{children:"~60+ FPS"}),(0,r.jsx)(n.td,{children:"High"}),(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Robotics (real-time)"})})]})]})]}),"\n",(0,r.jsx)(n.h3,{id:"using-yolov8-ultralytics",children:"Using YOLOv8 (Ultralytics)"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"# Install\npip install ultralytics opencv-python\n"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\nimport cv2\n\n# Load pre-trained model\nmodel = YOLO('yolov8n.pt')  # 'n' = nano (fastest), 's', 'm', 'l', 'x' (most accurate)\n\n# Method 1: Detect in single image\nresults = model('robot_workspace.jpg')\n\n# Visualize\nannotated = results[0].plot()\ncv2.imshow('Detections', annotated)\ncv2.waitKey(0)\n\n# Method 2: Real-time webcam detection\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Run detection\n    results = model(frame, conf=0.5)  # confidence threshold\n\n    # Get bounding boxes\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            # Extract box coordinates\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            conf = box.conf[0].cpu().numpy()\n            cls = int(box.cls[0].cpu().numpy())\n            label = model.names[cls]\n\n            # Draw on frame\n            cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)),\n                         (0, 255, 0), 2)\n            cv2.putText(frame, f'{label} {conf:.2f}',\n                       (int(x1), int(y1)-10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow('YOLO Detection', frame)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"getting-3d-position-from-detection",children:"Getting 3D Position from Detection"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# After detecting object in image, estimate 3D position\n# (requires depth camera or stereo vision)\n\ndef pixel_to_3d(u, v, depth, camera_matrix):\n    """\n    Convert pixel coordinates to 3D point.\n\n    Args:\n        u, v: Pixel coordinates\n        depth: Depth value at (u, v) in meters\n        camera_matrix: 3x3 camera intrinsic matrix\n\n    Returns:\n        (x, y, z) in camera frame\n    """\n    fx = camera_matrix[0, 0]\n    fy = camera_matrix[1, 1]\n    cx = camera_matrix[0, 2]\n    cy = camera_matrix[1, 2]\n\n    z = depth\n    x = (u - cx) * z / fx\n    y = (v - cy) * z / fy\n\n    return x, y, z\n\n# Example usage\nbbox_center_u = (x1 + x2) / 2\nbbox_center_v = (y1 + y2) / 2\ndepth_at_center = depth_image[int(bbox_center_v), int(bbox_center_u)]\n\nx, y, z = pixel_to_3d(bbox_center_u, bbox_center_v, depth_at_center, K)\nprint(f"Object at 3D position: ({x:.2f}, {y:.2f}, {z:.2f}) meters")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"transfer-learning-for-custom-objects",children:"Transfer Learning for Custom Objects"}),"\n",(0,r.jsx)(n.p,{children:"Train a detector for robot-specific objects (tools, parts, etc.)."}),"\n",(0,r.jsx)(n.h3,{id:"process",children:"Process"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Collect images"})," of your objects (~100-500 per class)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Annotate"})," bounding boxes (use tools like LabelImg, Roboflow)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fine-tune"})," pre-trained YOLO on your dataset"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"fine-tuning-yolov8",children:"Fine-Tuning YOLOv8"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\n\n# Load pre-trained model\nmodel = YOLO('yolov8n.pt')\n\n# Train on custom dataset\nresults = model.train(\n    data='custom_data.yaml',  # Dataset config\n    epochs=50,\n    imgsz=640,\n    batch=16,\n    name='robot_tools_detector'\n)\n\n# Use trained model\nmodel = YOLO('runs/detect/robot_tools_detector/weights/best.pt')\nresults = model('test_image.jpg')\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Dataset config"})," (",(0,r.jsx)(n.code,{children:"custom_data.yaml"}),"):"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-yaml",children:"train: /path/to/train/images\nval: /path/to/val/images\n\nnc: 3  # Number of classes\nnames: ['wrench', 'screwdriver', 'bolt']\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Why this works"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Pre-trained model already knows edges, shapes, textures"}),"\n",(0,r.jsx)(n.li,{children:"Fine-tuning adapts it to your specific objects"}),"\n",(0,r.jsx)(n.li,{children:"Needs far less data than training from scratch"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"semantic-segmentation",children:"Semantic Segmentation"}),"\n",(0,r.jsx)(n.p,{children:"Classify every pixel (not just bounding boxes)."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use cases"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Terrain classification (road, grass, obstacles)"}),"\n",(0,r.jsx)(n.li,{children:"Grasp point detection (object vs. background)"}),"\n",(0,r.jsx)(n.li,{children:"Scene understanding"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"example-with-deeplabv3",children:"Example with DeepLabV3"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import torchvision\nimport torch\n\n# Load pre-trained segmentation model\nmodel = torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\nmodel.eval()\n\n# Preprocess image\ninput_tensor = preprocess(img).unsqueeze(0)\n\n# Inference\nwith torch.no_grad():\n    output = model(input_tensor)['out'][0]\n\n# Get class for each pixel\noutput_predictions = output.argmax(0).byte().cpu().numpy()\n\n# Visualize (color-code each class)\nfrom matplotlib import pyplot as plt\nplt.imshow(output_predictions)\nplt.show()\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"practical-tips-for-robotics",children:"Practical Tips for Robotics"}),"\n",(0,r.jsx)(n.h3,{id:"1-model-selection",children:"1. Model Selection"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For real-time robotics (>30 FPS)"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"YOLOv8n (nano) or YOLOv8s (small)"}),"\n",(0,r.jsx)(n.li,{children:"MobileNetV3 + SSD"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"For accuracy-critical tasks"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"YOLOv8l/x"}),"\n",(0,r.jsx)(n.li,{children:"Faster R-CNN with ResNet101"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-hardware-acceleration",children:"2. Hardware Acceleration"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Use GPU if available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel.to(device)\n\n# For edge devices (Jetson, Raspberry Pi)\n# Use TensorRT or ONNX optimization\n"})}),"\n",(0,r.jsx)(n.h3,{id:"3-confidence-thresholding",children:"3. Confidence Thresholding"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Adjust confidence based on application\nresults = model(frame, conf=0.7)  # High conf = fewer false positives\n\n# For safety-critical: higher threshold (0.8-0.9)\n# For exploration: lower threshold (0.3-0.5)\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"1-pre-trained-model-exploration",children:"1. Pre-Trained Model Exploration"}),"\n",(0,r.jsx)(n.p,{children:"Download YOLOv8 and run it on 5 different images. Compare detection quality with different model sizes (yolov8n vs yolov8m). What's the FPS difference?"}),"\n",(0,r.jsx)(n.h3,{id:"2-custom-object-detection",children:"2. Custom Object Detection"}),"\n",(0,r.jsxs)(n.p,{children:["Create a small dataset (20-30 images) of an object in your environment (coffee mug, book, etc.). Annotate bounding boxes using ",(0,r.jsx)(n.a,{href:"https://roboflow.com/",children:"Roboflow"}),". Fine-tune YOLOv8 and test it."]}),"\n",(0,r.jsx)(n.h3,{id:"3-3d-position-estimation",children:"3. 3D Position Estimation"}),"\n",(0,r.jsx)(n.p,{children:"Combine YOLO detection with a depth camera (Intel RealSense or Kinect). For each detected object, compute its 3D position relative to the robot. Print coordinates."}),"\n",(0,r.jsx)(n.h3,{id:"4-segmentation-vs-detection",children:"4. Segmentation vs Detection"}),"\n",(0,r.jsx)(n.p,{children:"Compare outputs of YOLOv8 (bounding boxes) vs DeepLabV3 (segmentation) on the same image. Which is better for:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Counting objects?"}),"\n",(0,r.jsx)(n.li,{children:"Identifying traversable terrain?"}),"\n",(0,r.jsx)(n.li,{children:"Precise grasping?"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"CNNs"})," automatically learn hierarchical visual features\n\u2705 ",(0,r.jsx)(n.strong,{children:"Pre-trained models"})," (ImageNet, COCO) work well out-of-the-box\n\u2705 ",(0,r.jsx)(n.strong,{children:"YOLO"})," is the best choice for real-time robotic vision (60+ FPS)\n\u2705 ",(0,r.jsx)(n.strong,{children:"Transfer learning"})," enables custom detectors with limited data\n\u2705 ",(0,r.jsx)(n.strong,{children:"Segmentation"})," provides pixel-level understanding for complex tasks\n\u2705 Always use ",(0,r.jsx)(n.strong,{children:"GPU"})," for deep learning inference on robots"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html",children:"PyTorch Vision Tutorials"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://docs.ultralytics.com/",children:"Ultralytics YOLO Docs"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Deep Learning for Computer Vision"})," (Stanford CS231n course notes)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1",children:"\u2190 Chapter 2.1: CV Fundamentals"})," | ",(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3",children:"Chapter 2.3: 3D Vision and Depth \u2192"})]}),"\n",(0,r.jsx)(n.p,{children:"With 2D detection mastered, let's explore how robots perceive depth and understand 3D space!"})]})}function h(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);
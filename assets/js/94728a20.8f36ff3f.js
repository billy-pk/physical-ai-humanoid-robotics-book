"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[9264],{2734:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>l,contentTitle:()=>r,default:()=>m,frontMatter:()=>o,metadata:()=>t,toc:()=>d});const t=JSON.parse('{"id":"module-3/chapter-3-2","title":"3.2 Synthetic Data Generation for Training","description":"Training vision-based AI models requires thousands of labeled images. Isaac Sim provides built-in tools for generating synthetic data with perfect annotations\u2014enabling rapid dataset creation without manual labeling. This chapter covers data collection pipelines, domain randomization, and dataset export formats.","source":"@site/docs/module-3/chapter-3-2.md","sourceDirName":"module-3","slug":"/module-3/chapter-3-2","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-2","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-3/chapter-3-2.md","tags":[],"version":"current","sidebarPosition":3,"frontMatter":{"sidebar_position":3,"title":"3.2 Synthetic Data Generation for Training"},"sidebar":"defaultSidebar","previous":{"title":"3.1 Isaac Sim Setup & Photorealistic Simulation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-1"},"next":{"title":"3.3 Isaac ROS - Hardware-Accelerated VSLAM","permalink":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-3"}}');var a=i(4848),s=i(8453);const o={sidebar_position:3,title:"3.2 Synthetic Data Generation for Training"},r="Chapter 3.2: Synthetic Data Generation for Training",l={},d=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: Synthetic Data Fundamentals",id:"part-1-synthetic-data-fundamentals",level:2},{value:"Why Synthetic Data?",id:"why-synthetic-data",level:3},{value:"Data Types for Humanoid Robots",id:"data-types-for-humanoid-robots",level:3},{value:"Domain Randomization",id:"domain-randomization",level:3},{value:"Part 2: Hands-On Tutorial",id:"part-2-hands-on-tutorial",level:2},{value:"Project: Generate Synthetic Dataset for Object Detection",id:"project-generate-synthetic-dataset-for-object-detection",level:3},{value:"Step 1: Set Up Data Collection Script",id:"step-1-set-up-data-collection-script",level:3},{value:"Step 2: Configure Domain Randomization",id:"step-2-configure-domain-randomization",level:3},{value:"Step 3: Run Data Collection",id:"step-3-run-data-collection",level:3},{value:"Step 4: Export to YOLO Format",id:"step-4-export-to-yolo-format",level:3},{value:"Step 5: Validate Dataset Quality",id:"step-5-validate-dataset-quality",level:3},{value:"Step 6: Debugging Common Issues",id:"step-6-debugging-common-issues",level:3},{value:"Issue 1: &quot;No images generated&quot; or &quot;Empty directories&quot;",id:"issue-1-no-images-generated-or-empty-directories",level:4},{value:"Issue 2: &quot;Segmentation mask all zeros&quot;",id:"issue-2-segmentation-mask-all-zeros",level:4},{value:"Issue 3: &quot;Annotations missing bounding boxes&quot;",id:"issue-3-annotations-missing-bounding-boxes",level:4},{value:"Issue 4: &quot;Domain randomization not working&quot;",id:"issue-4-domain-randomization-not-working",level:4},{value:"Part 3: Advanced Topics (Optional)",id:"part-3-advanced-topics-optional",level:2},{value:"Multi-View Data Collection",id:"multi-view-data-collection",level:3},{value:"Active Learning Pipeline",id:"active-learning-pipeline",level:3},{value:"Integration with Capstone",id:"integration-with-capstone",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic Data Collection (Required)",id:"exercise-1-basic-data-collection-required",level:3},{value:"Exercise 2: Domain Randomization (Required)",id:"exercise-2-domain-randomization-required",level:3},{value:"Exercise 3: Multi-Format Export (Challenge)",id:"exercise-3-multi-format-export-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function c(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,s.R)(),...n.components};return(0,a.jsxs)(a.Fragment,{children:[(0,a.jsx)(e.header,{children:(0,a.jsx)(e.h1,{id:"chapter-32-synthetic-data-generation-for-training",children:"Chapter 3.2: Synthetic Data Generation for Training"})}),"\n",(0,a.jsx)(e.p,{children:"Training vision-based AI models requires thousands of labeled images. Isaac Sim provides built-in tools for generating synthetic data with perfect annotations\u2014enabling rapid dataset creation without manual labeling. This chapter covers data collection pipelines, domain randomization, and dataset export formats."}),"\n",(0,a.jsx)(e.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,a.jsx)(e.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Set up"})," data collection pipelines in Isaac Sim for ML training"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Generate"})," RGB, depth, and segmentation images with annotations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Implement"})," domain randomization for robust model training"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Export"})," datasets in COCO, YOLO, and custom formats"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Validate"})," synthetic data quality and diversity"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Isaac Sim 2023.1.1"})," installed and configured (Chapter 3.1)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Python 3.10+"})," with numpy, opencv-python"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Basic ML concepts"}),": datasets, annotations, training/validation splits"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Understanding"})," of image formats (PNG, JPEG) and coordinate systems"]}),"\n"]}),"\n",(0,a.jsx)(e.h2,{id:"part-1-synthetic-data-fundamentals",children:"Part 1: Synthetic Data Fundamentals"}),"\n",(0,a.jsx)(e.h3,{id:"why-synthetic-data",children:"Why Synthetic Data?"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Challenges with real-world data"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Labeling cost"}),": Manual annotation is expensive ($0.10-$1.00 per image)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Data scarcity"}),": Rare scenarios (accidents, edge cases) hard to capture"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Privacy"}),": Real images may contain sensitive information"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Variability"}),": Hard to control lighting, weather, object positions"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Advantages of synthetic data"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perfect labels"}),": Automatic, pixel-perfect annotations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Infinite variety"}),": Generate millions of images quickly"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Controlled conditions"}),": Test specific scenarios (lighting, weather)"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Cost-effective"}),": Generate 10,000 images in hours vs. weeks of manual labeling"]}),"\n"]}),"\n",(0,a.jsx)(e.h3,{id:"data-types-for-humanoid-robots",children:"Data Types for Humanoid Robots"}),"\n",(0,a.jsxs)(e.table,{children:[(0,a.jsx)(e.thead,{children:(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.th,{children:"Data Type"}),(0,a.jsx)(e.th,{children:"Use Case"}),(0,a.jsx)(e.th,{children:"Format"})]})}),(0,a.jsxs)(e.tbody,{children:[(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"RGB Images"})}),(0,a.jsx)(e.td,{children:"Object detection, classification"}),(0,a.jsx)(e.td,{children:"PNG/JPEG (H\xd7W\xd73)"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Depth Maps"})}),(0,a.jsx)(e.td,{children:"3D perception, manipulation"}),(0,a.jsx)(e.td,{children:"PNG 16-bit (H\xd7W\xd71)"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Segmentation Masks"})}),(0,a.jsx)(e.td,{children:"Instance segmentation"}),(0,a.jsx)(e.td,{children:"PNG (H\xd7W\xd71, class IDs)"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Bounding Boxes"})}),(0,a.jsx)(e.td,{children:"Object detection"}),(0,a.jsx)(e.td,{children:"JSON (COCO/YOLO format)"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Pose Annotations"})}),(0,a.jsx)(e.td,{children:"Keypoint detection"}),(0,a.jsx)(e.td,{children:"JSON (COCO format)"})]}),(0,a.jsxs)(e.tr,{children:[(0,a.jsx)(e.td,{children:(0,a.jsx)(e.strong,{children:"Point Clouds"})}),(0,a.jsx)(e.td,{children:"3D SLAM, manipulation"}),(0,a.jsx)(e.td,{children:"PLY/PCD (N\xd73)"})]})]})]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"This chapter focuses on"}),": RGB images, depth maps, segmentation masks, and bounding boxes."]}),"\n",(0,a.jsx)(e.h3,{id:"domain-randomization",children:"Domain Randomization"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Domain randomization"})," varies scene parameters to improve model robustness:"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Lighting"}),": Intensity, color temperature, shadows"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Textures"}),": Material properties, surface patterns"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Object positions"}),": Random placement, orientations"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Camera angles"}),": Varying viewpoints, distances"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Backgrounds"}),": Different environments, clutter"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Weather"}),": Fog, rain, snow (if outdoor)"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Goal"}),": Train models that generalize to real-world variations."]}),"\n",(0,a.jsx)(e.h2,{id:"part-2-hands-on-tutorial",children:"Part 2: Hands-On Tutorial"}),"\n",(0,a.jsx)(e.h3,{id:"project-generate-synthetic-dataset-for-object-detection",children:"Project: Generate Synthetic Dataset for Object Detection"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Goal"}),": Create a data collection pipeline that generates 500+ labeled images of humanoid robot interacting with objects."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tools"}),": Isaac Sim 2023.1.1, Python 3.10+, OpenCV"]}),"\n",(0,a.jsx)(e.h3,{id:"step-1-set-up-data-collection-script",children:"Step 1: Set Up Data Collection Script"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Create Python script"}),": ",(0,a.jsx)(e.code,{children:"scripts/collect_synthetic_data.py"})]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSynthetic data collection pipeline for Isaac Sim\nIsaac Sim 2023.1.1 | Python 3.10+\n"""\nfrom omni.isaac.kit import SimulationApp\n\n# Launch Isaac Sim in headless mode (faster)\nsimulation_app = SimulationApp({\n    "headless": False,  # Set True for batch processing\n    "width": 1920,\n    "height": 1080\n})\n\nimport numpy as np\nimport cv2\nimport json\nimport os\nfrom pathlib import Path\nfrom omni.isaac.core import World\nfrom omni.isaac.core.robots import Robot\nfrom omni.isaac.core.utils.stage import add_reference_to_stage\nfrom omni.isaac.sensor import Camera\nimport random\n\n# Configuration\nOUTPUT_DIR = Path("~/synthetic_data")\nOUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n\nRGB_DIR = OUTPUT_DIR / "rgb"\nDEPTH_DIR = OUTPUT_DIR / "depth"\nSEGMENTATION_DIR = OUTPUT_DIR / "segmentation"\nANNOTATIONS_DIR = OUTPUT_DIR / "annotations"\n\nfor dir_path in [RGB_DIR, DEPTH_DIR, SEGMENTATION_DIR, ANNOTATIONS_DIR]:\n    dir_path.mkdir(exist_ok=True)\n\nNUM_IMAGES = 500\nIMAGE_WIDTH = 1920\nIMAGE_HEIGHT = 1080\n\nclass DataCollector:\n    def __init__(self):\n        self.world = World(stage_units_in_meters=1.0)\n        self.frame_count = 0\n        self.annotations = []\n        \n        # Setup scene\n        self.setup_scene()\n        \n        # Setup camera\n        self.setup_camera()\n        \n    def setup_scene(self):\n        """Initialize scene with robot and objects"""\n        # Add ground plane\n        self.world.scene.add_default_ground_plane()\n        \n        # Add robot (from URDF)\n        robot_urdf_path = "/path/to/humanoid.urdf"\n        self.robot = self.world.scene.add(\n            Robot(\n                prim_path="/World/Robot",\n                name="humanoid",\n                usd_path=None,  # Will load URDF\n                position=np.array([0, 0, 1.0])\n            )\n        )\n        \n        # Add objects to detect (cubes, spheres, etc.)\n        self.objects = []\n        # ... (object creation code)\n        \n    def setup_camera(self):\n        """Create camera sensor"""\n        self.camera = self.world.scene.add(\n            Camera(\n                prim_path="/World/Camera",\n                name="camera",\n                position=np.array([0, -2, 1.6]),  # Eye height\n                resolution=(IMAGE_WIDTH, IMAGE_HEIGHT),\n                frequency=30\n            )\n        )\n        \n    def randomize_scene(self):\n        """Apply domain randomization"""\n        # Randomize lighting\n        light_intensity = random.uniform(0.5, 3.0)\n        # ... (light randomization)\n        \n        # Randomize object positions\n        for obj in self.objects:\n            obj.set_world_pose(\n                position=np.array([\n                    random.uniform(-2, 2),\n                    random.uniform(-2, 2),\n                    random.uniform(0.5, 1.5)\n                ])\n            )\n        \n        # Randomize robot pose\n        # ... (joint angle randomization)\n        \n        # Randomize camera angle\n        camera_angle = random.uniform(-30, 30)  # degrees\n        # ... (camera pose update)\n        \n    def capture_frame(self):\n        """Capture RGB, depth, and segmentation images"""\n        # Step simulation\n        self.world.step(render=True)\n        \n        # Get camera data\n        rgb_data = self.camera.get_rgba()  # H\xd7W\xd74 (RGBA)\n        depth_data = self.camera.get_current_frame()["distance_to_image_plane"]  # H\xd7W\n        segmentation_data = self.camera.get_current_frame()["semantic_segmentation"]  # H\xd7W\n        \n        # Convert to standard formats\n        rgb_image = (rgb_data[:, :, :3] * 255).astype(np.uint8)  # Remove alpha, scale to 0 to 255\n        depth_image = (depth_data * 1000).astype(np.uint16)  # Convert to mm, 16-bit\n        seg_image = segmentation_data.astype(np.uint8)  # Class IDs\n        \n        # Save images\n        frame_id = f"{self.frame_count:06d}"\n        cv2.imwrite(str(RGB_DIR / f"{frame_id}.png"), cv2.cvtColor(rgb_image, cv2.COLOR_RGB2BGR))\n        cv2.imwrite(str(DEPTH_DIR / f"{frame_id}.png"), depth_image)\n        cv2.imwrite(str(SEGMENTATION_DIR / f"{frame_id}.png"), seg_image)\n        \n        # Generate annotations (bounding boxes)\n        annotations = self.generate_annotations(seg_image)\n        \n        return annotations\n    \n    def generate_annotations(self, seg_image):\n        """Generate bounding box annotations from segmentation"""\n        annotations = []\n        \n        # Find unique object IDs (excluding background=0)\n        unique_ids = np.unique(seg_image)\n        unique_ids = unique_ids[unique_ids > 0]\n        \n        for obj_id in unique_ids:\n            # Find bounding box\n            mask = (seg_image == obj_id)\n            rows = np.any(mask, axis=1)\n            cols = np.any(mask, axis=0)\n            \n            if np.any(rows) and np.any(cols):\n                y_min, y_max = np.where(rows)[0][[0, -1]]\n                x_min, x_max = np.where(cols)[0][[0, -1]]\n                \n                # COCO format: [x_min, y_min, width, height]\n                bbox = [int(x_min), int(y_min), int(x_max - x_min), int(y_max - y_min)]\n                area = bbox[2] * bbox[3]\n                \n                annotations.append({\n                    "id": len(annotations),\n                    "image_id": self.frame_count,\n                    "category_id": int(obj_id),\n                    "bbox": bbox,\n                    "area": area,\n                    "iscrowd": 0\n                })\n        \n        return annotations\n    \n    def collect_dataset(self):\n        """Main data collection loop"""\n        print(f"Starting data collection: {NUM_IMAGES} images")\n        \n        for i in range(NUM_IMAGES):\n            # Randomize scene\n            self.randomize_scene()\n            \n            # Capture frame\n            annotations = self.capture_frame()\n            self.annotations.extend(annotations)\n            \n            self.frame_count += 1\n            \n            if (i + 1) % 50 == 0:\n                print(f"Collected {i + 1}/{NUM_IMAGES} images")\n        \n        # Save annotations (COCO format)\n        self.save_coco_annotations()\n        \n        print(f"Data collection complete: {self.frame_count} images")\n    \n    def save_coco_annotations(self):\n        """Save annotations in COCO format"""\n        coco_data = {\n            "images": [\n                {\n                    "id": i,\n                    "width": IMAGE_WIDTH,\n                    "height": IMAGE_HEIGHT,\n                    "file_name": f"{i:06d}.png"\n                }\n                for i in range(self.frame_count)\n            ],\n            "annotations": self.annotations,\n            "categories": [\n                {"id": 1, "name": "robot"},\n                {"id": 2, "name": "object"},\n                # ... (add more categories)\n            ]\n        }\n        \n        with open(ANNOTATIONS_DIR / "annotations.json", "w") as f:\n            json.dump(coco_data, f, indent=2)\n\nif __name__ == "__main__":\n    collector = DataCollector()\n    collector.world.reset()\n    collector.collect_dataset()\n    simulation_app.close()\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-2-configure-domain-randomization",children:"Step 2: Configure Domain Randomization"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Enhanced randomization"})," (",(0,a.jsx)(e.code,{children:"scripts/domain_randomization.py"}),"):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'class DomainRandomizer:\n    def __init__(self, world):\n        self.world = world\n        \n    def randomize_lighting(self):\n        """Randomize scene lighting"""\n        # Get dome light\n        light_prim = self.world.stage.GetPrimAtPath("/World/DomeLight")\n        \n        # Random intensity\n        intensity = random.uniform(0.5, 3.0)\n        light_prim.GetAttribute("intensity").Set(intensity)\n        \n        # Random color temperature\n        color_temp = random.uniform(3000, 7000)  # Kelvin\n        # Convert to RGB (simplified)\n        rgb = self.kelvin_to_rgb(color_temp)\n        light_prim.GetAttribute("color").Set(rgb)\n        \n    def randomize_materials(self):\n        """Randomize object materials"""\n        for obj in self.objects:\n            material = obj.get_applied_material()\n            \n            # Random base color\n            base_color = [\n                random.uniform(0, 1),\n                random.uniform(0, 1),\n                random.uniform(0, 1)\n            ]\n            material.set_attribute("base_color", base_color)\n            \n            # Random roughness\n            roughness = random.uniform(0.1, 0.9)\n            material.set_attribute("roughness", roughness)\n    \n    def randomize_camera(self):\n        """Randomize camera pose"""\n        # Random position (around robot)\n        angle = random.uniform(0, 2 * np.pi)\n        distance = random.uniform(1.5, 3.0)\n        height = random.uniform(1.2, 2.0)\n        \n        x = distance * np.cos(angle)\n        y = distance * np.sin(angle)\n        z = height\n        \n        # Random orientation (look at robot)\n        # ... (camera pose calculation)\n    \n    def kelvin_to_rgb(self, kelvin):\n        """Convert color temperature to RGB"""\n        # Simplified conversion (use proper algorithm in production)\n        if kelvin < 4000:\n            # Warm (reddish)\n            return [1.0, 0.7, 0.5]\n        elif kelvin < 6000:\n            # Neutral (white)\n            return [1.0, 1.0, 1.0]\n        else:\n            # Cool (bluish)\n            return [0.8, 0.9, 1.0]\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-3-run-data-collection",children:"Step 3: Run Data Collection"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Execute script"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"cd ~/isaac_sim_ws\npython3 scripts/collect_synthetic_data.py\n"})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Expected Output"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{children:"Starting data collection: 500 images\nCollected 50/500 images\nCollected 100/500 images\n...\nCollected 500/500 images\nData collection complete: 500 images\n"})}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Verify output"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-bash",children:"# Check directories\nls ~/synthetic_data/rgb/ | wc -l  # Should show 500\nls ~/synthetic_data/depth/ | wc -l\nls ~/synthetic_data/segmentation/ | wc -l\n\n# View sample image\neog ~/synthetic_data/rgb/000000.png\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-4-export-to-yolo-format",children:"Step 4: Export to YOLO Format"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Convert COCO to YOLO"})," (",(0,a.jsx)(e.code,{children:"scripts/coco_to_yolo.py"}),"):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nConvert COCO annotations to YOLO format\n\"\"\"\nimport json\nfrom pathlib import Path\n\ndef coco_to_yolo(coco_file, output_dir, image_width, image_height):\n    \"\"\"Convert COCO JSON to YOLO txt files\"\"\"\n    with open(coco_file, 'r') as f:\n        coco_data = json.load(f)\n    \n    # Create mapping: image_id -> annotations\n    image_annotations = {}\n    for ann in coco_data['annotations']:\n        image_id = ann['image_id']\n        if image_id not in image_annotations:\n            image_annotations[image_id] = []\n        image_annotations[image_id].append(ann)\n    \n    # Create category mapping\n    category_map = {cat['id']: idx for idx, cat in enumerate(coco_data['categories'])}\n    \n    # Convert each image's annotations\n    for image_info in coco_data['images']:\n        image_id = image_info['id']\n        file_name = image_info['file_name'].replace('.png', '.txt')\n        \n        yolo_file = output_dir / file_name\n        with open(yolo_file, 'w') as f:\n            if image_id in image_annotations:\n                for ann in image_annotations[image_id]:\n                    # YOLO format: class_id center_x center_y width height (normalized 0 to 1)\n                    bbox = ann['bbox']  # [x_min, y_min, width, height]\n                    center_x = (bbox[0] + bbox[2] / 2) / image_width\n                    center_y = (bbox[1] + bbox[3] / 2) / image_height\n                    width = bbox[2] / image_width\n                    height = bbox[3] / image_height\n                    \n                    class_id = category_map[ann['category_id']]\n                    \n                    f.write(f\"{class_id} {center_x} {center_y} {width} {height}\\n\")\n\n# Usage\ncoco_file = Path(\"~/synthetic_data/annotations/annotations.json\")\nyolo_dir = Path(\"~/synthetic_data/yolo_labels\")\nyolo_dir.mkdir(exist_ok=True)\n\ncoco_to_yolo(coco_file, yolo_dir, 1920, 1080)\n"})}),"\n",(0,a.jsx)(e.h3,{id:"step-5-validate-dataset-quality",children:"Step 5: Validate Dataset Quality"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Create validation script"})," (",(0,a.jsx)(e.code,{children:"scripts/validate_dataset.py"}),"):"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nValidate synthetic dataset quality\n"""\nimport cv2\nimport numpy as np\nfrom pathlib import Path\nimport json\n\ndef validate_dataset(data_dir):\n    """Check dataset for common issues"""\n    rgb_dir = Path(data_dir) / "rgb"\n    depth_dir = Path(data_dir) / "depth"\n    seg_dir = Path(data_dir) / "segmentation"\n    ann_file = Path(data_dir) / "annotations" / "annotations.json"\n    \n    issues = []\n    \n    # Check image counts match\n    rgb_count = len(list(rgb_dir.glob("*.png")))\n    depth_count = len(list(depth_dir.glob("*.png")))\n    seg_count = len(list(seg_dir.glob("*.png")))\n    \n    if rgb_count != depth_count or rgb_count != seg_count:\n        issues.append(f"Image count mismatch: RGB={rgb_count}, Depth={depth_count}, Seg={seg_count}")\n    \n    # Check annotations\n    with open(ann_file, \'r\') as f:\n        annotations = json.load(f)\n    \n    if len(annotations[\'images\']) != rgb_count:\n        issues.append(f"Annotation count mismatch: {len(annotations[\'images\'])} vs {rgb_count}")\n    \n    # Check image quality (sample check)\n    sample_image = cv2.imread(str(rgb_dir / "000000.png"))\n    if sample_image is None:\n        issues.append("Sample image failed to load")\n    else:\n        # Check for blank images\n        if np.mean(sample_image) < 10:\n            issues.append("Sample image appears blank (too dark)")\n    \n    # Check annotation coverage\n    images_with_annotations = set(ann[\'image_id\'] for ann in annotations[\'annotations\'])\n    if len(images_with_annotations) < rgb_count * 0.9:  # 90% should have annotations\n        issues.append(f"Low annotation coverage: {len(images_with_annotations)}/{rgb_count}")\n    \n    return issues\n\n# Run validation\ndata_dir = Path("~/synthetic_data")\nissues = validate_dataset(data_dir)\n\nif issues:\n    print("Dataset validation issues:")\n    for issue in issues:\n        print(f"  - {issue}")\nelse:\n    print("Dataset validation passed!")\n'})}),"\n",(0,a.jsx)(e.h3,{id:"step-6-debugging-common-issues",children:"Step 6: Debugging Common Issues"}),"\n",(0,a.jsx)(e.h4,{id:"issue-1-no-images-generated-or-empty-directories",children:'Issue 1: "No images generated" or "Empty directories"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Script runs but no files created"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Check output directory permissions\nimport os\nprint(os.access(OUTPUT_DIR, os.W_OK))  # Should be True\n\n# Verify camera is capturing\nrgb_data = self.camera.get_rgba()\nprint(f"RGB shape: {rgb_data.shape}")  # Should be (H, W, 4)\n\n# Check file paths\nprint(f"Saving to: {RGB_DIR / \'000000.png\'}")\n'})}),"\n",(0,a.jsx)(e.h4,{id:"issue-2-segmentation-mask-all-zeros",children:'Issue 2: "Segmentation mask all zeros"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": Segmentation images are blank"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Enable semantic segmentation in camera\nself.camera.enable_semantic_segmentation()\n\n# Assign semantic labels to objects\nfrom omni.isaac.core.utils.prims import set_targets\nset_targets(prim_path="/World/Robot", semantic_label="robot")\nset_targets(prim_path="/World/Objects", semantic_label="object")\n'})}),"\n",(0,a.jsx)(e.h4,{id:"issue-3-annotations-missing-bounding-boxes",children:'Issue 3: "Annotations missing bounding boxes"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": JSON file has empty annotations array"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Verify segmentation data\nseg_data = self.camera.get_current_frame()["semantic_segmentation"]\nprint(f"Segmentation unique values: {np.unique(seg_data)}")\n\n# Check object IDs match category mapping\n# Ensure objects have semantic labels assigned\n'})}),"\n",(0,a.jsx)(e.h4,{id:"issue-4-domain-randomization-not-working",children:'Issue 4: "Domain randomization not working"'}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Symptoms"}),": All images look similar"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Solutions"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Verify randomization is called\nprint(f"Random seed: {random.getstate()}")\n\n# Check randomization ranges\nlight_intensity = random.uniform(0.5, 3.0)\nprint(f"Light intensity: {light_intensity}")  # Should vary\n\n# Reset random seed if needed\nrandom.seed()  # Use system time\n'})}),"\n",(0,a.jsx)(e.h2,{id:"part-3-advanced-topics-optional",children:"Part 3: Advanced Topics (Optional)"}),"\n",(0,a.jsx)(e.h3,{id:"multi-view-data-collection",children:"Multi-View Data Collection"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Capture from multiple cameras"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'# Create multiple cameras\ncameras = [\n    Camera(prim_path="/World/Camera_Front", position=[0, -2, 1.6]),\n    Camera(prim_path="/World/Camera_Left", position=[-2, 0, 1.6]),\n    Camera(prim_path="/World/Camera_Right", position=[2, 0, 1.6]),\n]\n\n# Capture from all cameras\nfor cam in cameras:\n    rgb = cam.get_rgba()\n    # Save with camera identifier\n'})}),"\n",(0,a.jsx)(e.h3,{id:"active-learning-pipeline",children:"Active Learning Pipeline"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Select diverse samples"}),":"]}),"\n",(0,a.jsx)(e.pre,{children:(0,a.jsx)(e.code,{className:"language-python",children:'def select_diverse_samples(images, n_samples=100):\n    """Select diverse images using clustering"""\n    from sklearn.cluster import KMeans\n    \n    # Extract features (e.g., color histogram)\n    features = [extract_features(img) for img in images]\n    \n    # Cluster\n    kmeans = KMeans(n_clusters=n_samples)\n    kmeans.fit(features)\n    \n    # Select samples closest to cluster centers\n    # ... (sample selection logic)\n'})}),"\n",(0,a.jsx)(e.h2,{id:"integration-with-capstone",children:"Integration with Capstone"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"How this chapter contributes"})," to the Week 13 autonomous humanoid:"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Training data"}),": Capstone vision models will use synthetic data from Isaac Sim"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Domain adaptation"}),": Randomization ensures models generalize to real-world"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Rapid iteration"}),": Generate new datasets quickly as requirements change"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.strong,{children:"Perfect labels"}),": Automatic annotations enable supervised learning"]}),"\n"]}),"\n",(0,a.jsx)(e.p,{children:"Understanding synthetic data generation now is essential for training the capstone perception system."}),"\n",(0,a.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,a.jsx)(e.p,{children:"You learned:"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:["\u2705 Set up ",(0,a.jsx)(e.strong,{children:"data collection pipelines"})," in Isaac Sim"]}),"\n",(0,a.jsxs)(e.li,{children:["\u2705 Generated ",(0,a.jsx)(e.strong,{children:"RGB, depth, and segmentation images"})," with annotations"]}),"\n",(0,a.jsxs)(e.li,{children:["\u2705 Implemented ",(0,a.jsx)(e.strong,{children:"domain randomization"})," for robust training"]}),"\n",(0,a.jsxs)(e.li,{children:["\u2705 Exported ",(0,a.jsx)(e.strong,{children:"datasets in COCO and YOLO formats"})]}),"\n",(0,a.jsxs)(e.li,{children:["\u2705 Validated ",(0,a.jsx)(e.strong,{children:"synthetic data quality"})," and diversity"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Next steps"}),": In Chapter 3.3, you'll set up hardware-accelerated VSLAM using Isaac ROS."]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"exercises",children:"Exercises"}),"\n",(0,a.jsx)(e.h3,{id:"exercise-1-basic-data-collection-required",children:"Exercise 1: Basic Data Collection (Required)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Generate 100 labeled images with bounding boxes."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Create data collection script"}),"\n",(0,a.jsx)(e.li,{children:"Set up scene with robot and 3 objects"}),"\n",(0,a.jsx)(e.li,{children:"Configure camera for RGB + segmentation"}),"\n",(0,a.jsx)(e.li,{children:"Collect 100 images with annotations"}),"\n",(0,a.jsx)(e.li,{children:"Export to COCO format"}),"\n",(0,a.jsx)(e.li,{children:"Validate dataset quality"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,a.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","100 RGB images generated"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","100 segmentation masks generated"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","COCO annotations file created"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","All images have at least one annotation"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Dataset validation passes"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-2-domain-randomization-required",children:"Exercise 2: Domain Randomization (Required)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Implement comprehensive domain randomization."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Randomize lighting (intensity, color)"}),"\n",(0,a.jsx)(e.li,{children:"Randomize object positions and orientations"}),"\n",(0,a.jsx)(e.li,{children:"Randomize camera angles"}),"\n",(0,a.jsx)(e.li,{children:"Randomize materials (colors, textures)"}),"\n",(0,a.jsx)(e.li,{children:"Collect 200 images with randomization"}),"\n",(0,a.jsx)(e.li,{children:"Compare diversity (visual inspection)"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,a.jsxs)(e.ul,{className:"contains-task-list",children:["\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Lighting varies across images"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Object positions randomized"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Camera angles vary"]}),"\n",(0,a.jsxs)(e.li,{className:"task-list-item",children:[(0,a.jsx)(e.input,{type:"checkbox",disabled:!0})," ","Visual diversity confirmed"]}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Estimated Time"}),": 90 minutes"]}),"\n",(0,a.jsx)(e.h3,{id:"exercise-3-multi-format-export-challenge",children:"Exercise 3: Multi-Format Export (Challenge)"}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Objective"}),": Export dataset in multiple formats."]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Tasks"}),":"]}),"\n",(0,a.jsxs)(e.ol,{children:["\n",(0,a.jsx)(e.li,{children:"Export to COCO format (already done)"}),"\n",(0,a.jsx)(e.li,{children:"Convert to YOLO format"}),"\n",(0,a.jsx)(e.li,{children:"Create custom format (e.g., TensorFlow TFRecord)"}),"\n",(0,a.jsx)(e.li,{children:"Generate dataset statistics (class distribution, image sizes)"}),"\n",(0,a.jsx)(e.li,{children:"Create dataset README with format descriptions"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Requirements"}),":"]}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsx)(e.li,{children:"COCO JSON file"}),"\n",(0,a.jsx)(e.li,{children:"YOLO txt files (one per image)"}),"\n",(0,a.jsx)(e.li,{children:"Custom format (your choice)"}),"\n",(0,a.jsx)(e.li,{children:"Statistics report (JSON/Markdown)"}),"\n"]}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Estimated Time"}),": 180 minutes"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsx)(e.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,a.jsxs)(e.ul,{children:["\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://docs.omniverse.nvidia.com/app_isaacsim/app_isaacsim/tutorial_core_adding_sensors.html",children:"Isaac Sim Data Collection"})," - Official tutorial"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://cocodataset.org/#format-data",children:"COCO Format"})," - Annotation format specification"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data",children:"YOLO Format"})," - YOLO annotation guide"]}),"\n",(0,a.jsxs)(e.li,{children:[(0,a.jsx)(e.a,{href:"https://arxiv.org/abs/1703.06907",children:"Domain Randomization"})," - Research paper"]}),"\n"]}),"\n",(0,a.jsx)(e.hr,{}),"\n",(0,a.jsxs)(e.p,{children:[(0,a.jsx)(e.strong,{children:"Next"}),": [Chapter 3.3: Isaac ROS - Hardware-Accelerated VSLAM \u2192](chapter-3 to 3.md)"]})]})}function m(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,a.jsx)(e,{...n,children:(0,a.jsx)(c,{...n})}):c(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>o,x:()=>r});var t=i(6540);const a={},s=t.createContext(a);function o(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function r(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(a):n.components||a:o(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);
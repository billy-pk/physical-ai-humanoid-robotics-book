"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[9325],{8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var r=s(6540);const i={},a=r.createContext(i);function o(e){const n=r.useContext(a);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(i):e.components||i:o(e.components),r.createElement(a.Provider,{value:n},e.children)}},8506:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>c,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>t});const r=JSON.parse('{"id":"module-2/chapter-2-3","title":"2.3 Sensor Simulation (LiDAR, Cameras, IMUs)","description":"Sensors are the eyes and ears of humanoid robots. Gazebo provides realistic sensor models that publish ROS 2 topics, enabling you to develop perception algorithms in simulation before deploying to hardware. This chapter covers camera, LiDAR, and IMU sensor plugins.","source":"@site/docs/module-2/chapter-2-3.md","sourceDirName":"module-2","slug":"/module-2/chapter-2-3","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/chapter-2-3.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"2.3 Sensor Simulation (LiDAR, Cameras, IMUs)"},"sidebar":"defaultSidebar","previous":{"title":"2.2 Physics Simulation (Gravity, Collisions, Friction)","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2"},"next":{"title":"2.4 URDF/SDF Robot Description","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-4"}}');var i=s(4848),a=s(8453);const o={sidebar_position:4,title:"2.3 Sensor Simulation (LiDAR, Cameras, IMUs)"},l="Chapter 2.3: Sensor Simulation (LiDAR, Cameras, IMUs)",c={},t=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: Sensor Plugin Architecture",id:"part-1-sensor-plugin-architecture",level:2},{value:"How Gazebo Sensors Work",id:"how-gazebo-sensors-work",level:3},{value:"ROS 2 Message Types",id:"ros-2-message-types",level:3},{value:"Sensor Noise Models",id:"sensor-noise-models",level:3},{value:"Part 2: Hands-On Tutorial",id:"part-2-hands-on-tutorial",level:2},{value:"Project: Add Sensors to Humanoid Robot",id:"project-add-sensors-to-humanoid-robot",level:3},{value:"Step 1: Add Camera Sensor to Head",id:"step-1-add-camera-sensor-to-head",level:3},{value:"Step 2: Add LiDAR Sensor to Torso",id:"step-2-add-lidar-sensor-to-torso",level:3},{value:"Step 3: Add IMU Sensor to Torso",id:"step-3-add-imu-sensor-to-torso",level:3},{value:"Step 4: Launch Robot and Verify Sensors",id:"step-4-launch-robot-and-verify-sensors",level:3},{value:"Step 5: Visualize Sensor Data in RViz2",id:"step-5-visualize-sensor-data-in-rviz2",level:3},{value:"Step 6: Process Sensor Data in ROS 2 Node",id:"step-6-process-sensor-data-in-ros-2-node",level:3},{value:"Step 7: Debugging Common Sensor Issues",id:"step-7-debugging-common-sensor-issues",level:3},{value:"Issue 1: Camera Not Publishing",id:"issue-1-camera-not-publishing",level:4},{value:"Issue 2: LiDAR Shows No Data",id:"issue-2-lidar-shows-no-data",level:4},{value:"Issue 3: IMU Shows Zero Values",id:"issue-3-imu-shows-zero-values",level:4},{value:"Issue 4: Sensor Data Too Noisy",id:"issue-4-sensor-data-too-noisy",level:4},{value:"Part 3: Advanced Topics (Optional)",id:"part-3-advanced-topics-optional",level:2},{value:"Depth Camera (RGB-D)",id:"depth-camera-rgb-d",level:3},{value:"Stereo Camera Pair",id:"stereo-camera-pair",level:3},{value:"Integration with Capstone",id:"integration-with-capstone",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Camera Calibration (Required)",id:"exercise-1-camera-calibration-required",level:3},{value:"Exercise 2: LiDAR Obstacle Detection (Required)",id:"exercise-2-lidar-obstacle-detection-required",level:3},{value:"Exercise 3: IMU Balance Detection (Challenge)",id:"exercise-3-imu-balance-detection-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,a.R)(),...e.components};return(0,i.jsxs)(i.Fragment,{children:[(0,i.jsx)(n.header,{children:(0,i.jsx)(n.h1,{id:"chapter-23-sensor-simulation-lidar-cameras-imus",children:"Chapter 2.3: Sensor Simulation (LiDAR, Cameras, IMUs)"})}),"\n",(0,i.jsx)(n.p,{children:"Sensors are the eyes and ears of humanoid robots. Gazebo provides realistic sensor models that publish ROS 2 topics, enabling you to develop perception algorithms in simulation before deploying to hardware. This chapter covers camera, LiDAR, and IMU sensor plugins."}),"\n",(0,i.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,i.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Add"})," camera plugins to robot models publishing ROS 2 Image topics"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configure"})," LiDAR sensors for SLAM and obstacle detection"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulate"})," IMU data for balance control and orientation estimation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibrate"})," sensor noise models to match real hardware"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Process"})," sensor data in ROS 2 nodes for perception pipelines"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gazebo Classic 11"})," with ROS 2 integration (Chapters 2.1 to 2.2)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"ROS 2 Humble"})," with ",(0,i.jsx)(n.code,{children:"rclpy"})," experience (Module 1)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Understanding"})," of ROS 2 topics and messages (Module 1)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Basic image processing"})," concepts (pixels, frames, resolution)"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"part-1-sensor-plugin-architecture",children:"Part 1: Sensor Plugin Architecture"}),"\n",(0,i.jsx)(n.h3,{id:"how-gazebo-sensors-work",children:"How Gazebo Sensors Work"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Gazebo sensors"})," are plugins that:"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Simulate"})," physical sensor behavior (optics, noise, latency)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Publish"})," ROS 2 topics with sensor data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Configure"})," via SDF/URDF parameters (resolution, frame rate, noise)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Sensor types"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Camera"}),": RGB images, depth images, stereo pairs"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"LiDAR"}),": 2D/3D laser scans for SLAM"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"IMU"}),": Accelerometer, gyroscope, magnetometer"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Force/Torque"}),": Contact sensors for manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"GPS"}),": Localization (for outdoor robots)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"ros-2-message-types",children:"ROS 2 Message Types"}),"\n",(0,i.jsxs)(n.table,{children:[(0,i.jsx)(n.thead,{children:(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.th,{children:"Sensor"}),(0,i.jsx)(n.th,{children:"Gazebo Plugin"}),(0,i.jsx)(n.th,{children:"ROS 2 Message Type"}),(0,i.jsx)(n.th,{children:"Package"})]})}),(0,i.jsxs)(n.tbody,{children:[(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Camera"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"libgazebo_ros_camera.so"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})}),(0,i.jsx)(n.td,{children:"sensor_msgs"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"Depth Camera"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"libgazebo_ros_depth_camera.so"})}),(0,i.jsxs)(n.td,{children:[(0,i.jsx)(n.code,{children:"sensor_msgs/Image"})," (16-bit)"]}),(0,i.jsx)(n.td,{children:"sensor_msgs"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"LiDAR"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"libgazebo_ros_ray_sensor.so"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"sensor_msgs/LaserScan"})}),(0,i.jsx)(n.td,{children:"sensor_msgs"})]}),(0,i.jsxs)(n.tr,{children:[(0,i.jsx)(n.td,{children:(0,i.jsx)(n.strong,{children:"IMU"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"libgazebo_ros_imu_sensor.so"})}),(0,i.jsx)(n.td,{children:(0,i.jsx)(n.code,{children:"sensor_msgs/Imu"})}),(0,i.jsx)(n.td,{children:"sensor_msgs"})]})]})]}),"\n",(0,i.jsx)(n.h3,{id:"sensor-noise-models",children:"Sensor Noise Models"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Real sensors have noise"}),". Gazebo models:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Gaussian noise"}),": Random variations (like camera sensor noise)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Bias"}),": Systematic offset (like IMU drift)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Quantization"}),": Discrete steps (like digital sensors)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Why noise matters"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Realistic testing"}),": Algorithms must work with noisy data"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Robustness"}),": Test failure modes (low light, sensor failures)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Calibration"}),": Match simulation to real sensor characteristics"]}),"\n"]}),"\n",(0,i.jsx)(n.h2,{id:"part-2-hands-on-tutorial",children:"Part 2: Hands-On Tutorial"}),"\n",(0,i.jsx)(n.h3,{id:"project-add-sensors-to-humanoid-robot",children:"Project: Add Sensors to Humanoid Robot"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Goal"}),": Add camera, LiDAR, and IMU sensors to the humanoid model and verify ROS 2 topic publishing."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tools"}),": Gazebo Classic 11, ROS 2 Humble, URDF/SDF"]}),"\n",(0,i.jsx)(n.h3,{id:"step-1-add-camera-sensor-to-head",children:"Step 1: Add Camera Sensor to Head"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Modify URDF"})," (",(0,i.jsx)(n.code,{children:"models/simple_humanoid/model.urdf"}),"):"]}),"\n",(0,i.jsx)(n.p,{children:"Add head link and camera:"}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Head Link --\x3e\n<link name="head">\n  <visual name="head_visual">\n    <geometry>\n      <sphere radius="0.1"/>\n    </geometry>\n    <material name="blue">\n      <color rgba="0 0 1 1"/>\n    </material>\n  </visual>\n  <collision name="head_collision">\n    <geometry>\n      <sphere radius="0.1"/>\n    </geometry>\n  </collision>\n  <inertial>\n    <mass value="1.0"/>\n    <origin xyz="0 0 0" rpy="0 0 0"/>\n    <inertia ixx="0.01" ixy="0" ixz="0" iyy="0.01" iyz="0" izz="0.01"/>\n  </inertial>\n</link>\n\n\x3c!-- Head Joint --\x3e\n<joint name="head_joint" type="fixed">\n  <parent link="torso"/>\n  <child link="head"/>\n  <origin xyz="0 0 0.2" rpy="0 0 0"/>\n</joint>\n\n\x3c!-- Camera Sensor (Gazebo Plugin) --\x3e\n<gazebo reference="head">\n  <sensor name="camera" type="camera">\n    <update_rate>30</update_rate>  \x3c!-- 30 Hz --\x3e\n    <camera>\n      <horizontal_fov>1.047</horizontal_fov>  \x3c!-- 60 degrees --\x3e\n      <image>\n        <width>640</width>\n        <height>480</height>\n      </image>\n      <clip>\n        <near>0.05</near>\n        <far>100</far>\n      </clip>\n    </camera>\n    <plugin name="camera_controller" filename="libgazebo_ros_camera.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>image_raw:=camera/image_raw</remapping>\n      </ros>\n      <camera_name>head_camera</camera_name>\n      <frame_name>head_camera_frame</frame_name>\n      <hack_baseline>0.07</hack_baseline>\n      <min_depth>0.05</min_depth>\n      <max_depth>100</max_depth>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key parameters"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"update_rate"})}),": Frames per second (30 Hz = real-time video)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"horizontal_fov"})}),": Field of view in radians (1.047 \u2248 60\xb0)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"width/height"})}),": Image resolution (640\xd7480 = VGA)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"frame_name"})}),": TF frame for camera (for coordinate transforms)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-2-add-lidar-sensor-to-torso",children:"Step 2: Add LiDAR Sensor to Torso"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add LiDAR"})," (after camera sensor):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- LiDAR Sensor (Gazebo Plugin) --\x3e\n<gazebo reference="torso">\n  <sensor name="lidar" type="ray">\n    <pose>0 0 0.2 0 0 0</pose>  \x3c!-- Position on torso --\x3e\n    <visualize>true</visualize>  \x3c!-- Show rays in Gazebo --\x3e\n    <update_rate>10</update_rate>  \x3c!-- 10 Hz --\x3e\n    <ray>\n      <scan>\n        <horizontal>\n          <samples>360</samples>  \x3c!-- 360 points per scan --\x3e\n          <resolution>1.0</resolution>\n          <min_angle>-3.14159</min_angle>  \x3c!-- -180\xb0 --\x3e\n          <max_angle>3.14159</max_angle>   \x3c!-- +180\xb0 --\x3e\n        </horizontal>\n      </scan>\n      <range>\n        <min>0.1</min>   \x3c!-- 10 cm minimum --\x3e\n        <max>10.0</max>  \x3c!-- 10 m maximum --\x3e\n        <resolution>0.01</resolution>  \x3c!-- 1 cm resolution --\x3e\n      </range>\n    </ray>\n    <plugin name="lidar_controller" filename="libgazebo_ros_ray_sensor.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>scan:=lidar/scan</remapping>\n      </ros>\n      <output_type>sensor_msgs/LaserScan</output_type>\n      <frame_name>lidar_frame</frame_name>\n    </plugin>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key parameters"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"samples"})}),": Number of laser rays (360 = 1\xb0 resolution)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"min/max_angle"})}),": Scanning range (-180\xb0 to +180\xb0 = full circle)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsxs)(n.strong,{children:[(0,i.jsx)(n.code,{children:"min/max"})," range"]}),": Detection distance (0.1 to 10 m)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"visualize"})}),": Show laser rays in Gazebo GUI (helpful for debugging)"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-3-add-imu-sensor-to-torso",children:"Step 3: Add IMU Sensor to Torso"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add IMU"})," (after LiDAR sensor):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- IMU Sensor (Gazebo Plugin) --\x3e\n<gazebo reference="torso">\n  <sensor name="imu" type="imu">\n    <plugin name="imu_controller" filename="libgazebo_ros_imu_sensor.so">\n      <ros>\n        <namespace>/humanoid</namespace>\n        <remapping>imu:=imu/data</remapping>\n      </ros>\n      <frame_name>imu_frame</frame_name>\n      <initial_orientation_as_reference>false</initial_orientation_as_reference>\n    </plugin>\n    <always_on>true</always_on>\n    <update_rate>100</update_rate>  \x3c!-- 100 Hz --\x3e\n    <imu>\n      <angular_velocity>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>  \x3c!-- Gyroscope noise --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.0002</stddev>\n          </noise>\n        </z>\n      </angular_velocity>\n      <linear_acceleration>\n        <x>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>  \x3c!-- Accelerometer noise --\x3e\n          </noise>\n        </x>\n        <y>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </y>\n        <z>\n          <noise type="gaussian">\n            <mean>0.0</mean>\n            <stddev>0.017</stddev>\n          </noise>\n        </z>\n      </linear_acceleration>\n    </imu>\n  </sensor>\n</gazebo>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Key parameters"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"update_rate"})}),": IMU frequency (100 Hz typical for balance control)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"noise"})}),": Gaussian noise model (matches real IMU characteristics)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"stddev"})}),": Standard deviation (higher = more noise)"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:(0,i.jsx)(n.code,{children:"initial_orientation_as_reference"})}),": Use starting pose as reference frame"]}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-4-launch-robot-and-verify-sensors",children:"Step 4: Launch Robot and Verify Sensors"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch file"})," (",(0,i.jsx)(n.code,{children:"launch/humanoid_sensors.launch.py"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nLaunch humanoid with sensors in Gazebo\nROS 2 Humble | Gazebo Classic 11\n\"\"\"\nfrom launch import LaunchDescription\nfrom launch.actions import ExecuteProcess\nfrom launch_ros.actions import Node\nfrom launch_ros.substitutions import FindPackageShare\nimport os\n\ndef generate_launch_description():\n    pkg_share = FindPackageShare('gazebo_worlds').find('gazebo_worlds')\n    world_path = os.path.join(pkg_share, 'worlds', 'humanoid_physics.world')\n    urdf_path = os.path.join(pkg_share, 'models', 'simple_humanoid', 'model.urdf')\n\n    return LaunchDescription([\n        # Launch Gazebo\n        ExecuteProcess(\n            cmd=['gazebo', '--verbose', world_path],\n            output='screen'\n        ),\n        \n        # Spawn humanoid\n        Node(\n            package='gazebo_ros',\n            executable='spawn_entity.py',\n            arguments=['-entity', 'humanoid', '-file', urdf_path, '-x', '0', '-y', '0', '-z', '1.0'],\n            output='screen'\n        ),\n        \n        # Robot state publisher\n        Node(\n            package='robot_state_publisher',\n            executable='robot_state_publisher',\n            name='robot_state_publisher',\n            arguments=[urdf_path],\n            output='screen'\n        )\n    ])\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch and verify"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/gazebo_ws\ncolcon build --packages-select gazebo_worlds\nsource install/setup.bash\n\nros2 launch gazebo_worlds humanoid_sensors.launch.py\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Terminal 2: Check ROS 2 topics"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"source /opt/ros/humble/setup.bash\n\n# List all topics\nros2 topic list\n\n# You should see:\n# /humanoid/camera/image_raw\n# /humanoid/lidar/scan\n# /humanoid/imu/data\n# /clock\n# /model_states\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Verify camera"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check camera topic info\nros2 topic info /humanoid/camera/image_raw\n\n# Echo camera messages (header info)\nros2 topic echo /humanoid/camera/image_raw --no-arr | head -20\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Verify LiDAR"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check LiDAR topic\nros2 topic info /humanoid/lidar/scan\n\n# Echo LiDAR scan\nros2 topic echo /humanoid/lidar/scan --no-arr | head -30\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Verify IMU"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check IMU topic\nros2 topic info /humanoid/imu/data\n\n# Echo IMU data\nros2 topic echo /humanoid/imu/data\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"})," (IMU):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"header:\n  stamp:\n    sec: 0\n    nanosec: 123456789\n  frame_id: imu_frame\norientation:\n  x: 0.0\n  y: 0.0\n  z: 0.0\n  w: 1.0\nangular_velocity:\n  x: 0.001\n  y: -0.002\n  z: 0.0005\nlinear_acceleration:\n  x: 0.01\n  y: -0.02\n  z: -9.81  \x3c!-- Gravity in Z direction --\x3e\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-5-visualize-sensor-data-in-rviz2",children:"Step 5: Visualize Sensor Data in RViz2"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Launch RViz2"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"rviz2\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Configure displays"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add Image display"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Topic: ",(0,i.jsx)(n.code,{children:"/humanoid/camera/image_raw"})]}),"\n",(0,i.jsx)(n.li,{children:"You should see camera feed from robot's perspective"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add LaserScan display"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Topic: ",(0,i.jsx)(n.code,{children:"/humanoid/lidar/scan"})]}),"\n",(0,i.jsx)(n.li,{children:"You should see 2D laser scan visualization"}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.li,{children:["\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add TF display"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Shows coordinate frames (camera_frame, lidar_frame, imu_frame)"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Result"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Camera shows robot's view of Gazebo world"}),"\n",(0,i.jsx)(n.li,{children:"LiDAR shows distance measurements as colored points"}),"\n",(0,i.jsx)(n.li,{children:"IMU data visible in terminal (orientation, angular velocity, acceleration)"}),"\n"]}),"\n",(0,i.jsx)(n.h3,{id:"step-6-process-sensor-data-in-ros-2-node",children:"Step 6: Process Sensor Data in ROS 2 Node"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Create sensor processor node"}),": ",(0,i.jsx)(n.code,{children:"src/sensor_processor.py"})]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nProcess sensor data from humanoid robot\nROS 2 Humble | Python 3.10+\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, LaserScan, Imu\nimport cv2\nfrom cv_bridge import CvBridge\nimport numpy as np\n\nclass SensorProcessor(Node):\n    """\n    Processes camera, LiDAR, and IMU data from humanoid robot\n    """\n    def __init__(self):\n        super().__init__(\'sensor_processor\')\n        \n        # Subscribers\n        self.camera_sub = self.create_subscription(\n            Image,\n            \'/humanoid/camera/image_raw\',\n            self.camera_callback,\n            10\n        )\n        \n        self.lidar_sub = self.create_subscription(\n            LaserScan,\n            \'/humanoid/lidar/scan\',\n            self.lidar_callback,\n            10\n        )\n        \n        self.imu_sub = self.create_subscription(\n            Imu,\n            \'/humanoid/imu/data\',\n            self.imu_callback,\n            10\n        )\n        \n        # CV Bridge for image conversion\n        self.bridge = CvBridge()\n        \n        self.get_logger().info(\'Sensor processor node started\')\n    \n    def camera_callback(self, msg):\n        """Process camera images"""\n        try:\n            # Convert ROS Image to OpenCV format\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n            \n            # Get image dimensions\n            height, width = cv_image.shape[:2]\n            \n            # Example: Detect edges\n            gray = cv2.cvtColor(cv_image, cv2.COLOR_BGR2GRAY)\n            edges = cv2.Canny(gray, 50, 150)\n            \n            # Log image info\n            self.get_logger().info(\n                f\'Camera: {width}x{height}, edges detected: {np.sum(edges > 0)} pixels\'\n            )\n        except Exception as e:\n            self.get_logger().error(f\'Camera processing error: {e}\')\n    \n    def lidar_callback(self, msg):\n        """Process LiDAR scans"""\n        # Get ranges (distances)\n        ranges = np.array(msg.ranges)\n        \n        # Filter invalid readings (inf, nan)\n        valid_ranges = ranges[np.isfinite(ranges)]\n        \n        if len(valid_ranges) > 0:\n            min_dist = np.min(valid_ranges)\n            max_dist = np.max(valid_ranges)\n            avg_dist = np.mean(valid_ranges)\n            \n            self.get_logger().info(\n                f\'LiDAR: min={min_dist:.2f}m, max={max_dist:.2f}m, avg={avg_dist:.2f}m\'\n            )\n            \n            # Detect obstacles (within 1 meter)\n            obstacles = valid_ranges[valid_ranges < 1.0]\n            if len(obstacles) > 0:\n                self.get_logger().warn(f\'Obstacle detected! {len(obstacles)} points < 1m\')\n    \n    def imu_callback(self, msg):\n        """Process IMU data"""\n        # Extract linear acceleration (includes gravity)\n        accel = msg.linear_acceleration\n        accel_magnitude = np.sqrt(\n            accel.x**2 + accel.y**2 + accel.z**2\n        )\n        \n        # Extract angular velocity\n        gyro = msg.angular_velocity\n        \n        # Log IMU data\n        self.get_logger().info(\n            f\'IMU: accel_mag={accel_magnitude:.2f} m/s\xb2, \'\n            f\'gyro=({gyro.x:.3f}, {gyro.y:.3f}, {gyro.z:.3f}) rad/s\'\n        )\n        \n        # Detect fall (high acceleration)\n        if accel_magnitude > 15.0:\n            self.get_logger().warn(\'High acceleration detected - possible fall!\')\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = SensorProcessor()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add dependencies"})," (",(0,i.jsx)(n.code,{children:"package.xml"}),"):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"<depend>sensor_msgs</depend>\n<depend>cv_bridge</depend>\n<depend>opencv-python</depend>\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Install OpenCV"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"sudo apt install ros-humble-cv-bridge python3-opencv\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Run sensor processor"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"cd ~/gazebo_ws\ncolcon build --packages-select gazebo_worlds\nsource install/setup.bash\n\n# Terminal 1: Launch Gazebo + robot\nros2 launch gazebo_worlds humanoid_sensors.launch.py\n\n# Terminal 2: Run sensor processor\nros2 run gazebo_worlds sensor_processor\n"})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{children:"[INFO] [sensor_processor]: Sensor processor node started\n[INFO] [sensor_processor]: Camera: 640x480, edges detected: 15234 pixels\n[INFO] [sensor_processor]: LiDAR: min=0.15m, max=8.5m, avg=4.2m\n[INFO] [sensor_processor]: IMU: accel_mag=9.81 m/s\xb2, gyro=(0.001, -0.002, 0.0005) rad/s\n"})}),"\n",(0,i.jsx)(n.h3,{id:"step-7-debugging-common-sensor-issues",children:"Step 7: Debugging Common Sensor Issues"}),"\n",(0,i.jsx)(n.h4,{id:"issue-1-camera-not-publishing",children:"Issue 1: Camera Not Publishing"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": No ",(0,i.jsx)(n.code,{children:"/humanoid/camera/image_raw"})," topic"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-bash",children:"# Check if plugin loaded\ngazebo --verbose  # Look for plugin loading messages\n\n# Verify plugin filename\n# Should be: libgazebo_ros_camera.so\n\n# Check ROS namespace\nros2 topic list | grep camera\n"})}),"\n",(0,i.jsx)(n.h4,{id:"issue-2-lidar-shows-no-data",children:"Issue 2: LiDAR Shows No Data"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Empty scan ranges"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- Ensure visualize=true for debugging --\x3e\n<visualize>true</visualize>\n\n\x3c!-- Check range limits --\x3e\n<min>0.1</min>\n<max>10.0</max>\n\n\x3c!-- Verify sensor pose (should face forward) --\x3e\n<pose>0 0 0.2 0 0 0</pose>\n"})}),"\n",(0,i.jsx)(n.h4,{id:"issue-3-imu-shows-zero-values",children:"Issue 3: IMU Shows Zero Values"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": All IMU readings are 0.0"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:"\x3c!-- Ensure always_on=true --\x3e\n<always_on>true</always_on>\n\n\x3c!-- Check update_rate --\x3e\n<update_rate>100</update_rate>\n\n\x3c!-- Verify sensor reference --\x3e\n<frame_name>imu_frame</frame_name>\n"})}),"\n",(0,i.jsx)(n.h4,{id:"issue-4-sensor-data-too-noisy",children:"Issue 4: Sensor Data Too Noisy"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Symptoms"}),": Unrealistic noise levels"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Reduce noise standard deviation --\x3e\n<noise type="gaussian">\n  <mean>0.0</mean>\n  <stddev>0.001</stddev>  \x3c!-- Lower = less noise --\x3e\n</noise>\n'})}),"\n",(0,i.jsx)(n.h2,{id:"part-3-advanced-topics-optional",children:"Part 3: Advanced Topics (Optional)"}),"\n",(0,i.jsx)(n.h3,{id:"depth-camera-rgb-d",children:"Depth Camera (RGB-D)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add depth camera"})," (for 3D perception):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'<sensor name="depth_camera" type="depth">\n  <camera>\n    <horizontal_fov>1.047</horizontal_fov>\n    <image>\n      <width>640</width>\n      <height>480</height>\n    </image>\n    <clip>\n      <near>0.05</near>\n      <far>10.0</far>\n    </clip>\n  </camera>\n  <plugin name="depth_camera_controller" filename="libgazebo_ros_depth_camera.so">\n    <ros>\n      <namespace>/humanoid</namespace>\n      <remapping>depth/image_raw:=depth/image_raw</remapping>\n    </ros>\n    <output_type>sensor_msgs/Image</output_type>\n    <frame_name>depth_camera_frame</frame_name>\n  </plugin>\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),": Object manipulation, 3D mapping, obstacle avoidance"]}),"\n",(0,i.jsx)(n.h3,{id:"stereo-camera-pair",children:"Stereo Camera Pair"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Add stereo cameras"})," (for depth estimation):"]}),"\n",(0,i.jsx)(n.pre,{children:(0,i.jsx)(n.code,{className:"language-xml",children:'\x3c!-- Left camera --\x3e\n<sensor name="left_camera" type="camera">\n  <pose>-0.05 0 0 0 0 0</pose>  \x3c!-- Left eye --\x3e\n  \x3c!-- ... camera config ... --\x3e\n</sensor>\n\n\x3c!-- Right camera --\x3e\n<sensor name="right_camera" type="camera">\n  <pose>0.05 0 0 0 0 0</pose>  \x3c!-- Right eye --\x3e\n  \x3c!-- ... camera config ... --\x3e\n</sensor>\n'})}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Use case"}),": Stereo vision for 3D perception"]}),"\n",(0,i.jsx)(n.h2,{id:"integration-with-capstone",children:"Integration with Capstone"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"How this chapter contributes"})," to the Week 13 autonomous humanoid:"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Vision pipeline"}),": Capstone will use camera for object detection and manipulation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"SLAM"}),": LiDAR data for mapping and localization"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Balance control"}),": IMU data for maintaining upright posture"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.strong,{children:"Multi-modal perception"}),": Combining camera + LiDAR + IMU for robust navigation"]}),"\n"]}),"\n",(0,i.jsx)(n.p,{children:"Understanding sensor simulation now is essential for developing the capstone perception system."}),"\n",(0,i.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,i.jsx)(n.p,{children:"You learned:"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["\u2705 Added ",(0,i.jsx)(n.strong,{children:"camera, LiDAR, and IMU sensors"})," to humanoid model"]}),"\n",(0,i.jsxs)(n.li,{children:["\u2705 Configured ",(0,i.jsx)(n.strong,{children:"sensor plugins"})," publishing ROS 2 topics"]}),"\n",(0,i.jsxs)(n.li,{children:["\u2705 Verified ",(0,i.jsx)(n.strong,{children:"sensor data"})," using ROS 2 introspection tools"]}),"\n",(0,i.jsxs)(n.li,{children:["\u2705 Processed ",(0,i.jsx)(n.strong,{children:"sensor data"})," in Python ROS 2 nodes"]}),"\n",(0,i.jsxs)(n.li,{children:["\u2705 Debugged ",(0,i.jsx)(n.strong,{children:"common sensor issues"})," (missing topics, noise)"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next steps"}),": In Chapter 2.4, you'll convert URDF to SDF format and optimize robot models for Gazebo."]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,i.jsx)(n.h3,{id:"exercise-1-camera-calibration-required",children:"Exercise 1: Camera Calibration (Required)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Configure camera parameters to match real hardware."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsx)(n.li,{children:"Research specs for a real camera (e.g., Intel RealSense D435)"}),"\n",(0,i.jsxs)(n.li,{children:["Configure Gazebo camera to match:","\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsx)(n.li,{children:"Resolution (e.g., 1920\xd71080)"}),"\n",(0,i.jsx)(n.li,{children:"Field of view (e.g., 87\xb0)"}),"\n",(0,i.jsx)(n.li,{children:"Frame rate (e.g., 30 Hz)"}),"\n"]}),"\n"]}),"\n",(0,i.jsx)(n.li,{children:"Verify image dimensions match specification"}),"\n",(0,i.jsx)(n.li,{children:"Test with different resolutions (VGA, HD, 4K)"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Camera resolution matches specification"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Field of view calculated correctly"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Frame rate verified with ",(0,i.jsx)(n.code,{children:"ros2 topic hz"})]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated Time"}),": 60 minutes"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-2-lidar-obstacle-detection-required",children:"Exercise 2: LiDAR Obstacle Detection (Required)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Process LiDAR data to detect obstacles."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Create ROS 2 node subscribing to ",(0,i.jsx)(n.code,{children:"/humanoid/lidar/scan"})]}),"\n",(0,i.jsx)(n.li,{children:"Detect obstacles within 2 meters"}),"\n",(0,i.jsx)(n.li,{children:"Calculate obstacle angle (relative to robot)"}),"\n",(0,i.jsx)(n.li,{children:"Publish obstacle locations to new topic"}),"\n",(0,i.jsx)(n.li,{children:"Visualize obstacles in RViz2"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,i.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Node detects obstacles correctly"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Obstacle angles calculated accurately"]}),"\n",(0,i.jsxs)(n.li,{className:"task-list-item",children:[(0,i.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Visualization shows detected obstacles"]}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated Time"}),": 90 minutes"]}),"\n",(0,i.jsx)(n.h3,{id:"exercise-3-imu-balance-detection-challenge",children:"Exercise 3: IMU Balance Detection (Challenge)"}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Objective"}),": Use IMU data to detect robot balance state."]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,i.jsxs)(n.ol,{children:["\n",(0,i.jsxs)(n.li,{children:["Subscribe to ",(0,i.jsx)(n.code,{children:"/humanoid/imu/data"})]}),"\n",(0,i.jsx)(n.li,{children:"Calculate tilt angle from linear acceleration"}),"\n",(0,i.jsx)(n.li,{children:"Detect if robot is falling (high angular velocity)"}),"\n",(0,i.jsx)(n.li,{children:"Implement fall detection algorithm"}),"\n",(0,i.jsx)(n.li,{children:"Publish balance status topic"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Hints"}),":"]}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:["Tilt angle: ",(0,i.jsx)(n.code,{children:"atan2(accel.x, accel.z)"})]}),"\n",(0,i.jsx)(n.li,{children:"Fall threshold: Angular velocity > 2.0 rad/s"}),"\n"]}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,i.jsxs)(n.ul,{children:["\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://gazebosim.org/docs/latest/sensors",children:"Gazebo Sensors"})," - Sensor documentation"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://docs.ros2.org/humble/api/sensor_msgs/",children:"ROS 2 Sensor Messages"})," - Message types"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"http://wiki.ros.org/cv_bridge",children:"CV Bridge"})," - ROS-OpenCV integration"]}),"\n",(0,i.jsxs)(n.li,{children:[(0,i.jsx)(n.a,{href:"https://www.vectornav.com/resources/inertial-navigation-primer/specifications--and--error-sources/sensor-error-sources",children:"IMU Calibration"})," - Real-world noise models"]}),"\n"]}),"\n",(0,i.jsx)(n.hr,{}),"\n",(0,i.jsxs)(n.p,{children:[(0,i.jsx)(n.strong,{children:"Next"}),": [Chapter 2.4: URDF/SDF Robot Description \u2192](chapter-2 to 4.md)"]})]})}function h(e={}){const{wrapper:n}={...(0,a.R)(),...e.components};return n?(0,i.jsx)(n,{...e,children:(0,i.jsx)(d,{...e})}):d(e)}}}]);
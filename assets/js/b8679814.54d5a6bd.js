"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4440],{7508:e=>{e.exports=JSON.parse('{"version":{"pluginId":"default","version":"current","label":"Next","banner":null,"badge":false,"noIndex":false,"className":"docs-version-current","isLast":true,"docsSidebars":{"defaultSidebar":[{"type":"category","label":"Introduction","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/intro","label":"Physical AI & Humanoid Robotics","docId":"intro","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 0: Foundations of Physical AI","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-0/intro","label":"Module 0 Introduction","docId":"module-0/intro","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-0/chapter-0-1","label":"0.1 Introduction to Embodied Intelligence & Physical AI","docId":"module-0/chapter-0-1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-0/chapter-0-2","label":"0.2 Humanoid Robotics Landscape & Applications","docId":"module-0/chapter-0-2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-0/chapter-0-3","label":"0.3 Sensor Systems for Humanoid Robots","docId":"module-0/chapter-0-3","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 1: The Robotic Nervous System (ROS 2)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/intro","label":"Module 1 Introduction","docId":"module-1/intro","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-1","label":"1.1 ROS 2 Architecture & Core Concepts","docId":"module-1/chapter-1-1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-2","label":"1.2 Python Integration with rclpy","docId":"module-1/chapter-1-2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-3","label":"1.3 URDF for Humanoid Robots","docId":"module-1/chapter-1-3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-4","label":"1.4 Package Development & Launch Files","docId":"module-1/chapter-1-4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-5","label":"1.5 Parameter Management & Best Practices","docId":"module-1/chapter-1-5","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 2: The Digital Twin (Gazebo & Unity)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/intro","label":"Module 2 Introduction","docId":"module-2/intro","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1","label":"2.1 Gazebo Fundamentals & Setup","docId":"module-2/chapter-2-1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2","label":"2.2 Physics Simulation (Gravity, Collisions, Friction)","docId":"module-2/chapter-2-2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3","label":"2.3 Sensor Simulation (LiDAR, Cameras, IMUs)","docId":"module-2/chapter-2-3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-4","label":"2.4 URDF/SDF Robot Description","docId":"module-2/chapter-2-4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-5","label":"2.5 Unity Integration for Photorealistic Rendering","docId":"module-2/chapter-2-5","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 3: The AI-Robot Brain (NVIDIA Isaac)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/intro","label":"Module 3 Introduction","docId":"module-3/intro","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-1","label":"3.1 Isaac Sim Setup & Photorealistic Simulation","docId":"module-3/chapter-3-1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-2","label":"3.2 Synthetic Data Generation for Training","docId":"module-3/chapter-3-2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-3","label":"3.3 Isaac ROS - Hardware-Accelerated VSLAM","docId":"module-3/chapter-3-3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-4","label":"3.4 Nav2 Navigation for Bipedal Humanoids","docId":"module-3/chapter-3-4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-3/chapter-3-5","label":"3.5 Sim-to-Real Transfer Workflows","docId":"module-3/chapter-3-5","unlisted":false}],"collapsed":true,"collapsible":true},{"type":"category","label":"Module 4: Vision-Language-Action (VLA)","items":[{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/intro","label":"Module 4 Introduction","docId":"module-4/intro","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-1","label":"4.1 Voice-to-Action with OpenAI Whisper","docId":"module-4/chapter-4-1","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-2","label":"4.2 LLM-Based Cognitive Planning","docId":"module-4/chapter-4-2","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-3","label":"4.3 Natural Language to ROS 2 Actions","docId":"module-4/chapter-4-3","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-4","label":"4.4 Multi-Modal Integration (Speech + Vision + Gesture)","docId":"module-4/chapter-4-4","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-5","label":"4.5 Humanoid Kinematics & Balance Control","docId":"module-4/chapter-4-5","unlisted":false},{"type":"link","href":"/physical-ai-humanoid-robotics-book/docs/module-4/capstone","label":"Capstone Project - Autonomous Humanoid System","docId":"module-4/capstone","unlisted":false}],"collapsed":true,"collapsible":true}]},"docs":{"intro":{"id":"intro","title":"Physical AI & Humanoid Robotics","description":"AI Systems in the Physical World","sidebar":"defaultSidebar"},"module-0/chapter-0-1":{"id":"module-0/chapter-0-1","title":"0.1 Introduction to Embodied Intelligence & Physical AI","description":"What makes a robot \\"intelligent\\"? Unlike digital AI systems that process text and images, Physical AI systems must understand and interact with the physical world. This chapter introduces embodied intelligence\u2014the foundation of all humanoid robotics\u2014and explains why Physical AI represents a fundamentally different challenge than digital AI.","sidebar":"defaultSidebar"},"module-0/chapter-0-2":{"id":"module-0/chapter-0-2","title":"0.2 Humanoid Robotics Landscape & Applications","description":"Humanoid robotics has evolved from research curiosities to commercial products. This chapter explores the current state of humanoid robotics, key players in the industry, real-world applications, and the path from research to deployment. Understanding this landscape provides context for why we\'re learning ROS 2, Gazebo, and Isaac Sim.","sidebar":"defaultSidebar"},"module-0/chapter-0-3":{"id":"module-0/chapter-0-3","title":"0.3 Sensor Systems for Humanoid Robots","description":"Humanoid robots need \\"senses\\" to perceive the world\u2014just like humans have eyes, ears, and balance systems. This chapter covers the sensor systems that enable humanoid robots to see, hear, feel, and maintain balance. You\'ll learn about cameras, LiDAR, IMUs, force/torque sensors, and how to simulate them in PyBullet.","sidebar":"defaultSidebar"},"module-0/intro":{"id":"module-0/intro","title":"Module 0 Introduction","description":"Duration: Weeks 1 to 2 (2 weeks)","sidebar":"defaultSidebar"},"module-1/chapter-1-1":{"id":"module-1/chapter-1-1","title":"1.1 ROS 2 Architecture & Core Concepts","description":"ROS 2 is the middleware layer that enables humanoid robots to coordinate hundreds of sensors, motors, and algorithms in real-time. Understanding its architecture is fundamental to building autonomous Physical AI systems.","sidebar":"defaultSidebar"},"module-1/chapter-1-2":{"id":"module-1/chapter-1-2","title":"1.2 Python Integration with rclpy","description":"Python is the primary language for rapid robotics development. The \\\\rclpy\\\\ library provides a Pythonic interface to ROS 2, enabling you to build nodes, publish data, subscribe to topics, and create services\u2014all with clean, readable code.","sidebar":"defaultSidebar"},"module-1/chapter-1-3":{"id":"module-1/chapter-1-3","title":"1.3 URDF for Humanoid Robots","description":"URDF (Unified Robot Description Format) is the standard XML-based format for describing robot kinematics, dynamics, and sensors in ROS. Every humanoid robot\u2014from academic research platforms to Boston Dynamics\' Atlas\u2014uses URDF (or its derivatives) to define structure.","sidebar":"defaultSidebar"},"module-1/chapter-1-4":{"id":"module-1/chapter-1-4","title":"1.4 Package Development & Launch Files","description":"ROS 2 packages are the fundamental unit of organization\u2014they contain nodes, launch files, configuration files, and robot descriptions. Launch files automate the startup of multi-node systems, eliminating the need to manually run dozens of terminals.","sidebar":"defaultSidebar"},"module-1/chapter-1-5":{"id":"module-1/chapter-1-5","title":"1.5 Parameter Management & Best Practices","description":"Production robotics systems require runtime configurability, robust communication policies, and predictable lifecycle management. This chapter covers parameters, Quality of Service (QoS) profiles, lifecycle nodes, and best practices for building reliable ROS 2 systems.","sidebar":"defaultSidebar"},"module-1/intro":{"id":"module-1/intro","title":"Module 1 Introduction","description":"Duration: Weeks 3 to 5 (3 weeks)","sidebar":"defaultSidebar"},"module-2/chapter-2-1":{"id":"module-2/chapter-2-1","title":"2.1 Gazebo Fundamentals & Setup","description":"Gazebo is the industry-standard physics simulator for robotics. It provides accurate dynamics, realistic sensor models, and seamless ROS 2 integration\u2014making it essential for developing humanoid robots before deploying to expensive hardware.","sidebar":"defaultSidebar"},"module-2/chapter-2-2":{"id":"module-2/chapter-2-2","title":"2.2 Physics Simulation (Gravity, Collisions, Friction)","description":"Realistic physics simulation is critical for humanoid robots. Walking, balancing, and manipulation all depend on accurate modeling of gravity, collisions, and friction. This chapter covers configuring Gazebo\'s physics engines to simulate realistic humanoid dynamics.","sidebar":"defaultSidebar"},"module-2/chapter-2-3":{"id":"module-2/chapter-2-3","title":"2.3 Sensor Simulation (LiDAR, Cameras, IMUs)","description":"Sensors are the eyes and ears of humanoid robots. Gazebo provides realistic sensor models that publish ROS 2 topics, enabling you to develop perception algorithms in simulation before deploying to hardware. This chapter covers camera, LiDAR, and IMU sensor plugins.","sidebar":"defaultSidebar"},"module-2/chapter-2-4":{"id":"module-2/chapter-2-4","title":"2.4 URDF/SDF Robot Description","description":"URDF (Unified Robot Description Format) and SDF (Simulation Description Format) define robot structure, joints, links, sensors, and visual properties. This chapter covers converting URDF to SDF, optimizing models for Gazebo, and best practices for humanoid robot descriptions.","sidebar":"defaultSidebar"},"module-2/chapter-2-5":{"id":"module-2/chapter-2-5","title":"2.5 Unity Integration for Photorealistic Rendering","description":"While Gazebo provides accurate physics, Unity offers photorealistic rendering with ray-traced lighting, realistic materials, and massive asset libraries. Combining both enables physics-accurate simulation with visual realism\u2014essential for training vision-based AI models and validating humanoid robot behaviors in realistic environments.","sidebar":"defaultSidebar"},"module-2/intro":{"id":"module-2/intro","title":"Module 2 Introduction","description":"Duration: Weeks 6 to 7 (2 weeks)","sidebar":"defaultSidebar"},"module-3/chapter-3-1":{"id":"module-3/chapter-3-1","title":"3.1 Isaac Sim Setup & Photorealistic Simulation","description":"NVIDIA Isaac Sim provides photorealistic, GPU-accelerated simulation for robotics AI. Built on Omniverse and USD (Universal Scene Description), it enables training vision models, testing navigation algorithms, and validating behaviors in visually realistic environments before hardware deployment.","sidebar":"defaultSidebar"},"module-3/chapter-3-2":{"id":"module-3/chapter-3-2","title":"3.2 Synthetic Data Generation for Training","description":"Training vision-based AI models requires thousands of labeled images. Isaac Sim provides built-in tools for generating synthetic data with perfect annotations\u2014enabling rapid dataset creation without manual labeling. This chapter covers data collection pipelines, domain randomization, and dataset export formats.","sidebar":"defaultSidebar"},"module-3/chapter-3-3":{"id":"module-3/chapter-3-3","title":"3.3 Isaac ROS - Hardware-Accelerated VSLAM","description":"Visual SLAM (Simultaneous Localization and Mapping) enables humanoid robots to build maps and localize themselves using camera data. Isaac ROS provides GPU-accelerated VSLAM algorithms that run at real-time speeds\u2014essential for autonomous navigation in GPS-denied environments.","sidebar":"defaultSidebar"},"module-3/chapter-3-4":{"id":"module-3/chapter-3-4","title":"3.4 Nav2 Navigation for Bipedal Humanoids","description":"Nav2 (Navigation 2) is the ROS 2 navigation framework that enables robots to plan paths, avoid obstacles, and reach goals autonomously. Configuring Nav2 for bipedal humanoids requires special considerations: wider turning radius, balance constraints, and dynamic obstacle avoidance.","sidebar":"defaultSidebar"},"module-3/chapter-3-5":{"id":"module-3/chapter-3-5","title":"3.5 Sim-to-Real Transfer Workflows","description":"The ultimate test of simulation is deployment to physical hardware. Sim-to-real transfer validates that algorithms developed in Isaac Sim and Gazebo work on real robots. This chapter covers calibration, validation, and debugging the inevitable gaps between simulation and reality.","sidebar":"defaultSidebar"},"module-3/intro":{"id":"module-3/intro","title":"Module 3 Introduction","description":"Duration: Weeks 8 to 10 (3 weeks)","sidebar":"defaultSidebar"},"module-4/capstone":{"id":"module-4/capstone","title":"Capstone Project - Autonomous Humanoid System","description":"Duration: Week 13 (Final week)","sidebar":"defaultSidebar"},"module-4/chapter-4-1":{"id":"module-4/chapter-4-1","title":"4.1 Voice-to-Action with OpenAI Whisper","description":"Voice commands enable natural human-robot interaction. OpenAI Whisper provides state-of-the-art speech recognition that converts spoken language to text\u2014the first step in the VLA pipeline. This chapter covers Whisper installation, ROS 2 integration, and real-time voice command processing.","sidebar":"defaultSidebar"},"module-4/chapter-4-2":{"id":"module-4/chapter-4-2","title":"4.2 LLM-Based Cognitive Planning","description":"Large Language Models (LLMs) enable robots to understand natural language and decompose complex goals into executable steps. This chapter covers integrating GPT-4 or open-source LLMs (Llama, Mistral) with ROS 2 for task planning, prompt engineering, and handling ambiguous commands.","sidebar":"defaultSidebar"},"module-4/chapter-4-3":{"id":"module-4/chapter-4-3","title":"4.3 Natural Language to ROS 2 Actions","description":"LLM-generated plans must be executed by the robot. This chapter covers mapping natural language actions to ROS 2 Action goals, implementing action executors, handling preconditions and postconditions, and creating feedback mechanisms for robust task execution.","sidebar":"defaultSidebar"},"module-4/chapter-4-4":{"id":"module-4/chapter-4-4","title":"4.4 Multi-Modal Integration (Speech + Vision + Gesture)","description":"Human-robot interaction is most natural when robots understand multiple input modalities: speech, vision, and gesture. Combining these modalities enables robust understanding even when individual modalities fail or are ambiguous. This chapter covers fusing multi-modal inputs for unified perception-action pipelines.","sidebar":"defaultSidebar"},"module-4/chapter-4-5":{"id":"module-4/chapter-4-5","title":"4.5 Humanoid Kinematics & Balance Control","description":"Humanoid robots require sophisticated control algorithms to maintain balance while walking, manipulating objects, and navigating. This chapter covers forward/inverse kinematics, balance control (ZMP, LIPM), walking gaits, and integration with ROS 2 control stack.","sidebar":"defaultSidebar"},"module-4/intro":{"id":"module-4/intro","title":"Module 4 Introduction","description":"Duration: Weeks 11 to 13 (3 weeks)","sidebar":"defaultSidebar"},"tutorial-basics/congratulations":{"id":"tutorial-basics/congratulations","title":"Congratulations!","description":"You have just learned the basics of Docusaurus and made some changes to the initial template."},"tutorial-basics/create-a-blog-post":{"id":"tutorial-basics/create-a-blog-post","title":"Create a Blog Post","description":"Docusaurus creates a page for each blog post, but also a blog index page, a tag system, an RSS feed..."},"tutorial-basics/create-a-document":{"id":"tutorial-basics/create-a-document","title":"Create a Document","description":"Documents are groups of pages connected through:"},"tutorial-basics/create-a-page":{"id":"tutorial-basics/create-a-page","title":"Create a Page","description":"Add Markdown or React files to src/pages to create a standalone page:"},"tutorial-basics/deploy-your-site":{"id":"tutorial-basics/deploy-your-site","title":"Deploy your site","description":"Docusaurus is a static-site-generator (also called Jamstack)."},"tutorial-basics/markdown-features":{"id":"tutorial-basics/markdown-features","title":"Markdown Features","description":"Docusaurus supports Markdown and a few additional features."},"tutorial-extras/manage-docs-versions":{"id":"tutorial-extras/manage-docs-versions","title":"Manage Docs Versions","description":"Docusaurus can manage multiple versions of your docs."},"tutorial-extras/translate-your-site":{"id":"tutorial-extras/translate-your-site","title":"Translate your site","description":"Let\'s translate docs/intro.md to French."}}}}')}}]);
"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[282],{7736:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>c,default:()=>h,frontMatter:()=>o,metadata:()=>r,toc:()=>l});const r=JSON.parse('{"id":"module-2/chapter-2-1","title":"2.1 Computer Vision Fundamentals","description":"Computer vision enables robots to extract meaningful information from images. This chapter covers essential image processing techniques and classical computer vision algorithms used in robotics.","source":"@site/docs/module-2/chapter-2-1.md","sourceDirName":"module-2","slug":"/module-2/chapter-2-1","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/chapter-2-1.md","tags":[],"version":"current","sidebarPosition":2,"frontMatter":{"sidebar_position":2,"title":"2.1 Computer Vision Fundamentals"},"sidebar":"defaultSidebar","previous":{"title":"Module 2 Introduction","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/intro"},"next":{"title":"2.2 Deep Learning for Vision","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2"}}');var s=i(4848),t=i(8453);const o={sidebar_position:2,title:"2.1 Computer Vision Fundamentals"},c="Chapter 2.1: Computer Vision Fundamentals",a={},l=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Image Representation",id:"image-representation",level:2},{value:"Digital Images",id:"digital-images",level:3},{value:"Color Spaces",id:"color-spaces",level:3},{value:"Image Filtering",id:"image-filtering",level:2},{value:"Blurring (Smoothing)",id:"blurring-smoothing",level:3},{value:"Edge Detection",id:"edge-detection",level:3},{value:"Feature Detection",id:"feature-detection",level:2},{value:"Corner Detection (Harris)",id:"corner-detection-harris",level:3},{value:"Keypoint Detection (ORB)",id:"keypoint-detection-orb",level:3},{value:"Feature Matching",id:"feature-matching",level:2},{value:"Practical Example: Object Detection via Color",id:"practical-example-object-detection-via-color",level:2},{value:"OpenCV Basics",id:"opencv-basics",level:2},{value:"Installation",id:"installation",level:3},{value:"Essential Functions",id:"essential-functions",level:3},{value:"Exercises",id:"exercises",level:2},{value:"1. Color Space Exploration",id:"1-color-space-exploration",level:3},{value:"2. Edge Detection Tuning",id:"2-edge-detection-tuning",level:3},{value:"3. Feature Matching Challenge",id:"3-feature-matching-challenge",level:3},{value:"4. Color-Based Object Tracker",id:"4-color-based-object-tracker",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,s.jsxs)(s.Fragment,{children:[(0,s.jsx)(n.header,{children:(0,s.jsx)(n.h1,{id:"chapter-21-computer-vision-fundamentals",children:"Chapter 2.1: Computer Vision Fundamentals"})}),"\n",(0,s.jsx)(n.p,{children:"Computer vision enables robots to extract meaningful information from images. This chapter covers essential image processing techniques and classical computer vision algorithms used in robotics."}),"\n",(0,s.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Understand"})," image representation (pixels, channels, color spaces)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Apply"})," filtering techniques (blur, edge detection)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Extract"})," features (corners, keypoints) for object recognition"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.strong,{children:"Implement"})," basic CV pipelines using OpenCV"]}),"\n"]}),"\n",(0,s.jsx)(n.h2,{id:"image-representation",children:"Image Representation"}),"\n",(0,s.jsx)(n.h3,{id:"digital-images",children:"Digital Images"}),"\n",(0,s.jsx)(n.p,{children:"Images are 2D arrays of pixels. Each pixel stores intensity values."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Grayscale image"}),": Single channel (0-255)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\n# Create 100x100 black image\nimg = np.zeros((100, 100), dtype=np.uint8)\n\n# Set some pixels to white (255)\nimg[40:60, 40:60] = 255\n\ncv2.imshow('Image', img)\ncv2.waitKey(0)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Color image (RGB)"}),": Three channels (Red, Green, Blue)"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Color image: height \xd7 width \xd7 3\ncolor_img = np.zeros((100, 100, 3), dtype=np.uint8)\ncolor_img[:, :] = [0, 255, 0]  # Green image\n"})}),"\n",(0,s.jsx)(n.h3,{id:"color-spaces",children:"Color Spaces"}),"\n",(0,s.jsxs)(n.table,{children:[(0,s.jsx)(n.thead,{children:(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.th,{children:"Color Space"}),(0,s.jsx)(n.th,{children:"Channels"}),(0,s.jsx)(n.th,{children:"Use Case"})]})}),(0,s.jsxs)(n.tbody,{children:[(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"RGB"})}),(0,s.jsx)(n.td,{children:"Red, Green, Blue"}),(0,s.jsx)(n.td,{children:"Display, general processing"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"BGR"})}),(0,s.jsx)(n.td,{children:"Blue, Green, Red"}),(0,s.jsx)(n.td,{children:"OpenCV default"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"HSV"})}),(0,s.jsx)(n.td,{children:"Hue, Saturation, Value"}),(0,s.jsx)(n.td,{children:"Color-based segmentation"})]}),(0,s.jsxs)(n.tr,{children:[(0,s.jsx)(n.td,{children:(0,s.jsx)(n.strong,{children:"Grayscale"})}),(0,s.jsx)(n.td,{children:"Intensity only"}),(0,s.jsx)(n.td,{children:"Edge detection, faster processing"})]})]})]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Convert between color spaces"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Load image\nimg = cv2.imread('robot_view.jpg')\n\n# Convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Convert to HSV (for color detection)\nhsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"image-filtering",children:"Image Filtering"}),"\n",(0,s.jsx)(n.p,{children:"Filters modify images by applying operations on pixel neighborhoods."}),"\n",(0,s.jsx)(n.h3,{id:"blurring-smoothing",children:"Blurring (Smoothing)"}),"\n",(0,s.jsx)(n.p,{children:"Reduces noise by averaging neighboring pixels."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Gaussian blur (most common)\nblurred = cv2.GaussianBlur(img, (5, 5), sigmaX=1.5)\n\n# Kernel size must be odd: (3,3), (5,5), (7,7)\n# Larger kernel = more blur\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use case"}),": Reduce camera noise before edge detection"]}),"\n",(0,s.jsx)(n.h3,{id:"edge-detection",children:"Edge Detection"}),"\n",(0,s.jsx)(n.p,{children:"Finds boundaries where intensity changes sharply."}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Canny Edge Detector"})," (most popular):"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Convert to grayscale first\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n\n# Apply Gaussian blur\nblurred = cv2.GaussianBlur(gray, (5, 5), 1.5)\n\n# Canny edge detection\nedges = cv2.Canny(blurred, threshold1=50, threshold2=150)\n\ncv2.imshow('Edges', edges)\ncv2.waitKey(0)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Parameters"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"threshold1"}),": Lower threshold (weak edges)"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.code,{children:"threshold2"}),": Upper threshold (strong edges)"]}),"\n",(0,s.jsx)(n.li,{children:"Adjust based on lighting conditions"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"feature-detection",children:"Feature Detection"}),"\n",(0,s.jsx)(n.p,{children:"Features are distinctive image points used for object recognition and tracking."}),"\n",(0,s.jsx)(n.h3,{id:"corner-detection-harris",children:"Corner Detection (Harris)"}),"\n",(0,s.jsx)(n.p,{children:"Corners are points where intensity changes in multiple directions."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Convert to grayscale\ngray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\ngray = np.float32(gray)\n\n# Harris corner detection\ncorners = cv2.cornerHarris(gray, blockSize=2, ksize=3, k=0.04)\n\n# Threshold to mark corners\nimg[corners > 0.01 * corners.max()] = [0, 0, 255]  # Red dots\n\ncv2.imshow('Corners', img)\ncv2.waitKey(0)\n"})}),"\n",(0,s.jsx)(n.h3,{id:"keypoint-detection-orb",children:"Keypoint Detection (ORB)"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"ORB (Oriented FAST and Rotated BRIEF)"})," detects keypoints and computes descriptors."]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Create ORB detector\norb = cv2.ORB_create(nfeatures=500)\n\n# Detect keypoints and compute descriptors\nkeypoints, descriptors = orb.detectAndCompute(gray, None)\n\n# Draw keypoints on image\nimg_keypoints = cv2.drawKeypoints(\n    img, keypoints, None,\n    color=(0, 255, 0),\n    flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS\n)\n\ncv2.imshow('ORB Keypoints', img_keypoints)\ncv2.waitKey(0)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Keypoints"})," = location + orientation + scale\n",(0,s.jsx)(n.strong,{children:"Descriptors"})," = binary vectors describing local appearance"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"feature-matching",children:"Feature Matching"}),"\n",(0,s.jsx)(n.p,{children:"Match keypoints between two images (useful for object recognition)."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Detect keypoints in both images\norb = cv2.ORB_create()\nkp1, des1 = orb.detectAndCompute(img1, None)\nkp2, des2 = orb.detectAndCompute(img2, None)\n\n# Create matcher\nbf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n\n# Match descriptors\nmatches = bf.match(des1, des2)\n\n# Sort by distance (best matches first)\nmatches = sorted(matches, key=lambda x: x.distance)\n\n# Draw top 50 matches\nimg_matches = cv2.drawMatches(\n    img1, kp1, img2, kp2, matches[:50], None,\n    flags=cv2.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS\n)\n\ncv2.imshow('Matches', img_matches)\ncv2.waitKey(0)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Applications"}),":"]}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"Object detection in cluttered scenes"}),"\n",(0,s.jsx)(n.li,{children:"Template matching"}),"\n",(0,s.jsx)(n.li,{children:"Visual odometry (camera-based robot localization)"}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"practical-example-object-detection-via-color",children:"Practical Example: Object Detection via Color"}),"\n",(0,s.jsx)(n.p,{children:"Detect a red ball using color thresholding in HSV space."}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\n# Capture from camera\ncap = cv2.VideoCapture(0)\n\nwhile True:\n    ret, frame = cap.read()\n    if not ret:\n        break\n\n    # Convert to HSV\n    hsv = cv2.cvtColor(frame, cv2.COLOR_BGR2HSV)\n\n    # Define red color range in HSV\n    # Red wraps around in HSV (0-10 and 170-180)\n    lower_red1 = np.array([0, 100, 100])\n    upper_red1 = np.array([10, 255, 255])\n    lower_red2 = np.array([160, 100, 100])\n    upper_red2 = np.array([180, 255, 255])\n\n    # Create masks\n    mask1 = cv2.inRange(hsv, lower_red1, upper_red1)\n    mask2 = cv2.inRange(hsv, lower_red2, upper_red2)\n    mask = mask1 | mask2\n\n    # Find contours\n    contours, _ = cv2.findContours(\n        mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE\n    )\n\n    # Draw bounding boxes around detected objects\n    for contour in contours:\n        if cv2.contourArea(contour) > 500:  # Filter small noise\n            x, y, w, h = cv2.boundingRect(contour)\n            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n            cv2.putText(frame, 'Red Object', (x, y-10),\n                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow('Detection', frame)\n    cv2.imshow('Mask', mask)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n\ncap.release()\ncv2.destroyAllWindows()\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Use case"}),": Humanoid robot detecting colored objects for manipulation tasks."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"opencv-basics",children:"OpenCV Basics"}),"\n",(0,s.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-bash",children:"pip install opencv-python opencv-contrib-python numpy\n"})}),"\n",(0,s.jsx)(n.h3,{id:"essential-functions",children:"Essential Functions"}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Read/Write"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"img = cv2.imread('path/to/image.jpg')\ncv2.imwrite('output.jpg', img)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Display"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"cv2.imshow('Window Name', img)\ncv2.waitKey(0)  # Wait for keypress\ncv2.destroyAllWindows()\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Resize"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"resized = cv2.resize(img, (640, 480))  # (width, height)\n"})}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Draw shapes"}),":"]}),"\n",(0,s.jsx)(n.pre,{children:(0,s.jsx)(n.code,{className:"language-python",children:"# Rectangle\ncv2.rectangle(img, (x1, y1), (x2, y2), color=(0,255,0), thickness=2)\n\n# Circle\ncv2.circle(img, (cx, cy), radius=30, color=(255,0,0), thickness=-1)\n\n# Text\ncv2.putText(img, 'Hello', (50, 50), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2)\n"})}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,s.jsx)(n.h3,{id:"1-color-space-exploration",children:"1. Color Space Exploration"}),"\n",(0,s.jsx)(n.p,{children:"Load an image and convert it to RGB, HSV, and grayscale. Display all three side-by-side. Which color space makes it easiest to detect a specific colored object?"}),"\n",(0,s.jsx)(n.h3,{id:"2-edge-detection-tuning",children:"2. Edge Detection Tuning"}),"\n",(0,s.jsx)(n.p,{children:"Take a photo with your phone/webcam. Apply Canny edge detection with different threshold values:"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsx)(n.li,{children:"(50, 150)"}),"\n",(0,s.jsx)(n.li,{children:"(100, 200)"}),"\n",(0,s.jsx)(n.li,{children:"(30, 100)"}),"\n"]}),"\n",(0,s.jsx)(n.p,{children:"Compare results. Which works best? Why?"}),"\n",(0,s.jsx)(n.h3,{id:"3-feature-matching-challenge",children:"3. Feature Matching Challenge"}),"\n",(0,s.jsx)(n.p,{children:"Take two photos of the same object from different angles. Use ORB feature matching to find correspondences. How many good matches do you get? What happens if you rotate or scale the object significantly?"}),"\n",(0,s.jsx)(n.h3,{id:"4-color-based-object-tracker",children:"4. Color-Based Object Tracker"}),"\n",(0,s.jsx)(n.p,{children:"Modify the red ball detection code to:"}),"\n",(0,s.jsxs)(n.ol,{children:["\n",(0,s.jsx)(n.li,{children:"Track a blue object instead (find HSV range)"}),"\n",(0,s.jsx)(n.li,{children:"Display the center coordinates of the detected object"}),"\n",(0,s.jsx)(n.li,{children:"Draw a trail showing object movement over time"}),"\n"]}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Hint"}),": Store previous positions in a list and draw lines between them."]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,s.jsxs)(n.p,{children:["\u2705 Images are 2D/3D arrays of pixel intensity values\n\u2705 ",(0,s.jsx)(n.strong,{children:"Color spaces"})," (RGB, HSV) affect how we process images\n\u2705 ",(0,s.jsx)(n.strong,{children:"Filtering"})," (blur, edges) extracts structure from images\n\u2705 ",(0,s.jsx)(n.strong,{children:"Features"})," (corners, keypoints) enable object recognition\n\u2705 ",(0,s.jsx)(n.strong,{children:"OpenCV"})," is the standard library for computer vision in robotics\n\u2705 Classical CV still useful for real-time robotics (fast, lightweight)"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,s.jsxs)(n.ul,{children:["\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d6/d00/tutorial_py_root.html",children:"OpenCV Python Tutorials"})," - Official documentation"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.em,{children:"Learning OpenCV"})," by Bradski & Kaehler - Comprehensive textbook"]}),"\n",(0,s.jsxs)(n.li,{children:[(0,s.jsx)(n.a,{href:"https://pyimagesearch.com/",children:"PyImageSearch"})," - Practical CV tutorials"]}),"\n"]}),"\n",(0,s.jsx)(n.hr,{}),"\n",(0,s.jsxs)(n.p,{children:[(0,s.jsx)(n.strong,{children:"Previous"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/intro",children:"\u2190 Module 2 Introduction"})," | ",(0,s.jsx)(n.strong,{children:"Next"}),": ",(0,s.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2",children:"Chapter 2.2: Deep Learning for Vision \u2192"})]}),"\n",(0,s.jsx)(n.p,{children:"Classical computer vision provides fast, interpretable methods. Next, we'll explore how deep learning revolutionized visual perception!"})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,s.jsx)(n,{...e,children:(0,s.jsx)(d,{...e})}):d(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>c});var r=i(6540);const s={},t=r.createContext(s);function o(e){const n=r.useContext(t);return r.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function c(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(s):e.components||s:o(e.components),r.createElement(t.Provider,{value:n},e.children)}}}]);
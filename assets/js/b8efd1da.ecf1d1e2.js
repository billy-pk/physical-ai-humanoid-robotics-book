"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[8360],{981:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>a,contentTitle:()=>o,default:()=>h,frontMatter:()=>l,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-0/chapter-0-3","title":"0.3 Sensor Systems for Humanoid Robots","description":"Humanoid robots need \\"senses\\" to perceive the world\u2014just like humans have eyes, ears, and balance systems. This chapter covers the sensor systems that enable humanoid robots to see, hear, feel, and maintain balance. You\'ll learn about cameras, LiDAR, IMUs, force/torque sensors, and how to simulate them in PyBullet.","source":"@site/docs/module-0/chapter-0-3.md","sourceDirName":"module-0","slug":"/module-0/chapter-0-3","permalink":"/physical-ai-humanoid-robotics-book/docs/module-0/chapter-0-3","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-0/chapter-0-3.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"0.3 Sensor Systems for Humanoid Robots"},"sidebar":"defaultSidebar","previous":{"title":"0.2 Humanoid Robotics Landscape & Applications","permalink":"/physical-ai-humanoid-robotics-book/docs/module-0/chapter-0-2"},"next":{"title":"Module 1 Introduction","permalink":"/physical-ai-humanoid-robotics-book/docs/module-1/intro"}}');var r=s(4848),t=s(8453);const l={sidebar_position:4,title:"0.3 Sensor Systems for Humanoid Robots"},o="Chapter 0.3: Sensor Systems for Humanoid Robots",a={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: Sensor Overview",id:"part-1-sensor-overview",level:2},{value:"Why Sensors Matter",id:"why-sensors-matter",level:3},{value:"Sensor Categories",id:"sensor-categories",level:3},{value:"Part 2: Vision Sensors",id:"part-2-vision-sensors",level:2},{value:"RGB Cameras",id:"rgb-cameras",level:3},{value:"Depth Cameras",id:"depth-cameras",level:3},{value:"Stereo Vision",id:"stereo-vision",level:3},{value:"Part 3: LiDAR (Light Detection and Ranging)",id:"part-3-lidar-light-detection-and-ranging",level:2},{value:"What is LiDAR?",id:"what-is-lidar",level:3},{value:"LiDAR Specifications",id:"lidar-specifications",level:3},{value:"Use Cases",id:"use-cases",level:3},{value:"Limitations",id:"limitations",level:3},{value:"Part 4: IMU (Inertial Measurement Unit)",id:"part-4-imu-inertial-measurement-unit",level:2},{value:"What is an IMU?",id:"what-is-an-imu",level:3},{value:"How IMUs Work",id:"how-imus-work",level:3},{value:"Use Cases",id:"use-cases-1",level:3},{value:"Limitations",id:"limitations-1",level:3},{value:"Part 5: Force/Torque Sensors",id:"part-5-forcetorque-sensors",level:2},{value:"What are Force/Torque Sensors?",id:"what-are-forcetorque-sensors",level:3},{value:"Use Cases",id:"use-cases-2",level:3},{value:"Specifications",id:"specifications",level:3},{value:"Part 6: Sensor Fusion",id:"part-6-sensor-fusion",level:2},{value:"Why Combine Sensors?",id:"why-combine-sensors",level:3},{value:"Common Fusion Strategies",id:"common-fusion-strategies",level:3},{value:"Part 7: Hands-On Tutorial",id:"part-7-hands-on-tutorial",level:2},{value:"Project: Simulate Sensors in PyBullet",id:"project-simulate-sensors-in-pybullet",level:3},{value:"Step 1: Camera Simulation",id:"step-1-camera-simulation",level:3},{value:"Step 2: IMU Simulation",id:"step-2-imu-simulation",level:3},{value:"Step 3: LiDAR Simulation",id:"step-3-lidar-simulation",level:3},{value:"Step 4: Sensor Fusion Example",id:"step-4-sensor-fusion-example",level:3},{value:"Part 8: Debugging Common Issues",id:"part-8-debugging-common-issues",level:2},{value:"Issue 1: &quot;Camera image is black&quot;",id:"issue-1-camera-image-is-black",level:3},{value:"Issue 2: &quot;IMU readings are noisy&quot;",id:"issue-2-imu-readings-are-noisy",level:3},{value:"Issue 3: &quot;LiDAR doesn&#39;t detect objects&quot;",id:"issue-3-lidar-doesnt-detect-objects",level:3},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Camera Calibration (Required)",id:"exercise-1-camera-calibration-required",level:3},{value:"Exercise 2: IMU Drift Analysis (Required)",id:"exercise-2-imu-drift-analysis-required",level:3},{value:"Exercise 3: Sensor Fusion Implementation (Challenge)",id:"exercise-3-sensor-fusion-implementation-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-03-sensor-systems-for-humanoid-robots",children:"Chapter 0.3: Sensor Systems for Humanoid Robots"})}),"\n",(0,r.jsx)(n.p,{children:'Humanoid robots need "senses" to perceive the world\u2014just like humans have eyes, ears, and balance systems. This chapter covers the sensor systems that enable humanoid robots to see, hear, feel, and maintain balance. You\'ll learn about cameras, LiDAR, IMUs, force/torque sensors, and how to simulate them in PyBullet.'}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," different sensor types used in humanoid robots"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Explain"})," how vision sensors (cameras, depth cameras) work"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Describe"})," LiDAR for 3D mapping and obstacle detection"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," IMUs for balance and orientation tracking"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Implement"})," basic sensor simulation in PyBullet"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Appreciate"})," sensor fusion for robust perception"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Chapters 0.1 to 0.2"})," completed (Physical AI concepts, humanoid landscape)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Basic Python"})," knowledge"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PyBullet"})," installed (from Chapter 0.1)"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-1-sensor-overview",children:"Part 1: Sensor Overview"}),"\n",(0,r.jsx)(n.h3,{id:"why-sensors-matter",children:"Why Sensors Matter"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Without sensors"}),", a robot is ",(0,r.jsx)(n.strong,{children:"blind and deaf"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Can't see obstacles \u2192 will collide"}),"\n",(0,r.jsx)(n.li,{children:"Can't hear commands \u2192 can't respond"}),"\n",(0,r.jsx)(n.li,{children:"Can't feel objects \u2192 can't grasp properly"}),"\n",(0,r.jsx)(n.li,{children:"Can't sense balance \u2192 will fall over"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"With sensors"}),", a robot can:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Perceive"})," the environment"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Navigate"})," safely"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Manipulate"})," objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Interact"})," with humans"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Maintain"})," balance"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"sensor-categories",children:"Sensor Categories"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Category"}),(0,r.jsx)(n.th,{children:"Purpose"}),(0,r.jsx)(n.th,{children:"Examples"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Vision"})}),(0,r.jsx)(n.td,{children:"See the world"}),(0,r.jsx)(n.td,{children:"RGB cameras, depth cameras, stereo vision"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Range"})}),(0,r.jsx)(n.td,{children:"Measure distances"}),(0,r.jsx)(n.td,{children:"LiDAR, ultrasonic sensors, depth cameras"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Motion"})}),(0,r.jsx)(n.td,{children:"Track movement"}),(0,r.jsx)(n.td,{children:"IMU, encoders, gyroscopes"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Force/Torque"})}),(0,r.jsx)(n.td,{children:"Feel forces"}),(0,r.jsx)(n.td,{children:"Force sensors, tactile sensors"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Audio"})}),(0,r.jsx)(n.td,{children:"Hear sounds"}),(0,r.jsx)(n.td,{children:"Microphones, microphone arrays"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Humanoid robots"})," typically use ",(0,r.jsx)(n.strong,{children:"all of these"})," for robust perception."]}),"\n",(0,r.jsx)(n.h2,{id:"part-2-vision-sensors",children:"Part 2: Vision Sensors"}),"\n",(0,r.jsx)(n.h3,{id:"rgb-cameras",children:"RGB Cameras"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What they do"}),": Capture color images (like human eyes)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How they work"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lens"}),": Focuses light onto sensor"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Sensor"}),": Converts light to electrical signals"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Processor"}),": Converts signals to digital image (RGB pixels)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Specifications"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": 640x480 to 4K+ (more pixels = more detail)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Frame rate"}),": 30 to 60 FPS (frames per second)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Field of view"}),": 60 to 120\xb0 (how wide the view)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use cases"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object detection and recognition"}),"\n",(0,r.jsx)(n.li,{children:"Human detection and tracking"}),"\n",(0,r.jsx)(n.li,{children:"Visual navigation (following paths, reading signs)"}),"\n",(0,r.jsx)(n.li,{children:"Gesture recognition"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Lighting"}),": Poor performance in dark or bright conditions"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Occlusion"}),": Can't see behind objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"2D only"}),": No depth information (unless stereo)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"depth-cameras",children:"Depth Cameras"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What they do"}),": Measure distance to objects (like human depth perception)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How they work"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Structured light"}),": Projects pattern, measures distortion (Intel RealSense)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time-of-flight"}),": Measures time for light to bounce back"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stereo vision"}),": Two cameras, triangulate distance"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Depth map (each pixel = distance in meters)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Use cases"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"3D mapping and SLAM"}),"\n",(0,r.jsx)(n.li,{children:"Obstacle avoidance"}),"\n",(0,r.jsx)(n.li,{children:"Object manipulation (knowing how far to reach)"}),"\n",(0,r.jsx)(n.li,{children:"Human pose estimation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Specifications"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Range"}),": 0.5m to 10m+ (depending on technology)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": \xb11 to 5cm (varies with distance)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Resolution"}),": 640x480 to 1080p"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reflective surfaces"}),": Poor performance (glass, mirrors)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Outdoor lighting"}),": Struggles in bright sunlight"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Range limits"}),": Can't see beyond maximum range"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"stereo-vision",children:"Stereo Vision"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What they do"}),": Use two cameras to compute depth (like human eyes)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Two cameras capture images from slightly different positions"}),"\n",(0,r.jsx)(n.li,{children:"Find matching points in both images"}),"\n",(0,r.jsx)(n.li,{children:"Compute depth using triangulation"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Advantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Passive"}),": No special lighting needed"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Works outdoors"}),": Better than structured light in sunlight"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Color + depth"}),": RGB images with depth"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Disadvantages"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Texture required"}),": Needs features to match (fails on blank walls)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Computational"}),": More processing than single camera"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibration"}),": Cameras must be precisely aligned"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-3-lidar-light-detection-and-ranging",children:"Part 3: LiDAR (Light Detection and Ranging)"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-lidar",children:"What is LiDAR?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"})," uses lasers to measure distances in 360\xb0 around the robot."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Laser emitter"}),": Sends out laser pulses"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Reflection"}),": Light bounces off objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Receiver"}),": Detects reflected light"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Time measurement"}),": Calculate distance from time-of-flight"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Rotation"}),": Spins to scan 360\xb0"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Point cloud (3D points representing surfaces)"]}),"\n",(0,r.jsx)(n.h3,{id:"lidar-specifications",children:"LiDAR Specifications"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Range"}),": 10m to 200m+ (depending on model)\n",(0,r.jsx)(n.strong,{children:"Angular resolution"}),": 0.1\xb0 to 1\xb0 (how fine the scan)\n",(0,r.jsx)(n.strong,{children:"Scan rate"}),": 10 to 40 Hz (scans per second)\n",(0,r.jsx)(n.strong,{children:"Points per scan"}),": 1,000 to 1,000,000+ points"]}),"\n",(0,r.jsx)(n.h3,{id:"use-cases",children:"Use Cases"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Mapping"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Create 3D maps of environments"}),"\n",(0,r.jsx)(n.li,{children:"SLAM (Simultaneous Localization and Mapping)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Navigation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Obstacle detection and avoidance"}),"\n",(0,r.jsx)(n.li,{children:"Path planning in unknown environments"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Object Detection"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Identify objects by shape"}),"\n",(0,r.jsx)(n.li,{children:"Track moving objects"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"limitations",children:"Limitations"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cost"}),": Expensive ($1,000-$100,000+)\n",(0,r.jsx)(n.strong,{children:"Size"}),": Large and heavy (challenging for humanoids)\n",(0,r.jsx)(n.strong,{children:"Power"}),": High power consumption\n",(0,r.jsx)(n.strong,{children:"Outdoors"}),": Can struggle in rain, fog, bright sunlight"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Note"}),": Some humanoids use ",(0,r.jsx)(n.strong,{children:"solid-state LiDAR"})," (smaller, cheaper, but limited field of view)."]}),"\n",(0,r.jsx)(n.h2,{id:"part-4-imu-inertial-measurement-unit",children:"Part 4: IMU (Inertial Measurement Unit)"}),"\n",(0,r.jsx)(n.h3,{id:"what-is-an-imu",children:"What is an IMU?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"IMU"})," measures motion and orientation (like human inner ear/balance system)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Components"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer"}),": Measures linear acceleration (gravity + motion)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope"}),": Measures angular velocity (rotation)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Magnetometer"}),": Measures magnetic field (compass, optional)"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"how-imus-work",children:"How IMUs Work"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Accelerometer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses tiny masses that move when accelerated"}),"\n",(0,r.jsx)(n.li,{children:"Measures force on masses \u2192 acceleration"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Gravity"}),": Always measures 9.81 m/s\xb2 downward (when stationary)"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Gyroscope"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Uses spinning masses or MEMS (Micro-Electro-Mechanical Systems)"}),"\n",(0,r.jsx)(n.li,{children:"Measures rotation rate (degrees/second)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Integration"}),": Can compute orientation over time"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Magnetometer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measures Earth's magnetic field"}),"\n",(0,r.jsx)(n.li,{children:"Provides absolute heading (north direction)"}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Calibration"}),": Needs calibration for accuracy"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"use-cases-1",children:"Use Cases"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Balance Control"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Detect tilt and rotation"}),"\n",(0,r.jsx)(n.li,{children:"Maintain upright posture"}),"\n",(0,r.jsx)(n.li,{children:"Prevent falls"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Navigation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Dead reckoning (estimate position from motion)"}),"\n",(0,r.jsx)(n.li,{children:"Orientation tracking"}),"\n",(0,r.jsx)(n.li,{children:"Step counting (for walking robots)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Motion Tracking"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Measure movement during manipulation"}),"\n",(0,r.jsx)(n.li,{children:"Detect impacts or collisions"}),"\n",(0,r.jsx)(n.li,{children:"Monitor robot health"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"limitations-1",children:"Limitations"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Drift"}),": Orientation estimate drifts over time (gyroscope integration error)\n",(0,r.jsx)(n.strong,{children:"Noise"}),": Sensor measurements are noisy\n",(0,r.jsx)(n.strong,{children:"Calibration"}),": Requires calibration for accuracy\n",(0,r.jsx)(n.strong,{children:"Magnetic interference"}),": Magnetometer affected by metal objects"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solution"}),": ",(0,r.jsx)(n.strong,{children:"Sensor fusion"})," (combine IMU with other sensors like cameras)."]}),"\n",(0,r.jsx)(n.h2,{id:"part-5-forcetorque-sensors",children:"Part 5: Force/Torque Sensors"}),"\n",(0,r.jsx)(n.h3,{id:"what-are-forcetorque-sensors",children:"What are Force/Torque Sensors?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Force sensors"})," measure forces applied to the robot (like human touch)."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Types"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force sensors"}),": Measure linear forces (push/pull)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Torque sensors"}),": Measure rotational forces (twist)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Tactile sensors"}),": Measure contact pressure distribution"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"use-cases-2",children:"Use Cases"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Manipulation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grasping"}),": Know how hard to grip (don't crush objects)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Contact detection"}),": Know when touching objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force control"}),": Apply specific forces"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Balance"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Foot contact"}),": Detect when foot touches ground"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Weight distribution"}),": Measure forces on each foot"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Stability"}),": Detect if robot is tipping"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Collision detection"}),": Detect unexpected contact"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force limits"}),": Stop if force exceeds threshold"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Human interaction"}),": Gentle touch detection"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"specifications",children:"Specifications"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Range"}),": 0.1N to 10,000N+ (depending on application)\n",(0,r.jsx)(n.strong,{children:"Accuracy"}),": \xb11 to 5% of full scale\n",(0,r.jsx)(n.strong,{children:"Update rate"}),": 1 to 10 kHz (very fast)"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Placement"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Wrists"}),": Measure manipulation forces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Ankles"}),": Measure walking forces"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fingers"}),": Measure grasping forces"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-6-sensor-fusion",children:"Part 6: Sensor Fusion"}),"\n",(0,r.jsx)(n.h3,{id:"why-combine-sensors",children:"Why Combine Sensors?"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Single sensor limitations"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera"}),": No depth (unless stereo/depth camera)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": No color information"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Drifts over time"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force sensor"}),": Only measures contact"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Sensor fusion"})," combines multiple sensors for:"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Robustness"}),": If one sensor fails, others compensate"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Accuracy"}),": Multiple measurements improve accuracy"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Completeness"}),": Get both color and depth, position and orientation"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"common-fusion-strategies",children:"Common Fusion Strategies"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Vision + LiDAR"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Color, texture, object recognition"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"LiDAR"}),": Accurate 3D structure"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combined"}),": Colored 3D point cloud"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"IMU + Vision"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Fast motion tracking (high frequency)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": Accurate position (low frequency, no drift)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combined"}),": Smooth, accurate motion estimate"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Force + Vision"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Vision"}),": See object before contact"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Force"}),": Feel object during contact"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Combined"}),": Robust manipulation"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-7-hands-on-tutorial",children:"Part 7: Hands-On Tutorial"}),"\n",(0,r.jsx)(n.h3,{id:"project-simulate-sensors-in-pybullet",children:"Project: Simulate Sensors in PyBullet"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Goal"}),": Add sensors to a robot simulation and visualize sensor data."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tools"}),": PyBullet, Python 3.10+, NumPy, OpenCV (optional)"]}),"\n",(0,r.jsx)(n.h3,{id:"step-1-camera-simulation",children:"Step 1: Camera Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create file"}),": ",(0,r.jsx)(n.code,{children:"camera_sensor.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nCamera sensor simulation in PyBullet\nDemonstrates RGB and depth camera rendering\n"""\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport cv2\nimport time\n\n# Connect to physics server\nphysicsClient = p.connect(p.GUI)\np.setAdditionalSearchPath(pybullet_data.getDataPath())\n\n# Set gravity\np.setGravity(0, 0, -9.81)\n\n# Load ground\nplaneId = p.loadURDF("plane.urdf")\n\n# Create a simple scene with objects\nboxId = p.loadURDF("cube.urdf", [2, 0, 0.5], [0, 0, 0, 1])\nsphereId = p.loadURDF("sphere.urdf", [-2, 0, 0.5], [0, 0, 0, 1])\n\n# Camera parameters\ncamera_pos = [0, 0, 2]  # Camera position\ncamera_target = [0, 0, 0]  # Where camera looks\ncamera_up = [0, 1, 0]  # Up direction\n\n# Camera view matrix\nview_matrix = p.computeViewMatrix(\n    cameraEyePosition=camera_pos,\n    cameraTargetPosition=camera_target,\n    cameraUpVector=camera_up\n)\n\n# Camera projection matrix\naspect = 640.0 / 480.0  # Width/height\nfov = 60  # Field of view (degrees)\nnear = 0.01  # Near plane\nfar = 10  # Far plane\n\nprojection_matrix = p.computeProjectionMatrixFOV(\n    fov, aspect, near, far\n)\n\nprint("Camera sensor simulation started!")\nprint("Capturing RGB and depth images...")\n\n# Capture images\nfor i in range(100):\n    p.stepSimulation()\n    \n    # Render camera view\n    width, height, rgbImg, depthImg, segImg = p.getCameraImage(\n        width=640,\n        height=480,\n        viewMatrix=view_matrix,\n        projectionMatrix=projection_matrix\n    )\n    \n    # Convert RGB image (4 channels: RGBA)\n    rgb_array = np.array(rgbImg)\n    rgb_array = rgb_array[:, :, :3]  # Remove alpha channel\n    rgb_array = cv2.cvtColor(rgb_array, cv2.COLOR_RGB2BGR)  # OpenCV uses BGR\n    \n    # Convert depth image\n    depth_array = np.array(depthImg)\n    \n    # Normalize depth for visualization (0 to 255)\n    depth_normalized = (depth_array * 255 / far).astype(np.uint8)\n    depth_colormap = cv2.applyColorMap(depth_normalized, cv2.COLOR_MAP_INFERNO)\n    \n    # Display images (every 10 steps)\n    if i % 10 == 0:\n        cv2.imshow("RGB Camera", rgb_array)\n        cv2.imshow("Depth Camera", depth_colormap)\n        cv2.waitKey(1)\n        \n        print(f"Step {i}: Captured RGB ({rgb_array.shape}) and depth ({depth_array.shape})")\n    \n    time.sleep(1./240.)\n\ncv2.destroyAllWindows()\np.disconnect()\nprint("Camera simulation complete!")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Run simulation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 camera_sensor.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Two windows showing RGB and depth camera views"}),"\n",(0,r.jsx)(n.li,{children:"RGB shows colored objects"}),"\n",(0,r.jsx)(n.li,{children:"Depth shows distance (closer = brighter)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-2-imu-simulation",children:"Step 2: IMU Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create file"}),": ",(0,r.jsx)(n.code,{children:"imu_sensor.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nIMU sensor simulation in PyBullet\nDemonstrates accelerometer and gyroscope measurements\n"""\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport time\n\n# Connect to physics server\nphysicsClient = p.connect(p.GUI)\np.setAdditionalSearchPath(pybullet_data.getDataPath())\n\n# Set gravity\np.setGravity(0, 0, -9.81)\n\n# Load ground\nplaneId = p.loadURDF("plane.urdf")\n\n# Create a box that will tilt\nboxId = p.loadURDF("cube.urdf", [0, 0, 1], [0, 0, 0, 1])\n\n# Simulate IMU on the box\nprint("IMU sensor simulation started!")\nprint("Measuring acceleration and angular velocity...")\n\nprevious_linear_velocity = np.array([0, 0, 0])\nprevious_angular_velocity = np.array([0, 0, 0])\n\nfor i in range(1000):\n    p.stepSimulation()\n    \n    # Get box state\n    pos, orient = p.getBasePositionAndOrientation(boxId)\n    linear_velocity, angular_velocity = p.getBaseVelocity(boxId)\n    \n    # Convert to numpy arrays\n    linear_velocity = np.array(linear_velocity)\n    angular_velocity = np.array(angular_velocity)\n    \n    # Simulate accelerometer (measures linear acceleration + gravity)\n    # In real IMU: acceleration = linear_acceleration + gravity (in sensor frame)\n    # For simplicity, we\'ll compute linear acceleration from velocity change\n    dt = 1./240.  # Simulation timestep\n    linear_acceleration = (linear_velocity - previous_linear_velocity) / dt\n    \n    # Add gravity (in world frame, pointing down in Z)\n    gravity_world = np.array([0, 0, -9.81])\n    \n    # Transform gravity to box frame (simplified - would use rotation matrix)\n    # For now, assume box is upright (gravity = [0, 0, -9.81] in box frame)\n    # In reality, need to rotate gravity vector by box orientation\n    accelerometer_reading = linear_acceleration + gravity_world\n    \n    # Gyroscope reading (angular velocity in rad/s)\n    gyroscope_reading = angular_velocity\n    \n    # Print readings every 50 steps\n    if i % 50 == 0:\n        print(f"\\nStep {i}:")\n        print(f"  Position: ({pos[0]:.2f}, {pos[1]:.2f}, {pos[2]:.2f})")\n        print(f"  Accelerometer: ({accelerometer_reading[0]:.2f}, {accelerometer_reading[1]:.2f}, {accelerometer_reading[2]:.2f}) m/s\xb2")\n        print(f"  Gyroscope: ({gyroscope_reading[0]:.2f}, {gyroscope_reading[1]:.2f}, {gyroscope_reading[2]:.2f}) rad/s")\n    \n    previous_linear_velocity = linear_velocity\n    previous_angular_velocity = angular_velocity\n    \n    time.sleep(1./240.)\n\np.disconnect()\nprint("IMU simulation complete!")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Run simulation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 imu_sensor.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Box falls and tilts"}),"\n",(0,r.jsx)(n.li,{children:"Accelerometer readings show gravity + motion"}),"\n",(0,r.jsx)(n.li,{children:"Gyroscope readings show rotation"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-3-lidar-simulation",children:"Step 3: LiDAR Simulation"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create file"}),": ",(0,r.jsx)(n.code,{children:"lidar_sensor.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nLiDAR sensor simulation in PyBullet\nDemonstrates ray casting for distance measurement\n"""\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport time\n\n# Connect to physics server\nphysicsClient = p.connect(p.GUI)\np.setAdditionalSearchPath(pybullet_data.getDataPath())\n\n# Set gravity\np.setGravity(0, 0, -9.81)\n\n# Load ground\nplaneId = p.loadURDF("plane.urdf")\n\n# Create objects to detect\nboxId = p.loadURDF("cube.urdf", [2, 0, 0.5], [0, 0, 0, 1])\nsphereId = p.loadURDF("sphere.urdf", [-2, 0, 0.5], [0, 0, 0, 1])\n\n# LiDAR parameters\nlidar_pos = [0, 0, 1]  # LiDAR position\nnum_rays = 360  # Number of rays (360\xb0 scan)\nmax_range = 10  # Maximum range (meters)\nray_length = max_range\n\nprint("LiDAR sensor simulation started!")\nprint(f"Scanning {num_rays} rays in 360\xb0...")\n\n# Perform LiDAR scan\nray_from = []\nray_to = []\nray_results = []\n\nfor i in range(num_rays):\n    angle = 2 * np.pi * i / num_rays  # Angle in radians\n    \n    # Compute ray direction\n    ray_dir = [np.cos(angle), np.sin(angle), 0]  # Horizontal scan\n    ray_end = [\n        lidar_pos[0] + ray_length * ray_dir[0],\n        lidar_pos[1] + ray_length * ray_dir[1],\n        lidar_pos[2] + ray_length * ray_dir[2]\n    ]\n    \n    ray_from.append(lidar_pos)\n    ray_to.append(ray_end)\n\n# Cast rays\nray_results = p.rayTestBatch(ray_from, ray_to)\n\n# Extract distances\ndistances = []\nfor result in ray_results:\n    if result[0] != -1:  # Hit something\n        hit_pos = result[3]  # Hit position\n        distance = np.linalg.norm(np.array(hit_pos) - np.array(lidar_pos))\n        distances.append(distance)\n    else:  # No hit (max range)\n        distances.append(max_range)\n\n# Visualize LiDAR scan\nangles = np.linspace(0, 2*np.pi, num_rays)\nplt.figure(figsize=(10, 10))\nplt.polar(angles, distances)\nplt.title("LiDAR Scan (360\xb0)")\nplt.xlabel("Distance (m)")\nplt.show()\n\nprint(f"LiDAR scan complete!")\nprint(f"Detected {sum(1 for d in distances if d < max_range)} hits")\nprint(f"Average distance: {np.mean([d for d in distances if d < max_range]):.2f} m")\n\np.disconnect()\nprint("LiDAR simulation complete!")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Run simulation"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"python3 lidar_sensor.py\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Expected Output"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Polar plot showing 360\xb0 LiDAR scan"}),"\n",(0,r.jsx)(n.li,{children:"Detects objects at different distances"}),"\n",(0,r.jsx)(n.li,{children:"Visual representation of environment"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"step-4-sensor-fusion-example",children:"Step 4: Sensor Fusion Example"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Create file"}),": ",(0,r.jsx)(n.code,{children:"sensor_fusion.py"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nSimple sensor fusion example\nCombines camera and IMU for better state estimation\n"""\nimport pybullet as p\nimport pybullet_data\nimport numpy as np\nimport time\n\n# Connect to physics server\nphysicsClient = p.connect(p.GUI)\np.setAdditionalSearchPath(pybullet_data.getDataPath())\n\np.setGravity(0, 0, -9.81)\nplaneId = p.loadURDF("plane.urdf")\nboxId = p.loadURDF("cube.urdf", [0, 0, 1], [0, 0, 0, 1])\n\nprint("Sensor fusion simulation started!")\nprint("Combining camera and IMU measurements...")\n\n# Camera (low frequency, accurate position)\ncamera_update_rate = 10  # Hz\ncamera_last_update = 0\n\n# IMU (high frequency, drifts)\nimu_update_rate = 240  # Hz\n\n# State estimate (fused)\nestimated_pos = np.array([0, 0, 1])\nestimated_vel = np.array([0, 0, 0])\n\nfor i in range(1000):\n    p.stepSimulation()\n    t = i / 240.0  # Time in seconds\n    \n    # Get true state (for comparison)\n    true_pos, _ = p.getBasePositionAndOrientation(boxId)\n    true_vel, _ = p.getBaseVelocity(boxId)\n    true_pos = np.array(true_pos)\n    true_vel = np.array(true_vel)\n    \n    # IMU measurement (high frequency)\n    if i % (240 // imu_update_rate) == 0:\n        # Update velocity estimate from IMU\n        # (In reality, would integrate accelerometer)\n        estimated_vel = true_vel + np.random.normal(0, 0.01, 3)  # Add noise\n    \n    # Camera measurement (low frequency, accurate)\n    if i % (240 // camera_update_rate) == 0:\n        # Reset position estimate from camera\n        estimated_pos = true_pos + np.random.normal(0, 0.001, 3)  # Very accurate\n        camera_last_update = t\n    \n    # Fuse estimates (simple: use camera when available, IMU otherwise)\n    # Update position from velocity\n    dt = 1./240.\n    if t - camera_last_update < 1./camera_update_rate:\n        # Recent camera update, trust it\n        pass  # Keep camera position\n    else:\n        # No recent camera update, use IMU\n        estimated_pos = estimated_pos + estimated_vel * dt\n    \n    # Print comparison every 100 steps\n    if i % 100 == 0:\n        error = np.linalg.norm(estimated_pos - true_pos)\n        print(f"Step {i}: True pos = {true_pos}, Estimated pos = {estimated_pos}, Error = {error:.3f} m")\n    \n    time.sleep(1./240.)\n\np.disconnect()\nprint("Sensor fusion simulation complete!")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"What this demonstrates"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Camera"}),": Accurate but slow (low frequency)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"IMU"}),": Fast but drifts (high frequency)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Fusion"}),": Combines both for best estimate"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"part-8-debugging-common-issues",children:"Part 8: Debugging Common Issues"}),"\n",(0,r.jsx)(n.h3,{id:"issue-1-camera-image-is-black",children:'Issue 1: "Camera image is black"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Camera returns all zeros or black image"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Check camera position and target\n# Ensure objects are in view\n# Verify camera is not inside an object\n# Check lighting (some simulators need light sources)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"issue-2-imu-readings-are-noisy",children:'Issue 2: "IMU readings are noisy"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": Accelerometer/gyroscope values jump around"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Add filtering (moving average, Kalman filter)\n# Reduce simulation timestep (more frequent updates)\n# Add noise model (real sensors are noisy)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"issue-3-lidar-doesnt-detect-objects",children:'Issue 3: "LiDAR doesn\'t detect objects"'}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Symptoms"}),": All rays return max range"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"# Check ray origin and direction\n# Verify objects are within range\n# Check collision detection is enabled\n# Ensure objects have collision geometry\n"})}),"\n",(0,r.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,r.jsx)(n.p,{children:"You learned:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Vision sensors"}),": RGB cameras, depth cameras, stereo vision for seeing the world"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"LiDAR"}),": 3D mapping and obstacle detection using lasers"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"IMU"}),": Balance, orientation, and motion tracking"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Force/torque sensors"}),": Tactile feedback for manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Sensor fusion"}),": Combining multiple sensors for robust perception"]}),"\n",(0,r.jsxs)(n.li,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"Simulation"}),": How to simulate sensors in PyBullet"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next steps"}),": Complete the Module 0 project\u2014build a robot simulation with sensors!"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"exercise-1-camera-calibration-required",children:"Exercise 1: Camera Calibration (Required)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Understand camera parameters and calibration."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create simulation with camera"}),"\n",(0,r.jsxs)(n.li,{children:["Experiment with different:","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Field of view (FOV)"}),"\n",(0,r.jsx)(n.li,{children:"Resolution"}),"\n",(0,r.jsx)(n.li,{children:"Camera positions"}),"\n"]}),"\n"]}),"\n",(0,r.jsx)(n.li,{children:"Observe how parameters affect image"}),"\n",(0,r.jsx)(n.li,{children:"Document findings"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Camera simulation working"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Different FOVs tested"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Different resolutions tested"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Observations documented"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 45 minutes"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-2-imu-drift-analysis-required",children:"Exercise 2: IMU Drift Analysis (Required)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Understand IMU drift and its impact."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create simulation with IMU"}),"\n",(0,r.jsx)(n.li,{children:"Track orientation over time"}),"\n",(0,r.jsx)(n.li,{children:"Measure drift (error accumulation)"}),"\n",(0,r.jsx)(n.li,{children:"Compare with ground truth"}),"\n",(0,r.jsx)(n.li,{children:"Document drift rate"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,r.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","IMU simulation working"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Orientation tracked over time"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Drift measured and documented"]}),"\n",(0,r.jsxs)(n.li,{className:"task-list-item",children:[(0,r.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Comparison with ground truth"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 60 minutes"]}),"\n",(0,r.jsx)(n.h3,{id:"exercise-3-sensor-fusion-implementation-challenge",children:"Exercise 3: Sensor Fusion Implementation (Challenge)"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Objective"}),": Implement simple sensor fusion."]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Create simulation with camera and IMU"}),"\n",(0,r.jsx)(n.li,{children:"Implement fusion algorithm (weighted average or Kalman filter)"}),"\n",(0,r.jsx)(n.li,{children:"Compare fused estimate to individual sensors"}),"\n",(0,r.jsx)(n.li,{children:"Measure improvement in accuracy"}),"\n",(0,r.jsx)(n.li,{children:"Document fusion approach"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera and IMU simulated"}),"\n",(0,r.jsx)(n.li,{children:"Fusion algorithm implemented"}),"\n",(0,r.jsx)(n.li,{children:"Accuracy improvement demonstrated"}),"\n",(0,r.jsx)(n.li,{children:"Approach documented"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://docs.google.com/document/d/10sXEhzFRSnvFcl3XxNGhnD4N2SedqwdAvK3dsihxVUA/edit",children:"PyBullet Camera API"})," - Camera documentation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://www.intelrealsense.com/",children:"Intel RealSense"})," - Depth camera technology"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://velodynelidar.com/",children:"Velodyne LiDAR"})," - LiDAR specifications"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.a,{href:"https://github.com/rlabbe/Kalman-and-Bayesian-Filters-in-Python",children:"Sensor Fusion Tutorial"})," - Kalman filtering"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-0/intro#assessment",children:"Module 0 Project: Basic Robot Simulation \u2192"})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Return to"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-0/intro",children:"Module 0 Introduction \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>l,x:()=>o});var i=s(6540);const r={},t=i.createContext(r);function l(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function o(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:l(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);
"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[4617],{8048:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>d,contentTitle:()=>l,default:()=>u,frontMatter:()=>o,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"module-4/chapter-4-4","title":"4.4 Multi-Modal Integration (Speech + Vision + Gesture)","description":"Human-robot interaction is most natural when robots understand multiple input modalities: speech, vision, and gesture. Combining these modalities enables robust understanding even when individual modalities fail or are ambiguous. This chapter covers fusing multi-modal inputs for unified perception-action pipelines.","source":"@site/docs/module-4/chapter-4-4.md","sourceDirName":"module-4","slug":"/module-4/chapter-4-4","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-4","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-4/chapter-4-4.md","tags":[],"version":"current","sidebarPosition":5,"frontMatter":{"sidebar_position":5,"title":"4.4 Multi-Modal Integration (Speech + Vision + Gesture)"},"sidebar":"defaultSidebar","previous":{"title":"4.3 Natural Language to ROS 2 Actions","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-3"},"next":{"title":"4.5 Humanoid Kinematics & Balance Control","permalink":"/physical-ai-humanoid-robotics-book/docs/module-4/chapter-4-5"}}');var t=s(4848),r=s(8453);const o={sidebar_position:5,title:"4.4 Multi-Modal Integration (Speech + Vision + Gesture)"},l="Chapter 4.4: Multi-Modal Integration (Speech + Vision + Gesture)",d={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Prerequisites",id:"prerequisites",level:2},{value:"Part 1: Multi-Modal Fusion Fundamentals",id:"part-1-multi-modal-fusion-fundamentals",level:2},{value:"Why Multi-Modal?",id:"why-multi-modal",level:3},{value:"Fusion Strategies",id:"fusion-strategies",level:3},{value:"Modality Types",id:"modality-types",level:3},{value:"Part 2: Hands-On Tutorial",id:"part-2-hands-on-tutorial",level:2},{value:"Project: Multi-Modal Command Understanding",id:"project-multi-modal-command-understanding",level:3},{value:"Step 1: Set Up Vision System",id:"step-1-set-up-vision-system",level:3},{value:"Step 2: Set Up Gesture Recognition",id:"step-2-set-up-gesture-recognition",level:3},{value:"Step 3: Create Multi-Modal Fusion Node",id:"step-3-create-multi-modal-fusion-node",level:3},{value:"Step 4: Update LLM Planner for Multi-Modal Input",id:"step-4-update-llm-planner-for-multi-modal-input",level:3},{value:"Step 5: Test Multi-Modal Pipeline",id:"step-5-test-multi-modal-pipeline",level:3},{value:"Step 6: Debugging Common Issues",id:"step-6-debugging-common-issues",level:3},{value:"Issue 1: &quot;Modalities not synchronizing&quot;",id:"issue-1-modalities-not-synchronizing",level:4},{value:"Issue 2: &quot;Object matching fails&quot;",id:"issue-2-object-matching-fails",level:4},{value:"Issue 3: &quot;Gesture recognition inaccurate&quot;",id:"issue-3-gesture-recognition-inaccurate",level:4},{value:"Part 3: Advanced Topics (Optional)",id:"part-3-advanced-topics-optional",level:2},{value:"Attention-Based Fusion",id:"attention-based-fusion",level:3},{value:"Temporal Fusion",id:"temporal-fusion",level:3},{value:"Integration with Capstone",id:"integration-with-capstone",level:2},{value:"Summary",id:"summary",level:2},{value:"Exercises",id:"exercises",level:2},{value:"Exercise 1: Basic Multi-Modal Fusion (Required)",id:"exercise-1-basic-multi-modal-fusion-required",level:3},{value:"Exercise 2: Gesture Integration (Required)",id:"exercise-2-gesture-integration-required",level:3},{value:"Exercise 3: Attention-Based Fusion (Challenge)",id:"exercise-3-attention-based-fusion-challenge",level:3},{value:"Additional Resources",id:"additional-resources",level:2}];function a(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",hr:"hr",input:"input",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,r.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"chapter-44-multi-modal-integration-speech--vision--gesture",children:"Chapter 4.4: Multi-Modal Integration (Speech + Vision + Gesture)"})}),"\n",(0,t.jsx)(n.p,{children:"Human-robot interaction is most natural when robots understand multiple input modalities: speech, vision, and gesture. Combining these modalities enables robust understanding even when individual modalities fail or are ambiguous. This chapter covers fusing multi-modal inputs for unified perception-action pipelines."}),"\n",(0,t.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,t.jsx)(n.p,{children:"By the end of this chapter, you will be able to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Combine"})," voice commands with visual perception for object identification"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Implement"})," gesture recognition for human-robot interaction"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Fuse"})," multi-modal inputs for robust understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Create"})," unified perception-action pipeline"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Handle"})," modality conflicts and ambiguity resolution"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Chapters 4.1 to 4.3"})," completed (Whisper, LLM planning, action execution)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision system"})," (cameras, object detection) from Module 2/3"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2 Humble"})," configured"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Python 3.10+"})," with computer vision libraries (OpenCV, mediapipe)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Basic ML concepts"}),": Object detection, pose estimation"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"part-1-multi-modal-fusion-fundamentals",children:"Part 1: Multi-Modal Fusion Fundamentals"}),"\n",(0,t.jsx)(n.h3,{id:"why-multi-modal",children:"Why Multi-Modal?"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Single modality limitations"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech only"}),': "Pick up the cup" \u2192 Which cup? (ambiguous)']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision only"}),": Sees cup but doesn't know what human wants"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Gesture only"}),": Pointing gesture \u2192 What action? (unclear)"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Multi-modal advantages"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Disambiguation"}),': Speech + vision \u2192 "Pick up the red cup" + vision identifies red cup']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robustness"}),": If one modality fails, others compensate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rich context"}),": Multiple cues improve understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural interaction"}),": Humans use speech + gesture + vision together"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"fusion-strategies",children:"Fusion Strategies"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Strategy"}),(0,t.jsx)(n.th,{children:"Approach"}),(0,t.jsx)(n.th,{children:"Use Case"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Early Fusion"})}),(0,t.jsx)(n.td,{children:"Combine raw inputs"}),(0,t.jsx)(n.td,{children:"When modalities are complementary"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Late Fusion"})}),(0,t.jsx)(n.td,{children:"Combine processed outputs"}),(0,t.jsx)(n.td,{children:"When modalities are independent"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Attention-Based"})}),(0,t.jsx)(n.td,{children:"Weight modalities dynamically"}),(0,t.jsx)(n.td,{children:"When reliability varies"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Hierarchical"})}),(0,t.jsx)(n.td,{children:"Use one modality to guide another"}),(0,t.jsx)(n.td,{children:"When one modality is primary"})]})]})]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"This chapter uses"}),": Late fusion (combine processed outputs) and attention-based weighting."]}),"\n",(0,t.jsx)(n.h3,{id:"modality-types",children:"Modality Types"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Modality"}),(0,t.jsx)(n.th,{children:"Input"}),(0,t.jsx)(n.th,{children:"Processing"}),(0,t.jsx)(n.th,{children:"Output"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Speech"})}),(0,t.jsx)(n.td,{children:"Audio"}),(0,t.jsx)(n.td,{children:"Whisper \u2192 Text"}),(0,t.jsx)(n.td,{children:"Command text"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Vision"})}),(0,t.jsx)(n.td,{children:"Images"}),(0,t.jsx)(n.td,{children:"Object detection \u2192 Bounding boxes"}),(0,t.jsx)(n.td,{children:"Object list with poses"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:(0,t.jsx)(n.strong,{children:"Gesture"})}),(0,t.jsx)(n.td,{children:"Images/IMU"}),(0,t.jsx)(n.td,{children:"Pose estimation \u2192 Gesture classification"}),(0,t.jsx)(n.td,{children:"Gesture type (point, wave, etc.)"})]})]})]}),"\n",(0,t.jsx)(n.h2,{id:"part-2-hands-on-tutorial",children:"Part 2: Hands-On Tutorial"}),"\n",(0,t.jsx)(n.h3,{id:"project-multi-modal-command-understanding",children:"Project: Multi-Modal Command Understanding"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Goal"}),": Combine speech, vision, and gesture recognition to understand human commands robustly."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tools"}),": Whisper, OpenCV, MediaPipe, ROS 2 Humble, Python 3.10+"]}),"\n",(0,t.jsx)(n.h3,{id:"step-1-set-up-vision-system",children:"Step 1: Set Up Vision System"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Install computer vision libraries"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"pip3 install opencv-python mediapipe numpy\nsudo apt install ros-humble-vision-msgs\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create vision node"}),": ",(0,t.jsx)(n.code,{children:"voice_commands/vision_node.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nVision processing node for object detection\nROS 2 Humble | Python 3.10+ | OpenCV\n\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom vision_msgs.msg import Detection2DArray, ObjectHypothesisWithPose\nfrom cv_bridge import CvBridge\nimport cv2\nimport numpy as np\n\nclass VisionNode(Node):\n    \"\"\"\n    Processes camera images and detects objects\n    Publishes object detections for multi-modal fusion\n    \"\"\"\n    def __init__(self):\n        super().__init__('vision_node')\n        \n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            '/humanoid/camera/image_raw',\n            self.image_callback,\n            10\n        )\n        \n        # Publisher for detections\n        self.detections_pub = self.create_publisher(\n            Detection2DArray,\n            '/vision/detections',\n            10\n        )\n        \n        # CV Bridge\n        self.bridge = CvBridge()\n        \n        # Simple color-based object detection (would use YOLO/SSD in production)\n        self.color_ranges = {\n            'red': ([0, 50, 50], [10, 255, 255]),\n            'blue': ([100, 50, 50], [130, 255, 255]),\n            'green': ([50, 50, 50], [70, 255, 255]),\n        }\n        \n        self.get_logger().info('Vision node started')\n    \n    def image_callback(self, msg):\n        \"\"\"Process image and detect objects\"\"\"\n        try:\n            # Convert ROS Image to OpenCV\n            cv_image = self.bridge.imgmsg_to_cv2(msg, 'bgr8')\n            \n            # Detect objects (simplified - would use ML model in production)\n            detections = self.detect_objects(cv_image)\n            \n            # Publish detections\n            if detections:\n                detections_msg = self.create_detections_msg(msg.header, detections)\n                self.detections_pub.publish(detections_msg)\n                \n        except Exception as e:\n            self.get_logger().error(f'Vision processing error: {e}')\n    \n    def detect_objects(self, image):\n        \"\"\"Detect objects in image (simplified color-based detection)\"\"\"\n        detections = []\n        \n        # Convert to HSV for color detection\n        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n        \n        # Detect colored objects\n        for color_name, (lower, upper) in self.color_ranges.items():\n            mask = cv2.inRange(hsv, np.array(lower), np.array(upper))\n            contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n            \n            for contour in contours:\n                area = cv2.contourArea(contour)\n                if area > 500:  # Minimum area threshold\n                    x, y, w, h = cv2.boundingRect(contour)\n                    center_x = x + w / 2\n                    center_y = y + h / 2\n                    \n                    detections.append({\n                        'class': f'{color_name}_cup',  # Simplified class name\n                        'confidence': 0.8,\n                        'bbox': [x, y, w, h],\n                        'center': [center_x, center_y]\n                    })\n        \n        return detections\n    \n    def create_detections_msg(self, header, detections):\n        \"\"\"Create ROS 2 detection message\"\"\"\n        detections_msg = Detection2DArray()\n        detections_msg.header = header\n        \n        for det in detections:\n            detection = vision_msgs.msg.Detection2D()\n            detection.bbox.center.x = det['center'][0]\n            detection.bbox.center.y = det['center'][1]\n            detection.bbox.size_x = det['bbox'][2]\n            detection.bbox.size_y = det['bbox'][3]\n            \n            hypothesis = ObjectHypothesisWithPose()\n            hypothesis.id = det['class']\n            hypothesis.score = det['confidence']\n            detection.results.append(hypothesis)\n            \n            detections_msg.detections.append(detection)\n        \n        return detections_msg\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = VisionNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-2-set-up-gesture-recognition",children:"Step 2: Set Up Gesture Recognition"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create gesture recognition node"}),": ",(0,t.jsx)(n.code,{children:"voice_commands/gesture_node.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'#!/usr/bin/env python3\n"""\nGesture recognition node using MediaPipe\nROS 2 Humble | Python 3.10+ | MediaPipe\n"""\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom std_msgs.msg import String\nfrom cv_bridge import CvBridge\nimport cv2\nimport mediapipe as mp\n\nclass GestureNode(Node):\n    """\n    Recognizes hand gestures from camera images\n    """\n    def __init__(self):\n        super().__init__(\'gesture_node\')\n        \n        # Subscribe to camera\n        self.image_sub = self.create_subscription(\n            Image,\n            \'/humanoid/camera/image_raw\',\n            self.image_callback,\n            10\n        )\n        \n        # Publisher for gestures\n        self.gesture_pub = self.create_publisher(String, \'/gestures/recognized\', 10)\n        \n        # MediaPipe hands\n        self.mp_hands = mp.solutions.hands\n        self.hands = self.mp_hands.Hands(\n            static_image_mode=False,\n            max_num_hands=2,\n            min_detection_confidence=0.5\n        )\n        self.mp_drawing = mp.solutions.drawing_utils\n        \n        self.bridge = CvBridge()\n        \n        # Gesture definitions (simplified)\n        self.gestures = {\n            \'point\': self.is_pointing_gesture,\n            \'wave\': self.is_waving_gesture,\n            \'grasp\': self.is_grasping_gesture,\n        }\n        \n        self.get_logger().info(\'Gesture node started\')\n    \n    def image_callback(self, msg):\n        """Process image and recognize gestures"""\n        try:\n            cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n            \n            # Convert BGR to RGB for MediaPipe\n            rgb_image = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)\n            \n            # Process with MediaPipe\n            results = self.hands.process(rgb_image)\n            \n            if results.multi_hand_landmarks:\n                for hand_landmarks in results.multi_hand_landmarks:\n                    # Classify gesture\n                    gesture = self.classify_gesture(hand_landmarks)\n                    \n                    if gesture:\n                        self.get_logger().info(f\'Gesture detected: {gesture}\')\n                        \n                        # Publish gesture\n                        msg = String()\n                        msg.data = gesture\n                        self.gesture_pub.publish(msg)\n                        \n        except Exception as e:\n            self.get_logger().error(f\'Gesture recognition error: {e}\')\n    \n    def classify_gesture(self, landmarks):\n        """Classify gesture from hand landmarks"""\n        # Get key points\n        thumb_tip = landmarks.landmark[self.mp_hands.HandLandmark.THUMB_TIP]\n        index_tip = landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]\n        middle_tip = landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n        \n        # Simple gesture classification\n        if self.is_pointing_gesture(landmarks):\n            return \'point\'\n        elif self.is_waving_gesture(landmarks):\n            return \'wave\'\n        elif self.is_grasping_gesture(landmarks):\n            return \'grasp\'\n        \n        return None\n    \n    def is_pointing_gesture(self, landmarks):\n        """Check if pointing gesture (index finger extended, others curled)"""\n        # Simplified: Check if index finger is extended and others are not\n        index_tip = landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_TIP]\n        index_pip = landmarks.landmark[self.mp_hands.HandLandmark.INDEX_FINGER_PIP]\n        middle_tip = landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_TIP]\n        middle_pip = landmarks.landmark[self.mp_hands.HandLandmark.MIDDLE_FINGER_PIP]\n        \n        # Index finger extended (tip above PIP)\n        index_extended = index_tip.y < index_pip.y\n        # Middle finger curled (tip below PIP)\n        middle_curled = middle_tip.y > middle_pip.y\n        \n        return index_extended and middle_curled\n    \n    def is_waving_gesture(self, landmarks):\n        """Check if waving gesture (all fingers extended, moving)"""\n        # Simplified: All fingers extended\n        fingers_extended = all(\n            landmarks.landmark[i].y < landmarks.landmark[i-2].y\n            for i in [4, 8, 12, 16, 20]  # Finger tips\n        )\n        return fingers_extended\n    \n    def is_grasping_gesture(self, landmarks):\n        """Check if grasping gesture (fingers curled)"""\n        # Simplified: Fingers curled\n        fingers_curled = all(\n            landmarks.landmark[i].y > landmarks.landmark[i-2].y\n            for i in [8, 12, 16, 20]  # Finger tips (excluding thumb)\n        )\n        return fingers_curled\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = GestureNode()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-3-create-multi-modal-fusion-node",children:"Step 3: Create Multi-Modal Fusion Node"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Create fusion node"}),": ",(0,t.jsx)(n.code,{children:"voice_commands/multimodal_fusion.py"})]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"#!/usr/bin/env python3\n\"\"\"\nMulti-modal fusion node combining speech, vision, and gesture\nROS 2 Humble | Python 3.10+\n\"\"\"\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\nfrom vision_msgs.msg import Detection2DArray\nimport json\nfrom collections import deque\n\nclass MultimodalFusion(Node):\n    \"\"\"\n    Fuses speech, vision, and gesture inputs for robust command understanding\n    \"\"\"\n    def __init__(self):\n        super().__init__('multimodal_fusion')\n        \n        # Subscribers\n        self.speech_sub = self.create_subscription(\n            String,\n            '/voice_commands/text',\n            self.speech_callback,\n            10\n        )\n        \n        self.vision_sub = self.create_subscription(\n            Detection2DArray,\n            '/vision/detections',\n            self.vision_callback,\n            10\n        )\n        \n        self.gesture_sub = self.create_subscription(\n            String,\n            '/gestures/recognized',\n            self.gesture_callback,\n            10\n        )\n        \n        # Publisher for fused commands\n        self.fused_cmd_pub = self.create_publisher(String, '/voice_commands/fused_command', 10)\n        \n        # Modality buffers (store recent inputs)\n        self.speech_buffer = deque(maxlen=5)\n        self.vision_buffer = deque(maxlen=10)\n        self.gesture_buffer = deque(maxlen=5)\n        \n        # Fusion parameters\n        self.declare_parameter('fusion_timeout', 2.0)  # Seconds to wait for modalities\n        self.fusion_timeout = self.get_parameter('fusion_timeout').value\n        \n        self.get_logger().info('Multi-modal fusion node started')\n    \n    def speech_callback(self, msg):\n        \"\"\"Handle speech input\"\"\"\n        text = msg.data.strip()\n        if text:\n            self.speech_buffer.append({\n                'text': text,\n                'timestamp': self.get_clock().now().nanoseconds / 1e9\n            })\n            self.get_logger().info(f'Speech received: {text}')\n            self.attempt_fusion()\n    \n    def vision_callback(self, msg):\n        \"\"\"Handle vision input\"\"\"\n        detections = []\n        for det in msg.detections:\n            if det.results:\n                obj_class = det.results[0].id\n                confidence = det.results[0].score\n                center_x = det.bbox.center.x\n                center_y = det.bbox.center.y\n                \n                detections.append({\n                    'class': obj_class,\n                    'confidence': confidence,\n                    'center': [center_x, center_y]\n                })\n        \n        if detections:\n            self.vision_buffer.append({\n                'detections': detections,\n                'timestamp': self.get_clock().now().nanoseconds / 1e9\n            })\n            self.get_logger().info(f'Vision: {len(detections)} objects detected')\n            self.attempt_fusion()\n    \n    def gesture_callback(self, msg):\n        \"\"\"Handle gesture input\"\"\"\n        gesture = msg.data\n        if gesture:\n            self.gesture_buffer.append({\n                'gesture': gesture,\n                'timestamp': self.get_clock().now().nanoseconds / 1e9\n            })\n            self.get_logger().info(f'Gesture: {gesture}')\n            self.attempt_fusion()\n    \n    def attempt_fusion(self):\n        \"\"\"Attempt to fuse modalities when new input arrives\"\"\"\n        current_time = self.get_clock().now().nanoseconds / 1e9\n        \n        # Get recent inputs within timeout window\n        recent_speech = [\n            s for s in self.speech_buffer\n            if current_time - s['timestamp'] < self.fusion_timeout\n        ]\n        recent_vision = [\n            v for v in self.vision_buffer\n            if current_time - v['timestamp'] < self.fusion_timeout\n        ]\n        recent_gesture = [\n            g for g in self.gesture_buffer\n            if current_time - g['timestamp'] < self.fusion_timeout\n        ]\n        \n        # Fuse if we have speech (primary modality)\n        if recent_speech:\n            speech_text = recent_speech[-1]['text']  # Most recent\n            \n            # Extract object references from speech\n            objects_mentioned = self.extract_objects_from_speech(speech_text)\n            \n            # Match with vision detections\n            matched_objects = self.match_speech_to_vision(objects_mentioned, recent_vision)\n            \n            # Incorporate gesture (e.g., pointing)\n            gesture_context = recent_gesture[-1]['gesture'] if recent_gesture else None\n            \n            # Create fused command\n            fused_command = self.create_fused_command(\n                speech_text,\n                matched_objects,\n                gesture_context\n            )\n            \n            # Publish fused command\n            msg = String()\n            msg.data = json.dumps(fused_command)\n            self.fused_cmd_pub.publish(msg)\n            \n            self.get_logger().info(f'Fused command: {fused_command}')\n    \n    def extract_objects_from_speech(self, text):\n        \"\"\"Extract object mentions from speech\"\"\"\n        objects = []\n        text_lower = text.lower()\n        \n        # Simple keyword matching (would use NER in production)\n        color_keywords = ['red', 'blue', 'green', 'yellow']\n        object_keywords = ['cup', 'book', 'bottle', 'object', 'thing']\n        \n        for color in color_keywords:\n            if color in text_lower:\n                for obj in object_keywords:\n                    if obj in text_lower:\n                        objects.append(f'{color}_{obj}')\n        \n        return objects\n    \n    def match_speech_to_vision(self, objects_mentioned, vision_data):\n        \"\"\"Match speech objects to vision detections\"\"\"\n        matched = []\n        \n        if not vision_data:\n            return matched\n        \n        # Get most recent vision data\n        latest_vision = vision_data[-1]['detections']\n        \n        for obj_mentioned in objects_mentioned:\n            # Find matching detection\n            for det in latest_vision:\n                if obj_mentioned in det['class']:\n                    matched.append({\n                        'object_id': det['class'],\n                        'confidence': det['confidence'],\n                        'center': det['center']\n                    })\n                    break\n        \n        return matched\n    \n    def create_fused_command(self, speech_text, matched_objects, gesture):\n        \"\"\"Create fused command with all modalities\"\"\"\n        command = {\n            'speech': speech_text,\n            'objects': matched_objects,\n            'gesture': gesture,\n            'timestamp': self.get_clock().now().nanoseconds / 1e9\n        }\n        \n        # If gesture is pointing, add pointing direction\n        if gesture == 'point':\n            # Would extract pointing direction from gesture\n            command['pointing_direction'] = 'forward'  # Placeholder\n        \n        return command\n\ndef main(args=None):\n    rclpy.init(args=args)\n    node = MultimodalFusion()\n    rclpy.spin(node)\n    node.destroy_node()\n    rclpy.shutdown()\n\nif __name__ == '__main__':\n    main()\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-4-update-llm-planner-for-multi-modal-input",children:"Step 4: Update LLM Planner for Multi-Modal Input"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Modify LLM planner"})," to use fused commands:"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# In LLMPlanner.__init__\nself.fused_cmd_sub = self.create_subscription(\n    String,\n    \'/voice_commands/fused_command\',\n    self.fused_command_callback,\n    10\n)\n\ndef fused_command_callback(self, msg):\n    """Handle fused multi-modal command"""\n    fused_cmd = json.loads(msg.data)\n    speech_text = fused_cmd.get(\'speech\')\n    objects = fused_cmd.get(\'objects\', [])\n    gesture = fused_cmd.get(\'gesture\')\n    \n    # Enhance prompt with vision and gesture context\n    enhanced_prompt = f"""Command: {speech_text}\n\nVisual context:\n"""\n    for obj in objects:\n        enhanced_prompt += f"- {obj[\'object_id\']} detected at position {obj[\'center\']}\\n"\n    \n    if gesture:\n        enhanced_prompt += f"\\nGesture: {gesture}\\n"\n    \n    enhanced_prompt += "\\nGenerate action plan considering the visual context and gesture."\n    \n    # Generate plan with enhanced context\n    plan = self.generate_plan_with_prompt(enhanced_prompt)\n    # ... (publish plan)\n'})}),"\n",(0,t.jsx)(n.h3,{id:"step-5-test-multi-modal-pipeline",children:"Step 5: Test Multi-Modal Pipeline"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Launch complete system"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"# Terminal 1: Vision node\nros2 run voice_commands vision_node\n\n# Terminal 2: Gesture node\nros2 run voice_commands gesture_node\n\n# Terminal 3: Multi-modal fusion\nros2 run voice_commands multimodal_fusion\n\n# Terminal 4: LLM planner (updated)\nros2 run voice_commands llm_planner\n\n# Terminal 5: Action executor\nros2 run voice_commands action_executor\n"})}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Test scenarios"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech + Vision"}),': Say "Pick up the red cup" while camera sees red cup']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Speech + Gesture"}),': Say "Go there" while pointing']}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"All modalities"}),': Say "Pick up that cup" + point + camera sees cup']}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Monitor fusion"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-bash",children:"ros2 topic echo /voice_commands/fused_command\n"})}),"\n",(0,t.jsx)(n.h3,{id:"step-6-debugging-common-issues",children:"Step 6: Debugging Common Issues"}),"\n",(0,t.jsx)(n.h4,{id:"issue-1-modalities-not-synchronizing",children:'Issue 1: "Modalities not synchronizing"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),": Speech and vision arrive at different times"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Increase fusion timeout\nfusion_timeout = 3.0  # Wait longer for modalities\n\n# Use timestamps to align modalities\n# Match modalities within time window\n"})}),"\n",(0,t.jsx)(n.h4,{id:"issue-2-object-matching-fails",children:'Issue 2: "Object matching fails"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),': Speech mentions "red cup" but vision doesn\'t match']}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Improve object extraction from speech\n# Use more robust matching (fuzzy matching, synonyms)\n# Handle color variations ("red" vs "crimson")\n\n# Add fallback: If no match, use most confident detection\n'})}),"\n",(0,t.jsx)(n.h4,{id:"issue-3-gesture-recognition-inaccurate",children:'Issue 3: "Gesture recognition inaccurate"'}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Symptoms"}),": Wrong gestures detected"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Solutions"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Improve gesture classification\n# Add temporal smoothing (require gesture for N frames)\n# Use more sophisticated gesture models\n"})}),"\n",(0,t.jsx)(n.h2,{id:"part-3-advanced-topics-optional",children:"Part 3: Advanced Topics (Optional)"}),"\n",(0,t.jsx)(n.h3,{id:"attention-based-fusion",children:"Attention-Based Fusion"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Weight modalities dynamically"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'def fuse_with_attention(self, speech, vision, gesture):\n    """Fuse modalities with attention weights"""\n    # Calculate reliability scores\n    speech_reliability = self.calculate_speech_reliability(speech)\n    vision_reliability = self.calculate_vision_reliability(vision)\n    gesture_reliability = self.calculate_gesture_reliability(gesture)\n    \n    # Normalize weights\n    total = speech_reliability + vision_reliability + gesture_reliability\n    speech_weight = speech_reliability / total\n    vision_weight = vision_reliability / total\n    gesture_weight = gesture_reliability / total\n    \n    # Weighted fusion\n    # ...\n'})}),"\n",(0,t.jsx)(n.h3,{id:"temporal-fusion",children:"Temporal Fusion"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Fuse over time windows"}),":"]}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Maintain history of inputs\n# Fuse across time window (e.g., last 2 seconds)\n# Handle temporal relationships (gesture before speech, etc.)\n"})}),"\n",(0,t.jsx)(n.h2,{id:"integration-with-capstone",children:"Integration with Capstone"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"How this chapter contributes"})," to the Week 13 autonomous humanoid:"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Robust understanding"}),": Multi-modal fusion enables robust command understanding"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Natural interaction"}),": Supports natural human communication patterns"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Error recovery"}),": If one modality fails, others compensate"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Rich context"}),": Multiple cues improve action planning accuracy"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Understanding multi-modal fusion now is essential for the capstone natural interaction system."}),"\n",(0,t.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,t.jsx)(n.p,{children:"You learned:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:["\u2705 Combined ",(0,t.jsx)(n.strong,{children:"voice commands with visual perception"})," for object identification"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 Implemented ",(0,t.jsx)(n.strong,{children:"gesture recognition"})," for human-robot interaction"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 Fused ",(0,t.jsx)(n.strong,{children:"multi-modal inputs"})," for robust understanding"]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 Created ",(0,t.jsx)(n.strong,{children:"unified perception-action pipeline"})]}),"\n",(0,t.jsxs)(n.li,{children:["\u2705 Handled ",(0,t.jsx)(n.strong,{children:"modality conflicts"})," and ambiguity resolution"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next steps"}),": In Chapter 4.5, you'll learn humanoid kinematics and balance control for executing manipulation and navigation actions."]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,t.jsx)(n.h3,{id:"exercise-1-basic-multi-modal-fusion-required",children:"Exercise 1: Basic Multi-Modal Fusion (Required)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Combine speech and vision for object disambiguation."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up vision node detecting objects"}),"\n",(0,t.jsx)(n.li,{children:"Create fusion node combining speech and vision"}),"\n",(0,t.jsxs)(n.li,{children:["Test with ambiguous commands:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'"Pick up the cup" (multiple cups visible)'}),"\n",(0,t.jsx)(n.li,{children:'"Pick up the red cup" (vision identifies red cup)'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Verify fusion improves command understanding"}),"\n",(0,t.jsx)(n.li,{children:"Document fusion improvements"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Vision node detecting objects"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fusion node combining modalities"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Ambiguous commands resolved correctly"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Fusion improves accuracy"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 120 minutes"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-2-gesture-integration-required",children:"Exercise 2: Gesture Integration (Required)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Add gesture recognition to multi-modal system."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Set up gesture recognition node"}),"\n",(0,t.jsx)(n.li,{children:"Integrate gestures into fusion pipeline"}),"\n",(0,t.jsxs)(n.li,{children:["Test scenarios:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:'Pointing gesture + "Go there"'}),"\n",(0,t.jsx)(n.li,{children:'Waving gesture + "Come here"'}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Verify gesture enhances commands"}),"\n",(0,t.jsx)(n.li,{children:"Document gesture impact"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Acceptance Criteria"}),":"]}),"\n",(0,t.jsxs)(n.ul,{className:"contains-task-list",children:["\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Gesture node recognizing gestures"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Gestures integrated into fusion"]}),"\n",(0,t.jsxs)(n.li,{className:"task-list-item",children:[(0,t.jsx)(n.input,{type:"checkbox",disabled:!0})," ","Gesture-enhanced commands work correctly"]}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 180 minutes"]}),"\n",(0,t.jsx)(n.h3,{id:"exercise-3-attention-based-fusion-challenge",children:"Exercise 3: Attention-Based Fusion (Challenge)"}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Objective"}),": Implement dynamic modality weighting."]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Tasks"}),":"]}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsx)(n.li,{children:"Calculate reliability scores for each modality"}),"\n",(0,t.jsx)(n.li,{children:"Implement attention-based fusion"}),"\n",(0,t.jsxs)(n.li,{children:["Test with varying reliability:","\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"High noise speech (low reliability)"}),"\n",(0,t.jsx)(n.li,{children:"Poor lighting vision (low reliability)"}),"\n",(0,t.jsx)(n.li,{children:"Clear gesture (high reliability)"}),"\n"]}),"\n"]}),"\n",(0,t.jsx)(n.li,{children:"Verify attention weights adapt correctly"}),"\n",(0,t.jsx)(n.li,{children:"Compare to fixed-weight fusion"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Requirements"}),":"]}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsx)(n.li,{children:"Reliability calculation"}),"\n",(0,t.jsx)(n.li,{children:"Attention-based weighting"}),"\n",(0,t.jsx)(n.li,{children:"Performance comparison"}),"\n"]}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Estimated Time"}),": 240 minutes"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsx)(n.h2,{id:"additional-resources",children:"Additional Resources"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://google.github.io/mediapipe/solutions/hands.html",children:"MediaPipe Hands"})," - Gesture recognition"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://docs.opencv.org/4.x/d9/db7/tutorial_py_table_of_contents_contours.html",children:"OpenCV Object Detection"})," - Vision processing"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://arxiv.org/abs/2209.03499",children:"Multi-Modal Fusion"})," - Research paper"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.a,{href:"https://github.com/ros-perception/vision_msgs",children:"ROS 2 Vision Messages"})," - Vision message types"]}),"\n"]}),"\n",(0,t.jsx)(n.hr,{}),"\n",(0,t.jsxs)(n.p,{children:[(0,t.jsx)(n.strong,{children:"Next"}),": [Chapter 4.5: Humanoid Kinematics & Balance Control \u2192](chapter-4 to 5.md)"]})]})}function u(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(a,{...e})}):a(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>o,x:()=>l});var i=s(6540);const t={},r=i.createContext(t);function o(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);
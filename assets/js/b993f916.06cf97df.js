"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[138],{2754:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>a,contentTitle:()=>l,default:()=>h,frontMatter:()=>o,metadata:()=>s,toc:()=>d});const s=JSON.parse('{"id":"module-2/intro","title":"Module 2 Introduction","description":"Welcome to Module 2! Having mastered the foundations of Physical AI in Module 1, we now dive deep into one of the most critical capabilities for humanoid robots: computer vision.","source":"@site/docs/module-2/intro.md","sourceDirName":"module-2","slug":"/module-2/intro","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/intro","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/intro.md","tags":[],"version":"current","sidebarPosition":1,"frontMatter":{"sidebar_position":1,"title":"Module 2 Introduction"},"sidebar":"defaultSidebar","previous":{"title":"1.4 ROS and Simulation","permalink":"/physical-ai-humanoid-robotics-book/docs/module-1/chapter-1-4"},"next":{"title":"2.1 Computer Vision Fundamentals","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1"}}');var r=i(4848),t=i(8453);const o={sidebar_position:1,title:"Module 2 Introduction"},l="Module 2: Computer Vision for Robotics",a={},d=[{value:"Module Overview",id:"module-overview",level:2},{value:"What You&#39;ll Learn",id:"what-youll-learn",level:3},{value:"Prerequisites",id:"prerequisites",level:3},{value:"Why Computer Vision for Robotics?",id:"why-computer-vision-for-robotics",level:3},{value:"Module Structure",id:"module-structure",level:3},{value:"Real-World Applications",id:"real-world-applications",level:3},{value:"Tools We&#39;ll Use",id:"tools-well-use",level:3},{value:"Learning Path",id:"learning-path",level:2},{value:"Module Goals",id:"module-goals",level:2},{value:"Getting Started",id:"getting-started",level:2}];function c(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"module-2-computer-vision-for-robotics",children:"Module 2: Computer Vision for Robotics"})}),"\n",(0,r.jsxs)(n.p,{children:["Welcome to Module 2! Having mastered the foundations of Physical AI in Module 1, we now dive deep into one of the most critical capabilities for humanoid robots: ",(0,r.jsx)(n.strong,{children:"computer vision"}),"."]}),"\n",(0,r.jsx)(n.h2,{id:"module-overview",children:"Module Overview"}),"\n",(0,r.jsx)(n.p,{children:"Vision is arguably the most important sense for humanoid robots. Just as humans rely heavily on sight to navigate, manipulate objects, and interact with others, robots need robust visual perception to operate in real-world environments."}),"\n",(0,r.jsx)(n.p,{children:"This module covers the full spectrum of computer vision for robotics\u2014from basic image processing to state-of-the-art deep learning models."}),"\n",(0,r.jsx)(n.h3,{id:"what-youll-learn",children:"What You'll Learn"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Chapter 2.1: Computer Vision Fundamentals"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Image representation and processing basics"}),"\n",(0,r.jsx)(n.li,{children:"Filtering, edge detection, feature extraction"}),"\n",(0,r.jsx)(n.li,{children:"Classical computer vision techniques"}),"\n",(0,r.jsx)(n.li,{children:"OpenCV for robotics applications"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Chapter 2.2: Deep Learning for Vision"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Convolutional Neural Networks (CNNs) architecture"}),"\n",(0,r.jsx)(n.li,{children:"Object detection (YOLO, Faster R-CNN)"}),"\n",(0,r.jsx)(n.li,{children:"Semantic and instance segmentation"}),"\n",(0,r.jsx)(n.li,{children:"Transfer learning and pre-trained models"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Chapter 2.3: 3D Vision and Depth Perception"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Stereo vision and disparity maps"}),"\n",(0,r.jsx)(n.li,{children:"Structure from Motion (SfM)"}),"\n",(0,r.jsx)(n.li,{children:"Depth camera processing"}),"\n",(0,r.jsx)(n.li,{children:"Point cloud analysis"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"Chapter 2.4: Visual Odometry and SLAM"})}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Camera-based localization"}),"\n",(0,r.jsx)(n.li,{children:"Visual SLAM algorithms (ORB-SLAM, LSD-SLAM)"}),"\n",(0,r.jsx)(n.li,{children:"Sensor fusion (visual-inertial odometry)"}),"\n",(0,r.jsx)(n.li,{children:"Loop closure and map optimization"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"prerequisites",children:"Prerequisites"}),"\n",(0,r.jsx)(n.p,{children:"Before starting this module, you should be comfortable with:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Python programming (NumPy, Matplotlib)"}),"\n",(0,r.jsx)(n.li,{children:"Basic linear algebra (vectors, matrices, transformations)"}),"\n",(0,r.jsx)(n.li,{children:"Neural network fundamentals (from Chapter 1.1)"}),"\n",(0,r.jsx)(n.li,{children:"Robot coordinate frames and kinematics (from Chapter 1.2)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"why-computer-vision-for-robotics",children:"Why Computer Vision for Robotics?"}),"\n",(0,r.jsx)(n.p,{children:"Traditional computer vision (for image classification, facial recognition) differs from robotic vision in key ways:"}),"\n",(0,r.jsxs)(n.table,{children:[(0,r.jsx)(n.thead,{children:(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.th,{children:"Aspect"}),(0,r.jsx)(n.th,{children:"Traditional CV"}),(0,r.jsx)(n.th,{children:"Robotic CV"})]})}),(0,r.jsxs)(n.tbody,{children:[(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Goal"})}),(0,r.jsx)(n.td,{children:"Classify or detect objects in images"}),(0,r.jsx)(n.td,{children:"Enable physical action in 3D world"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Output"})}),(0,r.jsx)(n.td,{children:"Labels, bounding boxes"}),(0,r.jsx)(n.td,{children:"3D positions, orientations, trajectories"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Real-time"})}),(0,r.jsx)(n.td,{children:"Often offline processing acceptable"}),(0,r.jsx)(n.td,{children:"Must run at 10-30 Hz"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Robustness"})}),(0,r.jsx)(n.td,{children:"Can reject ambiguous cases"}),(0,r.jsx)(n.td,{children:"Must handle all situations (safety-critical)"})]}),(0,r.jsxs)(n.tr,{children:[(0,r.jsx)(n.td,{children:(0,r.jsx)(n.strong,{children:"Integration"})}),(0,r.jsx)(n.td,{children:"Standalone"}),(0,r.jsx)(n.td,{children:"Must interface with planning, control"})]})]})]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Robotic vision must answer"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Where is the object in 3D space? (not just in the image)"}),"\n",(0,r.jsx)(n.li,{children:"How can the robot grasp it? (pose estimation)"}),"\n",(0,r.jsx)(n.li,{children:"Is the path clear? (obstacle detection)"}),"\n",(0,r.jsx)(n.li,{children:"Where is the robot? (localization)"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"module-structure",children:"Module Structure"}),"\n",(0,r.jsx)(n.p,{children:"Each chapter includes:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Theory"}),": Mathematical foundations and algorithms"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Code examples"}),": OpenCV and PyTorch implementations"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Practical applications"}),": Real humanoid robot use cases"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Exercises"}),": Hands-on projects to reinforce learning"]}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"real-world-applications",children:"Real-World Applications"}),"\n",(0,r.jsx)(n.p,{children:"By the end of this module, you'll understand the vision systems behind:"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Humanoid Manipulation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Object detection and pose estimation for grasping"}),"\n",(0,r.jsx)(n.li,{children:"Visual servoing (closing gripper based on camera feedback)"}),"\n",(0,r.jsx)(n.li,{children:"Bin picking in warehouses"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Navigation"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Obstacle detection and avoidance"}),"\n",(0,r.jsx)(n.li,{children:"Visual odometry (estimating robot motion from camera)"}),"\n",(0,r.jsx)(n.li,{children:"SLAM (building maps while localizing)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Human-Robot Interaction"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Face detection and recognition"}),"\n",(0,r.jsx)(n.li,{children:"Gesture recognition for commands"}),"\n",(0,r.jsx)(n.li,{children:"Social cues (eye contact, attention tracking)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Human detection and tracking"}),"\n",(0,r.jsx)(n.li,{children:"Collision prediction"}),"\n",(0,r.jsx)(n.li,{children:"Emergency stop triggers"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"tools-well-use",children:"Tools We'll Use"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"OpenCV"}),": Industry-standard library for image processing"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"PyTorch/TensorFlow"}),": Deep learning frameworks"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Open3D"}),": 3D data processing and visualization"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"ROS"}),": Camera drivers and image transport"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"COLMAP, ORB-SLAM"}),": SLAM implementations"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"learning-path",children:"Learning Path"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{children:"Chapter 2.1: CV Fundamentals\n      \u2193\nChapter 2.2: Deep Learning for Vision\n      \u2193\nChapter 2.3: 3D Vision & Depth\n      \u2193\nChapter 2.4: Visual Odometry & SLAM\n"})}),"\n",(0,r.jsx)(n.p,{children:"Start with fundamentals, build up to deep learning, then integrate with 3D geometry and robot motion."}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"module-goals",children:"Module Goals"}),"\n",(0,r.jsx)(n.p,{children:"By completing Module 2, you will be able to:"}),"\n",(0,r.jsx)(n.p,{children:"\u2705 Process and analyze images from robot cameras using OpenCV\n\u2705 Implement object detection using modern deep learning models\n\u2705 Extract 3D information from stereo cameras and depth sensors\n\u2705 Build visual SLAM systems for robot localization and mapping\n\u2705 Integrate vision pipelines with ROS for real-time robotics\n\u2705 Understand trade-offs (accuracy vs. speed, 2D vs. 3D)\n\u2705 Debug vision systems and handle edge cases"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"getting-started",children:"Getting Started"}),"\n",(0,r.jsx)(n.p,{children:"Ready to give your robot the gift of sight? Let's start with the fundamentals of computer vision!"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-1",children:"Chapter 2.1: Computer Vision Fundamentals \u2192"})]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(c,{...e})}):c(e)}},8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>l});var s=i(6540);const r={},t=s.createContext(r);function o(e){const n=s.useContext(t);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function l(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),s.createElement(t.Provider,{value:n},e.children)}}}]);
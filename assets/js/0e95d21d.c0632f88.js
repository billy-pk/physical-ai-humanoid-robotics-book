"use strict";(globalThis.webpackChunkfrontend=globalThis.webpackChunkfrontend||[]).push([[325],{8453:(e,n,i)=>{i.d(n,{R:()=>o,x:()=>a});var t=i(6540);const r={},s=t.createContext(r);function o(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(r):e.components||r:o(e.components),t.createElement(s.Provider,{value:n},e.children)}},8506:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>p,frontMatter:()=>o,metadata:()=>t,toc:()=>c});const t=JSON.parse('{"id":"module-2/chapter-2-3","title":"2.3 3D Vision and Depth Perception","description":"Robots need to understand 3D space to navigate, grasp objects, and avoid obstacles. This chapter covers depth sensing technologies and 3D data processing for robotics.","source":"@site/docs/module-2/chapter-2-3.md","sourceDirName":"module-2","slug":"/module-2/chapter-2-3","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-3","draft":false,"unlisted":false,"editUrl":"https://github.com/billy-pk/physical-ai-humanoid-robotics-book/tree/main/frontend/docs/module-2/chapter-2-3.md","tags":[],"version":"current","sidebarPosition":4,"frontMatter":{"sidebar_position":4,"title":"2.3 3D Vision and Depth Perception"},"sidebar":"defaultSidebar","previous":{"title":"2.2 Deep Learning for Vision","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2"},"next":{"title":"2.4 Visual SLAM Basics","permalink":"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-4"}}');var r=i(4848),s=i(8453);const o={sidebar_position:4,title:"2.3 3D Vision and Depth Perception"},a="Chapter 2.3: 3D Vision and Depth Perception",l={},c=[{value:"Learning Outcomes",id:"learning-outcomes",level:2},{value:"Why 3D Vision?",id:"why-3d-vision",level:2},{value:"Depth Sensing Technologies",id:"depth-sensing-technologies",level:2},{value:"1. Stereo Vision",id:"1-stereo-vision",level:3},{value:"2. Time-of-Flight (ToF)",id:"2-time-of-flight-tof",level:3},{value:"3. Structured Light",id:"3-structured-light",level:3},{value:"Using Depth Cameras: Intel RealSense",id:"using-depth-cameras-intel-realsense",level:2},{value:"Installation",id:"installation",level:3},{value:"Basic Depth Capture",id:"basic-depth-capture",level:3},{value:"Get 3D Coordinates from Pixel",id:"get-3d-coordinates-from-pixel",level:3},{value:"Point Clouds",id:"point-clouds",level:2},{value:"Creating Point Clouds from Depth",id:"creating-point-clouds-from-depth",level:3},{value:"Point Cloud Processing",id:"point-cloud-processing",level:3},{value:"3D Object Detection",id:"3d-object-detection",level:2},{value:"Stereo Vision with OpenCV",id:"stereo-vision-with-opencv",level:2},{value:"Exercises",id:"exercises",level:2},{value:"1. Depth Camera Exploration",id:"1-depth-camera-exploration",level:3},{value:"2. Point Cloud Manipulation",id:"2-point-cloud-manipulation",level:3},{value:"3. 3D Object Localization",id:"3-3d-object-localization",level:3},{value:"4. Stereo Calibration Research",id:"4-stereo-calibration-research",level:3},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Further Reading",id:"further-reading",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.header,{children:(0,r.jsx)(n.h1,{id:"chapter-23-3d-vision-and-depth-perception",children:"Chapter 2.3: 3D Vision and Depth Perception"})}),"\n",(0,r.jsx)(n.p,{children:"Robots need to understand 3D space to navigate, grasp objects, and avoid obstacles. This chapter covers depth sensing technologies and 3D data processing for robotics."}),"\n",(0,r.jsx)(n.h2,{id:"learning-outcomes",children:"Learning Outcomes"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Understand"})," depth sensing technologies (stereo, ToF, structured light)"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Process"})," depth images and point clouds"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Apply"})," 3D vision for navigation and manipulation"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Use"})," libraries (OpenCV, Open3D) for 3D data"]}),"\n"]}),"\n",(0,r.jsx)(n.h2,{id:"why-3d-vision",children:"Why 3D Vision?"}),"\n",(0,r.jsx)(n.p,{children:"2D images lack depth\u2014you can't tell if an object is far away or just small."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"3D vision enables"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Navigation"}),": Detect obstacles, measure distances"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Grasping"}),": Know exact 3D position/orientation of objects"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Mapping"}),": Build 3D maps of environments"]}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.strong,{children:"Safety"}),": Detect humans and collision risks"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"depth-sensing-technologies",children:"Depth Sensing Technologies"}),"\n",(0,r.jsx)(n.h3,{id:"1-stereo-vision",children:"1. Stereo Vision"}),"\n",(0,r.jsx)(n.p,{children:"Uses two cameras (like human eyes) to compute depth via triangulation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"How it works"}),":"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"Capture images from left and right cameras"}),"\n",(0,r.jsx)(n.li,{children:"Find corresponding points in both images (matching)"}),"\n",(0,r.jsxs)(n.li,{children:["Compute ",(0,r.jsx)(n.strong,{children:"disparity"})," (pixel difference)"]}),"\n",(0,r.jsxs)(n.li,{children:["Convert disparity to depth: ",(0,r.jsx)(n.code,{children:"depth = (focal_length \xd7 baseline) / disparity"})]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Passive (no active illumination)"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Works outdoors in sunlight"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Inexpensive (just two cameras)"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Computationally expensive (matching)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Fails on textureless surfaces"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Requires precise calibration"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"2-time-of-flight-tof",children:"2. Time-of-Flight (ToF)"}),"\n",(0,r.jsx)(n.p,{children:"Measures time for light pulse to return."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),": Microsoft Kinect Azure, PMD sensors"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 Fast, dense depth maps"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Works on textureless surfaces"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Limited range (~5m indoors)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Fails in bright sunlight"}),"\n",(0,r.jsx)(n.li,{children:"\u274c More expensive"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"3-structured-light",children:"3. Structured Light"}),"\n",(0,r.jsx)(n.p,{children:"Projects known pattern, measures deformation."}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Examples"}),": Intel RealSense D435, Kinect v1"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Pros"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u2705 High accuracy at close range"}),"\n",(0,r.jsx)(n.li,{children:"\u2705 Dense depth maps"}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Cons"}),":"]}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u274c Interference from multiple devices"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Doesn't work outdoors (sunlight)"}),"\n",(0,r.jsx)(n.li,{children:"\u274c Limited range (~3-5m)"}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"using-depth-cameras-intel-realsense",children:"Using Depth Cameras: Intel RealSense"}),"\n",(0,r.jsx)(n.h3,{id:"installation",children:"Installation"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-bash",children:"pip install pyrealsense2 opencv-python numpy\n"})}),"\n",(0,r.jsx)(n.h3,{id:"basic-depth-capture",children:"Basic Depth Capture"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import pyrealsense2 as rs\nimport numpy as np\nimport cv2\n\n# Configure depth and color streams\npipeline = rs.pipeline()\nconfig = rs.config()\nconfig.enable_stream(rs.stream.depth, 640, 480, rs.format.z16, 30)\nconfig.enable_stream(rs.stream.color, 640, 480, rs.format.bgr8, 30)\n\n# Start streaming\npipeline.start(config)\n\ntry:\n    while True:\n        # Wait for frames\n        frames = pipeline.wait_for_frames()\n        depth_frame = frames.get_depth_frame()\n        color_frame = frames.get_color_frame()\n\n        if not depth_frame or not color_frame:\n            continue\n\n        # Convert to numpy arrays\n        depth_image = np.asanyarray(depth_frame.get_data())\n        color_image = np.asanyarray(color_frame.get_data())\n\n        # Apply colormap to depth (for visualization)\n        depth_colormap = cv2.applyColorMap(\n            cv2.convertScaleAbs(depth_image, alpha=0.03),\n            cv2.COLORMAP_JET\n        )\n\n        # Stack images horizontally\n        images = np.hstack((color_image, depth_colormap))\n\n        cv2.imshow('RealSense', images)\n\n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n\nfinally:\n    pipeline.stop()\n    cv2.destroyAllWindows()\n"})}),"\n",(0,r.jsx)(n.h3,{id:"get-3d-coordinates-from-pixel",children:"Get 3D Coordinates from Pixel"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'# Get intrinsic parameters\ndepth_intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n\n# Pixel coordinates (example: center of image)\npixel_x, pixel_y = 320, 240\n\n# Get depth at this pixel (in millimeters)\ndepth_value = depth_frame.get_distance(pixel_x, pixel_y)  # meters\n\n# Convert to 3D point\npoint_3d = rs.rs2_deproject_pixel_to_point(\n    depth_intrin,\n    [pixel_x, pixel_y],\n    depth_value\n)\n\nprint(f"3D point: x={point_3d[0]:.3f}m, y={point_3d[1]:.3f}m, z={point_3d[2]:.3f}m")\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"point-clouds",children:"Point Clouds"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Point cloud"})," = collection of 3D points representing a scene."]}),"\n",(0,r.jsx)(n.h3,{id:"creating-point-clouds-from-depth",children:"Creating Point Clouds from Depth"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import open3d as o3d\n\ndef depth_to_pointcloud(depth_image, color_image, intrinsics):\n    """\n    Convert depth + color images to point cloud.\n\n    Args:\n        depth_image: HxW depth map (meters)\n        color_image: HxWx3 RGB image\n        intrinsics: Camera intrinsic parameters\n\n    Returns:\n        Open3D point cloud\n    """\n    h, w = depth_image.shape\n    fx, fy = intrinsics.fx, intrinsics.fy\n    cx, cy = intrinsics.ppx, intrinsics.ppy\n\n    # Generate pixel coordinates\n    u, v = np.meshgrid(np.arange(w), np.arange(h))\n\n    # Compute 3D points\n    z = depth_image\n    x = (u - cx) * z / fx\n    y = (v - cy) * z / fy\n\n    # Stack into Nx3 array\n    points = np.stack((x, y, z), axis=-1).reshape(-1, 3)\n\n    # Get colors\n    colors = color_image.reshape(-1, 3) / 255.0\n\n    # Filter invalid points (z == 0)\n    valid = points[:, 2] > 0\n    points = points[valid]\n    colors = colors[valid]\n\n    # Create Open3D point cloud\n    pcd = o3d.geometry.PointCloud()\n    pcd.points = o3d.utility.Vector3dVector(points)\n    pcd.colors = o3d.utility.Vector3dVector(colors)\n\n    return pcd\n\n# Visualize\no3d.visualization.draw_geometries([pcd])\n'})}),"\n",(0,r.jsx)(n.h3,{id:"point-cloud-processing",children:"Point Cloud Processing"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:'import open3d as o3d\n\n# Load point cloud (example: from PLY file)\npcd = o3d.io.read_point_cloud("scene.ply")\n\n# Downsample (reduce points for speed)\npcd_down = pcd.voxel_down_sample(voxel_size=0.02)  # 2cm voxels\n\n# Remove outliers\npcd_clean, ind = pcd_down.remove_statistical_outlier(\n    nb_neighbors=20,\n    std_ratio=2.0\n)\n\n# Estimate normals (useful for surface reconstruction)\npcd_clean.estimate_normals(\n    search_param=o3d.geometry.KDTreeSearchParamHybrid(radius=0.1, max_nn=30)\n)\n\n# Segment plane (e.g., find ground/table)\nplane_model, inliers = pcd_clean.segment_plane(\n    distance_threshold=0.01,\n    ransac_n=3,\n    num_iterations=1000\n)\n\n# Separate plane and objects\nplane_cloud = pcd_clean.select_by_index(inliers)\nobject_cloud = pcd_clean.select_by_index(inliers, invert=True)\n\n# Color code (plane = red, objects = original colors)\nplane_cloud.paint_uniform_color([1, 0, 0])\n\no3d.visualization.draw_geometries([plane_cloud, object_cloud])\n'})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"3d-object-detection",children:"3D Object Detection"}),"\n",(0,r.jsx)(n.p,{children:"Combine YOLO (2D) with depth to get 3D bounding boxes."}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"from ultralytics import YOLO\nimport pyrealsense2 as rs\n\nmodel = YOLO('yolov8n.pt')\n\n# ... (RealSense initialization code)\n\nwhile True:\n    # Get frames\n    frames = pipeline.wait_for_frames()\n    depth_frame = frames.get_depth_frame()\n    color_frame = frames.get_color_frame()\n\n    color_image = np.asanyarray(color_frame.get_data())\n\n    # Run YOLO detection\n    results = model(color_image, conf=0.5)\n\n    for result in results:\n        boxes = result.boxes\n        for box in boxes:\n            x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n            cls = int(box.cls[0])\n            label = model.names[cls]\n\n            # Get depth at bounding box center\n            center_x = int((x1 + x2) / 2)\n            center_y = int((y1 + y2) / 2)\n\n            # Get 3D position\n            depth = depth_frame.get_distance(center_x, center_y)\n\n            if depth > 0:\n                # Deproject to 3D\n                intrin = depth_frame.profile.as_video_stream_profile().intrinsics\n                point_3d = rs.rs2_deproject_pixel_to_point(\n                    intrin, [center_x, center_y], depth\n                )\n\n                # Draw on image\n                cv2.rectangle(color_image,\n                             (int(x1), int(y1)), (int(x2), int(y2)),\n                             (0, 255, 0), 2)\n                text = f\"{label} ({point_3d[0]:.2f}, {point_3d[1]:.2f}, {point_3d[2]:.2f})m\"\n                cv2.putText(color_image, text, (int(x1), int(y1)-10),\n                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n\n    cv2.imshow('3D Detection', color_image)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Output"}),": Object label + 3D position in robot's coordinate frame."]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"stereo-vision-with-opencv",children:"Stereo Vision with OpenCV"}),"\n",(0,r.jsx)(n.p,{children:"If you have two calibrated cameras but no depth camera:"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-python",children:"import cv2\nimport numpy as np\n\n# Load stereo calibration parameters (from calibration process)\nstereo_map_left_x = np.load('stereo_map_left_x.npy')\nstereo_map_left_y = np.load('stereo_map_left_y.npy')\nstereo_map_right_x = np.load('stereo_map_right_x.npy')\nstereo_map_right_y = np.load('stereo_map_right_y.npy')\n\n# Stereo matcher\nstereo = cv2.StereoSGBM_create(\n    minDisparity=0,\n    numDisparities=16*6,  # Must be divisible by 16\n    blockSize=11,\n    P1=8 * 3 * 11**2,\n    P2=32 * 3 * 11**2,\n    disp12MaxDiff=1,\n    uniquenessRatio=10,\n    speckleWindowSize=100,\n    speckleRange=32\n)\n\n# Capture from left and right cameras\ncap_left = cv2.VideoCapture(0)\ncap_right = cv2.VideoCapture(1)\n\nwhile True:\n    ret_left, frame_left = cap_left.read()\n    ret_right, frame_right = cap_right.read()\n\n    if not ret_left or not ret_right:\n        break\n\n    # Rectify images\n    frame_left_rect = cv2.remap(frame_left, stereo_map_left_x,\n                                stereo_map_left_y, cv2.INTER_LINEAR)\n    frame_right_rect = cv2.remap(frame_right, stereo_map_right_x,\n                                 stereo_map_right_y, cv2.INTER_LINEAR)\n\n    # Convert to grayscale\n    gray_left = cv2.cvtColor(frame_left_rect, cv2.COLOR_BGR2GRAY)\n    gray_right = cv2.cvtColor(frame_right_rect, cv2.COLOR_BGR2GRAY)\n\n    # Compute disparity\n    disparity = stereo.compute(gray_left, gray_right).astype(np.float32) / 16.0\n\n    # Visualize\n    disparity_vis = cv2.normalize(disparity, None, 0, 255, cv2.NORM_MINMAX, cv2.CV_8U)\n    disparity_colormap = cv2.applyColorMap(disparity_vis, cv2.COLORMAP_JET)\n\n    cv2.imshow('Disparity', disparity_colormap)\n\n    if cv2.waitKey(1) & 0xFF == ord('q'):\n        break\n"})}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"exercises",children:"Exercises"}),"\n",(0,r.jsx)(n.h3,{id:"1-depth-camera-exploration",children:"1. Depth Camera Exploration"}),"\n",(0,r.jsx)(n.p,{children:"If you have access to a depth camera (RealSense, Kinect, or smartphone with LiDAR), capture depth images of different surfaces:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Smooth wall"}),"\n",(0,r.jsx)(n.li,{children:"Textured carpet"}),"\n",(0,r.jsx)(n.li,{children:"Reflective surface (mirror, metal)"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Which surfaces produce clean depth maps? Which fail?"}),"\n",(0,r.jsx)(n.h3,{id:"2-point-cloud-manipulation",children:"2. Point Cloud Manipulation"}),"\n",(0,r.jsx)(n.p,{children:"Load a point cloud (use Open3D sample data or capture your own). Apply:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Downsampling (different voxel sizes)"}),"\n",(0,r.jsx)(n.li,{children:"Outlier removal"}),"\n",(0,r.jsx)(n.li,{children:"Plane segmentation"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"Visualize results at each step. How does voxel size affect detail vs. speed?"}),"\n",(0,r.jsx)(n.h3,{id:"3-3d-object-localization",children:"3. 3D Object Localization"}),"\n",(0,r.jsx)(n.p,{children:"Combine YOLO with a depth camera to detect and localize objects in 3D. Create a simple application that:"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:'Detects a specific object class (e.g., "cup")'}),"\n",(0,r.jsx)(n.li,{children:"Prints its 3D coordinates"}),"\n",(0,r.jsx)(n.li,{children:"Draws an arrow pointing to it in the visualization"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"4-stereo-calibration-research",children:"4. Stereo Calibration Research"}),"\n",(0,r.jsx)(n.p,{children:"Research stereo camera calibration. What parameters need to be determined? Why is calibration critical for accurate depth?"}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,r.jsxs)(n.p,{children:["\u2705 ",(0,r.jsx)(n.strong,{children:"3D vision"})," enables robots to navigate and manipulate in real environments\n\u2705 ",(0,r.jsx)(n.strong,{children:"Depth cameras"})," (ToF, structured light) are easiest for robotics\n\u2705 ",(0,r.jsx)(n.strong,{children:"Stereo vision"})," is passive and works outdoors, but computationally heavy\n\u2705 ",(0,r.jsx)(n.strong,{children:"Point clouds"})," represent 3D scenes for mapping and object detection\n\u2705 ",(0,r.jsx)(n.strong,{children:"Combining 2D detection + depth"})," = full 3D object localization\n\u2705 Use ",(0,r.jsx)(n.strong,{children:"Open3D"})," for point cloud processing, ",(0,r.jsx)(n.strong,{children:"RealSense SDK"})," for depth cameras"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsx)(n.h2,{id:"further-reading",children:"Further Reading"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"https://www.intelrealsense.com/sdk-2/",children:"Intel RealSense Documentation"})}),"\n",(0,r.jsx)(n.li,{children:(0,r.jsx)(n.a,{href:"http://www.open3d.org/docs/release/tutorial/index.html",children:"Open3D Tutorials"})}),"\n",(0,r.jsxs)(n.li,{children:[(0,r.jsx)(n.em,{children:"Multiple View Geometry in Computer Vision"})," by Hartley & Zisserman (stereo vision theory)"]}),"\n"]}),"\n",(0,r.jsx)(n.hr,{}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"Previous"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-2",children:"\u2190 Chapter 2.2: Deep Learning for Vision"})," | ",(0,r.jsx)(n.strong,{children:"Next"}),": ",(0,r.jsx)(n.a,{href:"/physical-ai-humanoid-robotics-book/docs/module-2/chapter-2-4",children:"Chapter 2.4: Visual SLAM Basics \u2192"})]}),"\n",(0,r.jsx)(n.p,{children:"3D perception unlocked! Next, we'll see how robots use vision to localize themselves and build maps simultaneously."})]})}function p(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}}}]);